{"timestamp": "2025-06-16 11:21:00,389", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-06-16 11:21:00,396", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-06-16 11:21:00,397", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-06-16 11:21:00,400", "level": "WARNING", "logger": "root", "message": "LLM API key not configured. Some features may not work.", "module": "system_init", "function": "validate_system_requirements", "line": 125}
{"timestamp": "2025-06-16 11:21:00,402", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-06-16 11:21:00,416", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-06-16 11:21:00,417", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-06-16 11:21:00,422", "level": "INFO", "logger": "root", "message": "Created new FAISS index with dimension 384", "module": "faiss_store", "function": "_create_new_index", "line": 141}
{"timestamp": "2025-06-16 11:21:00,423", "level": "INFO", "logger": "root", "message": "FAISS store initialized with dimension 384", "module": "faiss_store", "function": "__init__", "line": 41}
{"timestamp": "2025-06-16 11:21:00,425", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-06-16 11:21:01,398", "level": "INFO", "logger": "root", "message": "Loaded SentenceTransformer model: sentence-transformers/all-MiniLM-L6-v2", "module": "embedder", "function": "_load_model", "line": 47}
{"timestamp": "2025-06-16 11:21:01,415", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: sentence-transformers", "module": "embedder", "function": "__init__", "line": 153}
{"timestamp": "2025-06-16 11:21:01,416", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-06-16 11:21:01,417", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['ingestion_engine', 'json_store', 'metadata_store', 'servicenow_integration', 'log_store', 'faiss_store', 'query_engine', 'reranker', 'llm_client', 'embedder', 'query_enhancer', 'config_manager', 'chunker']}", "module": "system_init", "function": "log_system_info", "line": 313}
{"timestamp": "2025-06-16 11:21:01,599", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-06-16 11:21:01,599", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 59}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking: size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 38}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 84}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "root", "message": "Excel processor registered without Azure AI support", "module": "ingestion_engine", "function": "_register_excel_processor", "line": 55}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized", "module": "ingestion_engine", "function": "__init__", "line": 29}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "root", "message": "Looking for existing vectors for file_path: D:\\Projects-D\\pepsi-final2\\document_generator\\test_data\\Facility_Managers_2024.xlsx, doc_path: facility_managers_2024.xlsx, filename: None", "module": "ingestion_engine", "function": "_handle_existing_file", "line": 166}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "root", "message": "No existing vectors found for file: D:\\Projects-D\\pepsi-final2\\document_generator\\test_data\\Facility_Managers_2024.xlsx", "module": "ingestion_engine", "function": "_handle_existing_file", "line": 217}
{"timestamp": "2025-06-16 11:21:01,820", "level": "INFO", "logger": "ExcelProcessor", "message": "Processing Excel file: D:\\Projects-D\\pepsi-final2\\document_generator\\test_data\\Facility_Managers_2024.xlsx", "module": "excel_processor", "function": "process", "line": 99}
{"timestamp": "2025-06-16 11:21:01,844", "level": "INFO", "logger": "ExcelProcessor", "message": "Successfully processed Excel file with 3 sheets, 0 embedded objects, 0 images", "module": "excel_processor", "function": "process", "line": 160}
{"timestamp": "2025-06-16 11:21:02,334", "level": "INFO", "logger": "root", "message": "Saved FAISS index and metadata", "module": "faiss_store", "function": "save_index", "line": 493}
{"timestamp": "2025-06-16 11:21:02,335", "level": "INFO", "logger": "root", "message": "Added 1 vectors to FAISS index", "module": "faiss_store", "function": "add_vectors", "line": 226}
{"timestamp": "2025-06-16 11:21:02,335", "level": "INFO", "logger": "root", "message": "Successfully ingested file: D:\\Projects-D\\pepsi-final2\\document_generator\\test_data\\Facility_Managers_2024.xlsx (1 chunks)", "module": "ingestion_engine", "function": "ingest_file", "line": 135}
{"timestamp": "2025-06-16 11:23:17,038", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-06-16 11:23:17,046", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-06-16 11:23:17,047", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-06-16 11:23:17,047", "level": "WARNING", "logger": "root", "message": "LLM API key not configured. Some features may not work.", "module": "system_init", "function": "validate_system_requirements", "line": 125}
{"timestamp": "2025-06-16 11:23:17,050", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-06-16 11:23:17,052", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-06-16 11:23:17,052", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-06-16 11:23:17,095", "level": "INFO", "logger": "root", "message": "Loaded existing FAISS index with 1 vectors", "module": "faiss_store", "function": "_initialize_index", "line": 51}
{"timestamp": "2025-06-16 11:23:17,097", "level": "INFO", "logger": "root", "message": "FAISS store initialized with dimension 384", "module": "faiss_store", "function": "__init__", "line": 41}
{"timestamp": "2025-06-16 11:23:17,099", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-06-16 11:23:18,342", "level": "INFO", "logger": "root", "message": "Loaded SentenceTransformer model: sentence-transformers/all-MiniLM-L6-v2", "module": "embedder", "function": "_load_model", "line": 47}
{"timestamp": "2025-06-16 11:23:18,342", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: sentence-transformers", "module": "embedder", "function": "__init__", "line": 153}
{"timestamp": "2025-06-16 11:23:18,342", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-06-16 11:23:18,355", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['chunker', 'llm_client', 'config_manager', 'faiss_store', 'query_engine', 'ingestion_engine', 'servicenow_integration', 'json_store', 'embedder', 'metadata_store', 'query_enhancer', 'reranker', 'log_store']}", "module": "system_init", "function": "log_system_info", "line": 313}
{"timestamp": "2025-06-16 11:23:18,534", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-06-16 11:23:18,534", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-06-16 11:23:18,762", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 59}
{"timestamp": "2025-06-16 11:23:18,762", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking: size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 38}
{"timestamp": "2025-06-16 11:23:18,762", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 84}
{"timestamp": "2025-06-16 11:23:18,763", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Excel processor registered without Azure AI support", "module": "ingestion_engine", "function": "_register_excel_processor", "line": 55}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized", "module": "ingestion_engine", "function": "__init__", "line": 29}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Looking for existing vectors for file_path: D:\\Projects-D\\pepsi-final2\\document_generator\\test_data\\Facility_Managers_2024.xlsx, doc_path: facility_managers_2024.xlsx, filename: None", "module": "ingestion_engine", "function": "_handle_existing_file", "line": 166}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Found matching vector 0: doc_path match: facility_managers_2024.xlsx", "module": "ingestion_engine", "function": "_handle_existing_file", "line": 208}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Found 1 existing vectors", "module": "ingestion_engine", "function": "_handle_existing_file", "line": 211}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Saved FAISS index and metadata", "module": "faiss_store", "function": "save_index", "line": 493}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Cleaned up 1 deleted vectors", "module": "faiss_store", "function": "_cleanup_deleted_vectors", "line": 106}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Deleted 1 vectors", "module": "faiss_store", "function": "delete_vectors", "line": 414}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "root", "message": "Deleted 1 old vectors for file update", "module": "ingestion_engine", "function": "_handle_existing_file", "line": 214}
{"timestamp": "2025-06-16 11:23:18,764", "level": "INFO", "logger": "ExcelProcessor", "message": "Processing Excel file: D:\\Projects-D\\pepsi-final2\\document_generator\\test_data\\Facility_Managers_2024.xlsx", "module": "excel_processor", "function": "process", "line": 99}
{"timestamp": "2025-06-16 11:23:18,780", "level": "INFO", "logger": "ExcelProcessor", "message": "Successfully processed Excel file with 3 sheets, 0 embedded objects, 0 images", "module": "excel_processor", "function": "process", "line": 160}
{"timestamp": "2025-06-16 11:23:18,872", "level": "INFO", "logger": "root", "message": "Saved FAISS index and metadata", "module": "faiss_store", "function": "save_index", "line": 493}
{"timestamp": "2025-06-16 11:23:18,873", "level": "INFO", "logger": "root", "message": "Added 1 vectors to FAISS index", "module": "faiss_store", "function": "add_vectors", "line": 226}
{"timestamp": "2025-06-16 11:23:18,874", "level": "INFO", "logger": "root", "message": "Successfully ingested file: D:\\Projects-D\\pepsi-final2\\document_generator\\test_data\\Facility_Managers_2024.xlsx (1 chunks)", "module": "ingestion_engine", "function": "ingest_file", "line": 135}
{"timestamp": "2025-06-16 11:23:18,874", "level": "INFO", "logger": "root", "message": "Replaced 1 old vectors for updated file", "module": "ingestion_engine", "function": "ingest_file", "line": 137}
{"timestamp": "2025-06-16 11:26:34,619", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-06-16 11:26:34,621", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-06-16 11:26:34,621", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-06-16 11:26:34,621", "level": "WARNING", "logger": "root", "message": "LLM API key not configured. Some features may not work.", "module": "system_init", "function": "validate_system_requirements", "line": 125}
{"timestamp": "2025-06-16 11:26:34,623", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-06-16 11:26:34,625", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-06-16 11:26:34,625", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-06-16 11:26:34,636", "level": "INFO", "logger": "root", "message": "Loaded existing FAISS index with 1 vectors", "module": "faiss_store", "function": "_initialize_index", "line": 51}
{"timestamp": "2025-06-16 11:26:34,636", "level": "INFO", "logger": "root", "message": "FAISS store initialized with dimension 384", "module": "faiss_store", "function": "__init__", "line": 41}
{"timestamp": "2025-06-16 11:26:34,639", "level": "ERROR", "logger": "root", "message": "\u274c RAG System initialization failed: Unsupported embedding provider: azure", "module": "system_init", "function": "initialize_system", "line": 293}
{"timestamp": "2025-06-16 11:30:26,644", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-06-16 11:30:26,644", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-06-16 11:30:26,644", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-06-16 11:30:26,651", "level": "WARNING", "logger": "root", "message": "LLM API key not configured. Some features may not work.", "module": "system_init", "function": "validate_system_requirements", "line": 125}
{"timestamp": "2025-06-16 11:30:26,652", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-06-16 11:30:26,654", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-06-16 11:30:26,654", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-06-16 11:30:26,676", "level": "INFO", "logger": "root", "message": "Loaded existing FAISS index with 1 vectors", "module": "faiss_store", "function": "_initialize_index", "line": 51}
{"timestamp": "2025-06-16 11:30:26,677", "level": "INFO", "logger": "root", "message": "FAISS store initialized with dimension 1024", "module": "faiss_store", "function": "__init__", "line": 41}
{"timestamp": "2025-06-16 11:30:27,253", "level": "INFO", "logger": "root", "message": "Loaded Cohere client with model: embed-english-v3.0", "module": "embedder", "function": "_load_client", "line": 96}
{"timestamp": "2025-06-16 11:30:27,349", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 401 Unauthorized\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-06-16 11:30:27,396", "level": "ERROR", "logger": "root", "message": "\u274c RAG System initialization failed: Failed to initialize Cohere client: status_code: 401, body: {'message': 'invalid api token'}", "module": "system_init", "function": "initialize_system", "line": 293}
{"timestamp": "2025-06-16 11:40:16,142", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-06-16 11:40:16,145", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-06-16 11:40:16,145", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-06-16 11:40:16,146", "level": "WARNING", "logger": "root", "message": "LLM API key not configured. Some features may not work.", "module": "system_init", "function": "validate_system_requirements", "line": 125}
{"timestamp": "2025-06-16 11:40:16,147", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-06-16 11:40:16,149", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-06-16 11:40:16,149", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-06-16 11:40:16,164", "level": "INFO", "logger": "root", "message": "Loaded existing FAISS index with 1 vectors", "module": "faiss_store", "function": "_initialize_index", "line": 51}
{"timestamp": "2025-06-16 11:40:16,165", "level": "INFO", "logger": "root", "message": "FAISS store initialized with dimension 1024", "module": "faiss_store", "function": "__init__", "line": 41}
{"timestamp": "2025-06-16 11:40:16,730", "level": "INFO", "logger": "root", "message": "Loaded Cohere client with model: embed-english-v3.0", "module": "embedder", "function": "_load_client", "line": 96}
{"timestamp": "2025-06-16 11:40:16,951", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.cohere.com/v1/embed \"HTTP/1.1 401 Unauthorized\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-06-16 11:40:16,954", "level": "ERROR", "logger": "root", "message": "\u274c RAG System initialization failed: Failed to initialize Cohere client: status_code: 401, body: {'message': 'invalid api token'}", "module": "system_init", "function": "initialize_system", "line": 293}
{"timestamp": "2025-07-01 07:15:01,783", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 07:15:01,808", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 07:15:01,810", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 07:15:01,812", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 07:15:01,823", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 07:15:01,825", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 07:15:03,156", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:15:03,170", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:15:03,173", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 07:15:03,174", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 07:15:03,535", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 07:15:03,537", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '1bad8219-564b-11f0-a409-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 07:15:03,612", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 07:15:02 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 07:15:03,616", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 07:15:03,616", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 07:15:03,947", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 07:15:03,950", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 07:15:03,951", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 07:15:03,952", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['ingestion_engine', 'ingestion_debugger', 'ingestion_verifier', 'llm_client', 'vector_store', 'chunker', 'reranker', 'embedder', 'config_manager', 'log_store', 'json_store', 'query_enhancer', 'query_engine', 'metadata_store', 'servicenow_integration', 'faiss_store', 'conversation_manager', 'verified_ingestion_engine']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 07:15:04,574", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s", "module": "model_memory_manager", "function": "__init__", "line": 95}
{"timestamp": "2025-07-01 07:15:04,575", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Loading model: semantic_chunker_all-MiniLM-L6-v2", "module": "model_memory_manager", "function": "get_model", "line": 117}
{"timestamp": "2025-07-01 07:15:04,575", "level": "INFO", "logger": "root", "message": "Loading sentence transformer: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "load_model", "line": 340}
{"timestamp": "2025-07-01 07:15:04,577", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-07-01 07:15:04,577", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-07-01 07:15:06,444", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.87s. Current memory: 515.52MB", "module": "model_memory_manager", "function": "get_model", "line": 144}
{"timestamp": "2025-07-01 07:15:06,445", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 345}
{"timestamp": "2025-07-01 07:15:06,445", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 47}
{"timestamp": "2025-07-01 07:15:06,448", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 07:15:06,448", "level": "INFO", "logger": "root", "message": "DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']", "module": "ingestion_engine", "function": "__init__", "line": 45}
{"timestamp": "2025-07-01 07:15:06,448", "level": "INFO", "logger": "root", "message": "Azure AI config added to processor config", "module": "ingestion_engine", "function": "__init__", "line": 54}
{"timestamp": "2025-07-01 07:15:06,450", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 07:15:06,450", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 07:15:06,450", "level": "INFO", "logger": "root", "message": "Extracted Azure AI config from general config", "module": "pdf_processor", "function": "create_pdf_processor", "line": 82}
{"timestamp": "2025-07-01 07:15:06,513", "level": "INFO", "logger": "rag_system.src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 07:15:06,513", "level": "INFO", "logger": "root", "message": "Azure AI client created successfully for PDF processing", "module": "pdf_processor", "function": "create_pdf_processor", "line": 93}
{"timestamp": "2025-07-01 07:15:06,513", "level": "INFO", "logger": "root", "message": "Azure CV endpoint: https://computervision1298.cognitiveservices.azure...", "module": "pdf_processor", "function": "create_pdf_processor", "line": 94}
{"timestamp": "2025-07-01 07:15:06,514", "level": "INFO", "logger": "root", "message": "Using EnhancedPDFProcessor with Azure AI integration", "module": "pdf_processor", "function": "create_pdf_processor", "line": 106}
{"timestamp": "2025-07-01 07:15:06,514", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 07:15:06,514", "level": "INFO", "logger": "WordProcessor", "message": "Word processor initialized", "module": "word_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 07:15:06,514", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: WordProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 07:15:06,515", "level": "INFO", "logger": "ImageProcessor", "message": "Image processor initialized", "module": "image_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 07:15:06,515", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ImageProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 07:15:06,515", "level": "INFO", "logger": "ServiceNowProcessor", "message": "ServiceNow processor initialized", "module": "servicenow_processor", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 07:15:06,515", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ServiceNowProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 07:15:06,515", "level": "INFO", "logger": "TextProcessor", "message": "Text processor initialized", "module": "text_processor", "function": "__init__", "line": 30}
{"timestamp": "2025-07-01 07:15:06,515", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: TextProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 07:15:06,518", "level": "INFO", "logger": "rag_system.src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 07:15:06,519", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 07:15:06,519", "level": "INFO", "logger": "root", "message": "Enhanced PDF Processor with Azure CV registered successfully", "module": "ingestion_engine", "function": "__init__", "line": 73}
{"timestamp": "2025-07-01 07:15:06,521", "level": "INFO", "logger": "root", "message": "Processor registry initialized with 7 processors", "module": "ingestion_engine", "function": "__init__", "line": 77}
{"timestamp": "2025-07-01 07:15:06,521", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized with managed metadata", "module": "ingestion_engine", "function": "__init__", "line": 89}
{"timestamp": "2025-07-01 07:15:06,584", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:15:06,586", "level": "INFO", "logger": "root", "message": "\ud83d\udd0d Using processor: TextProcessor for C:\\Users\\JAIDEE~2\\AppData\\Local\\Temp\\tmpvr5jvggn.txt", "module": "ingestion_engine", "function": "_extract_text", "line": 734}
{"timestamp": "2025-07-01 07:15:06,588", "level": "INFO", "logger": "root", "message": "\ud83d\udd0d Using processor: TextProcessor for C:\\Users\\JAIDEE~2\\AppData\\Local\\Temp\\tmpvr5jvggn.txt", "module": "ingestion_engine", "function": "_extract_text", "line": 746}
{"timestamp": "2025-07-01 07:15:06,589", "level": "INFO", "logger": "TextProcessor", "message": "Processing text file: C:\\Users\\JAIDEE~2\\AppData\\Local\\Temp\\tmpvr5jvggn.txt", "module": "text_processor", "function": "process", "line": 66}
{"timestamp": "2025-07-01 07:15:06,591", "level": "INFO", "logger": "root", "message": "\u2705 Processor succeeded, chunks: 1", "module": "ingestion_engine", "function": "_extract_text", "line": 754}
{"timestamp": "2025-07-01 07:15:06,591", "level": "INFO", "logger": "root", "message": "   Processing time: 0.00s", "module": "ingestion_engine", "function": "_extract_text", "line": 755}
{"timestamp": "2025-07-01 07:15:06,593", "level": "INFO", "logger": "root", "message": "Using 1 chunks from processor", "module": "ingestion_engine", "function": "ingest_file", "line": 326}
{"timestamp": "2025-07-01 07:15:06,610", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '584'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '1d8251f9-564b-11f0-9a5a-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 07:15:06,663", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 07:15:05 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 07:15:06,672", "level": "WARNING", "logger": "root", "message": "Flattening nested metadata structure - this should be avoided in future", "module": "metadata_manager", "function": "normalize", "line": 205}
{"timestamp": "2025-07-01 07:15:06,676", "level": "WARNING", "logger": "root", "message": "Skipping nested key 'file_size' - conflicts with top-level key", "module": "metadata_manager", "function": "normalize", "line": 213}
{"timestamp": "2025-07-01 07:15:06,678", "level": "WARNING", "logger": "root", "message": "Skipping nested key 'content_type' - conflicts with top-level key", "module": "metadata_manager", "function": "normalize", "line": 213}
{"timestamp": "2025-07-01 07:15:06,678", "level": "WARNING", "logger": "root", "message": "Skipping nested key 'original_filename' - conflicts with top-level key", "module": "metadata_manager", "function": "normalize", "line": 213}
{"timestamp": "2025-07-01 07:15:06,679", "level": "WARNING", "logger": "root", "message": "Skipping nested key 'filename' - conflicts with top-level key", "module": "metadata_manager", "function": "normalize", "line": 213}
{"timestamp": "2025-07-01 07:15:06,679", "level": "WARNING", "logger": "root", "message": "Skipping nested key 'upload_source' - conflicts with top-level key", "module": "metadata_manager", "function": "normalize", "line": 213}
{"timestamp": "2025-07-01 07:15:06,679", "level": "WARNING", "logger": "root", "message": "Skipping nested key 'upload_timestamp' - conflicts with top-level key", "module": "metadata_manager", "function": "normalize", "line": 213}
{"timestamp": "2025-07-01 07:15:06,680", "level": "WARNING", "logger": "root", "message": "Skipping nested key 'description' - conflicts with top-level key", "module": "metadata_manager", "function": "normalize", "line": 213}
{"timestamp": "2025-07-01 07:15:06,680", "level": "INFO", "logger": "root", "message": "Removing conflicting key 'file_name', keeping 'filename'", "module": "metadata_manager", "function": "normalize", "line": 227}
{"timestamp": "2025-07-01 07:15:06,680", "level": "INFO", "logger": "root", "message": "Removing conflicting key 'content', keeping 'text'", "module": "metadata_manager", "function": "normalize", "line": 227}
{"timestamp": "2025-07-01 07:15:06,930", "level": "INFO", "logger": "httpx", "message": "HTTP Request: PUT http://localhost:6333/collections/rag_documents/points?wait=true \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:15:06,932", "level": "INFO", "logger": "root", "message": "Added 1 vectors to Qdrant", "module": "qdrant_store", "function": "add_vectors", "line": 123}
{"timestamp": "2025-07-01 07:15:06,933", "level": "INFO", "logger": "root", "message": "Successfully ingested file: C:\\Users\\JAIDEE~2\\AppData\\Local\\Temp\\tmpvr5jvggn.txt (1 chunks)", "module": "ingestion_engine", "function": "ingest_file", "line": 519}
{"timestamp": "2025-07-01 07:15:06,940", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:15:06,963", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Shutting down model memory manager...", "module": "model_memory_manager", "function": "shutdown", "line": 413}
{"timestamp": "2025-07-01 07:15:07,264", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 516.59MB", "module": "model_memory_manager", "function": "_unload_model", "line": 260}
{"timestamp": "2025-07-01 07:15:07,566", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model memory manager shutdown complete", "module": "model_memory_manager", "function": "shutdown", "line": 426}
{"timestamp": "2025-07-01 07:15:58,646", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 07:15:58,655", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 07:15:58,656", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 07:15:58,657", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 07:15:58,668", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 07:15:58,669", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 07:15:59,902", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:15:59,908", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:15:59,910", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 07:15:59,910", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 07:16:00,244", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 07:16:00,245", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '3d7a556d-564b-11f0-93ac-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 07:16:00,391", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 07:16:00 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 07:16:00,397", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 07:16:00,397", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 07:16:00,678", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 07:16:00,680", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 07:16:00,682", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 07:16:00,683", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['ingestion_verifier', 'embedder', 'chunker', 'servicenow_integration', 'config_manager', 'json_store', 'reranker', 'conversation_manager', 'verified_ingestion_engine', 'ingestion_debugger', 'ingestion_engine', 'log_store', 'metadata_store', 'faiss_store', 'llm_client', 'vector_store', 'query_engine', 'query_enhancer']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 07:16:00,693", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:49,729", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 07:36:49,734", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 07:36:49,734", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 07:36:49,736", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 07:36:49,738", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 07:36:49,738", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 07:36:51,361", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:51,368", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:51,370", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 07:36:51,372", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 07:36:51,778", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 07:36:51,780", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '27733afd-564e-11f0-b3b7-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 07:36:51,942", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 07:36:51 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 07:36:51,949", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 07:36:51,949", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 07:36:52,311", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 07:36:52,313", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 07:36:52,324", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 07:36:52,325", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['chunker', 'faiss_store', 'vector_store', 'verified_ingestion_engine', 'ingestion_verifier', 'log_store', 'embedder', 'query_engine', 'llm_client', 'config_manager', 'ingestion_debugger', 'servicenow_integration', 'json_store', 'reranker', 'query_enhancer', 'metadata_store', 'conversation_manager', 'ingestion_engine']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 07:36:52,819", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Initialized Memory checkpointer for state persistence", "module": "conversation_graph", "function": "__init__", "line": 36}
{"timestamp": "2025-07-01 07:36:52,838", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Conversation graph compiled successfully with Memory state persistence", "module": "conversation_graph", "function": "_build_graph", "line": 104}
{"timestamp": "2025-07-01 07:36:52,838", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "ConversationGraph initialized with state persistence", "module": "conversation_graph", "function": "__init__", "line": 41}
{"timestamp": "2025-07-01 07:36:52,839", "level": "WARNING", "logger": "rag_system.src.conversation.conversation_graph", "message": "Checkpointer not available or missing client attribute", "module": "conversation_graph", "function": "list_conversation_threads", "line": 309}
{"timestamp": "2025-07-01 07:36:52,840", "level": "INFO", "logger": "rag_system.src.conversation.conversation_manager", "message": "Conversation cleanup completed: 0 old conversations removed", "module": "conversation_manager", "function": "cleanup_old_conversations", "line": 173}
{"timestamp": "2025-07-01 07:36:52,840", "level": "INFO", "logger": "rag_system.src.conversation.conversation_manager", "message": "Initial conversation cleanup: 0 conversations removed", "module": "conversation_manager", "function": "__init__", "line": 34}
{"timestamp": "2025-07-01 07:36:52,840", "level": "INFO", "logger": "rag_system.src.conversation.conversation_manager", "message": "ConversationManager initialized with LangGraph state persistence and memory management", "module": "conversation_manager", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 07:36:52,842", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Created new conversation state for thread 6de94747-1d16-412b-a0e2-38da559ebc2c", "module": "conversation_graph", "function": "_get_or_create_state", "line": 259}
{"timestamp": "2025-07-01 07:36:52,871", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 07:36:52,872", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 07:36:52,873", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Routing after understanding - intent: None, turn: 1", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 07:36:52,874", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 07:36:52,874", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 07:36:52,886", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 07:36:52,888", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 354}
{"timestamp": "2025-07-01 07:36:52,889", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 07:36:52,889", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=None, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 07:36:52,891", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 6de94747-1d16-412b-a0e2-38da559ebc2c, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 07:36:52,891", "level": "INFO", "logger": "rag_system.src.conversation.conversation_manager", "message": "Started/retrieved conversation for thread: 6de94747-1d16-412b-a0e2-38da559ebc2c", "module": "conversation_manager", "function": "start_conversation", "line": 53}
{"timestamp": "2025-07-01 07:36:52,892", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 6de94747-1d16-412b-a0e2-38da559ebc2c", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 07:36:52,895", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 07:36:52,895", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 07:36:52,898", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 424}
{"timestamp": "2025-07-01 07:36:52,898", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Building contextual query from: 'How many routers are in the system?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 07:36:52,899", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Enhanced query to: 'How many routers are in the system? (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 07:36:52,899", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'How many routers are in the system? (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 07:36:52,900", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'routers', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 07:36:52,901", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 07:36:52,901", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 07:36:52,902", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 3", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 07:36:52,903", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 07:36:52,904", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 07:36:52,904", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'How many routers are in the system? (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 07:36:52,905", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 07:36:52,959", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:52,966", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:52,984", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,012", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,018", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,020", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Search strategy 2: Original query: 'How many routers are in the system?'", "module": "conversation_nodes", "function": "search_knowledge", "line": 182}
{"timestamp": "2025-07-01 07:36:53,020", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 07:36:53,025", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,030", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,046", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,053", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,061", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,064", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 07:36:53,072", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'aggregation_results', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 07:36:53,072", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "No sources found in search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 262}
{"timestamp": "2025-07-01 07:36:53,073", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 07:36:53,074", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 07:36:53,074", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Using query engine response directly", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 513}
{"timestamp": "2025-07-01 07:36:53,076", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 337}
{"timestamp": "2025-07-01 07:36:53,076", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 722}
{"timestamp": "2025-07-01 07:36:53,076", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 738}
{"timestamp": "2025-07-01 07:36:53,077", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Built context info with length: 331", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 747}
{"timestamp": "2025-07-01 07:36:53,077", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 773}
{"timestamp": "2025-07-01 07:36:53,373", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 07:36:53,375", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "LLM response length: 330", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 775}
{"timestamp": "2025-07-01 07:36:53,375", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Parsed 3 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 779}
{"timestamp": "2025-07-01 07:36:53,377", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Generated 3 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 782}
{"timestamp": "2025-07-01 07:36:53,377", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 3 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 339}
{"timestamp": "2025-07-01 07:36:53,377", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 354}
{"timestamp": "2025-07-01 07:36:53,377", "level": "INFO", "logger": "rag_system.src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 07:36:53,378", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=4", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 07:36:53,384", "level": "INFO", "logger": "rag_system.src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 6de94747-1d16-412b-a0e2-38da559ebc2c, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 07:36:53,384", "level": "INFO", "logger": "rag_system.src.conversation.conversation_manager", "message": "Processed message for thread 6de94747-1d16-412b-a0e2-38da559ebc2c", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 10:40:27,840", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 10:40:27,844", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 10:40:27,844", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 10:40:27,845", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 10:40:27,848", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 10:40:27,848", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 10:40:29,175", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 10:40:29,193", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 10:40:29,195", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 10:40:29,195", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 10:40:29,202", "level": "ERROR", "logger": "root", "message": "\u274c RAG System initialization failed: Azure endpoint not provided. Set AZURE_EMBEDDINGS_ENDPOINT environment variable.", "module": "system_init", "function": "initialize_system", "line": 299}
{"timestamp": "2025-07-01 10:40:52,547", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 10:40:52,560", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 10:40:52,561", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 10:40:52,563", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 10:40:52,564", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 10:40:52,565", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 10:40:53,735", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 10:40:53,740", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 10:40:53,743", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 10:40:53,744", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 10:40:54,090", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 10:40:54,090", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': 'dd2e5a12-5667-11f0-9f65-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 10:40:54,200", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 10:40:53 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 10:40:54,205", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 10:40:54,206", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 10:40:54,514", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 10:40:54,516", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 10:40:54,518", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 10:40:54,518", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['reranker', 'chunker', 'metadata_store', 'config_manager', 'query_engine', 'faiss_store', 'verified_ingestion_engine', 'log_store', 'vector_store', 'llm_client', 'conversation_manager', 'json_store', 'query_enhancer', 'servicenow_integration', 'ingestion_verifier', 'embedder', 'ingestion_debugger', 'ingestion_engine']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 11:42:38,658", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 11:42:38,661", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 11:42:38,662", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 11:42:38,664", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 11:42:38,667", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 11:42:38,668", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 11:42:40,137", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 11:42:40,143", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 11:42:40,148", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 11:42:40,150", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 11:42:40,537", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 11:42:40,539", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '7e6585e1-5670-11f0-8566-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 11:42:40,669", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 11:42:40 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 11:42:40,674", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 11:42:40,679", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 11:42:41,102", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 11:42:41,104", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 11:42:41,106", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 11:42:41,107", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['query_engine', 'metadata_store', 'json_store', 'servicenow_integration', 'log_store', 'vector_store', 'verified_ingestion_engine', 'chunker', 'ingestion_engine', 'ingestion_debugger', 'llm_client', 'faiss_store', 'conversation_manager', 'embedder', 'ingestion_verifier', 'config_manager', 'query_enhancer', 'reranker']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 11:42:41,881", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s", "module": "model_memory_manager", "function": "__init__", "line": 95}
{"timestamp": "2025-07-01 11:42:41,881", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Loading model: semantic_chunker_all-MiniLM-L6-v2", "module": "model_memory_manager", "function": "get_model", "line": 117}
{"timestamp": "2025-07-01 11:42:41,882", "level": "INFO", "logger": "root", "message": "Loading sentence transformer: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "load_model", "line": 340}
{"timestamp": "2025-07-01 11:42:41,882", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-07-01 11:42:41,883", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-07-01 11:42:44,452", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 loaded in 2.57s. Current memory: 516.89MB", "module": "model_memory_manager", "function": "get_model", "line": 144}
{"timestamp": "2025-07-01 11:42:44,452", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 345}
{"timestamp": "2025-07-01 11:42:44,452", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 47}
{"timestamp": "2025-07-01 11:42:44,469", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 11:42:44,469", "level": "INFO", "logger": "root", "message": "DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']", "module": "ingestion_engine", "function": "__init__", "line": 45}
{"timestamp": "2025-07-01 11:42:44,470", "level": "INFO", "logger": "root", "message": "Azure AI config added to processor config", "module": "ingestion_engine", "function": "__init__", "line": 54}
{"timestamp": "2025-07-01 11:42:44,470", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 11:42:44,470", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:42:44,471", "level": "INFO", "logger": "root", "message": "Extracted Azure AI config from general config", "module": "pdf_processor", "function": "create_pdf_processor", "line": 82}
{"timestamp": "2025-07-01 11:42:44,578", "level": "INFO", "logger": "rag_system.src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 11:42:44,578", "level": "INFO", "logger": "root", "message": "Azure AI client created successfully for PDF processing", "module": "pdf_processor", "function": "create_pdf_processor", "line": 93}
{"timestamp": "2025-07-01 11:42:44,578", "level": "INFO", "logger": "root", "message": "Azure CV endpoint: https://computervision1298.cognitiveservices.azure...", "module": "pdf_processor", "function": "create_pdf_processor", "line": 94}
{"timestamp": "2025-07-01 11:42:44,579", "level": "INFO", "logger": "root", "message": "Using EnhancedPDFProcessor with Azure AI integration", "module": "pdf_processor", "function": "create_pdf_processor", "line": 106}
{"timestamp": "2025-07-01 11:42:44,579", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:42:44,579", "level": "INFO", "logger": "WordProcessor", "message": "Word processor initialized", "module": "word_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 11:42:44,585", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: WordProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:42:44,585", "level": "INFO", "logger": "ImageProcessor", "message": "Image processor initialized", "module": "image_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 11:42:44,589", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ImageProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:42:44,589", "level": "INFO", "logger": "ServiceNowProcessor", "message": "ServiceNow processor initialized", "module": "servicenow_processor", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 11:42:44,590", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ServiceNowProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:42:44,591", "level": "INFO", "logger": "TextProcessor", "message": "Text processor initialized", "module": "text_processor", "function": "__init__", "line": 30}
{"timestamp": "2025-07-01 11:42:44,591", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: TextProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:42:44,595", "level": "INFO", "logger": "rag_system.src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 11:42:44,596", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:42:44,596", "level": "INFO", "logger": "root", "message": "Enhanced PDF Processor with Azure CV registered successfully", "module": "ingestion_engine", "function": "__init__", "line": 73}
{"timestamp": "2025-07-01 11:42:44,596", "level": "INFO", "logger": "root", "message": "Processor registry initialized with 7 processors", "module": "ingestion_engine", "function": "__init__", "line": 77}
{"timestamp": "2025-07-01 11:42:44,596", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized with managed metadata", "module": "ingestion_engine", "function": "__init__", "line": 89}
{"timestamp": "2025-07-01 11:42:44,604", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '236'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '80d1c7df-5670-11f0-aaf7-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 11:42:44,696", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 11:42:44 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 11:42:44,704", "level": "INFO", "logger": "root", "message": "Removing conflicting key 'content', keeping 'text'", "module": "metadata_manager", "function": "normalize", "line": 227}
{"timestamp": "2025-07-01 11:42:44,731", "level": "INFO", "logger": "httpx", "message": "HTTP Request: PUT http://localhost:6333/collections/rag_documents/points?wait=true \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 11:42:44,733", "level": "INFO", "logger": "root", "message": "Added 1 vectors to Qdrant", "module": "qdrant_store", "function": "add_vectors", "line": 123}
{"timestamp": "2025-07-01 11:42:44,734", "level": "INFO", "logger": "root", "message": "Successfully ingested text content (1 chunks)", "module": "ingestion_engine", "function": "ingest_text", "line": 637}
{"timestamp": "2025-07-01 11:42:44,735", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Shutting down model memory manager...", "module": "model_memory_manager", "function": "shutdown", "line": 413}
{"timestamp": "2025-07-01 11:42:45,076", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 517.94MB", "module": "model_memory_manager", "function": "_unload_model", "line": 260}
{"timestamp": "2025-07-01 11:42:45,405", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model memory manager shutdown complete", "module": "model_memory_manager", "function": "shutdown", "line": 426}
{"timestamp": "2025-07-01 11:44:10,539", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 11:44:10,542", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 11:44:10,544", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 11:44:10,545", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 11:44:10,556", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 11:44:10,557", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 11:44:11,991", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 11:44:12,002", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 11:44:12,003", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 11:44:12,004", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 11:44:12,442", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 11:44:12,444", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': 'b52d2b07-5670-11f0-a424-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 11:44:12,575", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 11:44:11 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 11:44:12,580", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 11:44:12,580", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 11:44:12,930", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 11:44:12,932", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 11:44:12,936", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 11:44:14,264", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['metadata_store', 'embedder', 'reranker', 'chunker', 'query_enhancer', 'log_store', 'ingestion_debugger', 'json_store', 'servicenow_integration', 'ingestion_engine', 'faiss_store', 'vector_store', 'conversation_manager', 'query_engine', 'llm_client', 'ingestion_verifier', 'config_manager', 'verified_ingestion_engine']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 11:44:14,891", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s", "module": "model_memory_manager", "function": "__init__", "line": 95}
{"timestamp": "2025-07-01 11:44:14,892", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Loading model: semantic_chunker_all-MiniLM-L6-v2", "module": "model_memory_manager", "function": "get_model", "line": 117}
{"timestamp": "2025-07-01 11:44:14,892", "level": "INFO", "logger": "root", "message": "Loading sentence transformer: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "load_model", "line": 340}
{"timestamp": "2025-07-01 11:44:14,894", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-07-01 11:44:14,894", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-07-01 11:44:16,394", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.50s. Current memory: 516.59MB", "module": "model_memory_manager", "function": "get_model", "line": 144}
{"timestamp": "2025-07-01 11:44:16,400", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 345}
{"timestamp": "2025-07-01 11:44:16,400", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 47}
{"timestamp": "2025-07-01 11:44:16,405", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 11:44:16,405", "level": "INFO", "logger": "root", "message": "DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']", "module": "ingestion_engine", "function": "__init__", "line": 45}
{"timestamp": "2025-07-01 11:44:16,406", "level": "INFO", "logger": "root", "message": "Azure AI config added to processor config", "module": "ingestion_engine", "function": "__init__", "line": 54}
{"timestamp": "2025-07-01 11:44:16,406", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 11:44:16,406", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:44:16,407", "level": "INFO", "logger": "root", "message": "Extracted Azure AI config from general config", "module": "pdf_processor", "function": "create_pdf_processor", "line": 82}
{"timestamp": "2025-07-01 11:44:16,475", "level": "INFO", "logger": "rag_system.src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 11:44:16,477", "level": "INFO", "logger": "root", "message": "Azure AI client created successfully for PDF processing", "module": "pdf_processor", "function": "create_pdf_processor", "line": 93}
{"timestamp": "2025-07-01 11:44:16,479", "level": "INFO", "logger": "root", "message": "Azure CV endpoint: https://computervision1298.cognitiveservices.azure...", "module": "pdf_processor", "function": "create_pdf_processor", "line": 94}
{"timestamp": "2025-07-01 11:44:16,479", "level": "INFO", "logger": "root", "message": "Using EnhancedPDFProcessor with Azure AI integration", "module": "pdf_processor", "function": "create_pdf_processor", "line": 106}
{"timestamp": "2025-07-01 11:44:16,479", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:44:16,480", "level": "INFO", "logger": "WordProcessor", "message": "Word processor initialized", "module": "word_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 11:44:16,480", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: WordProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:44:16,480", "level": "INFO", "logger": "ImageProcessor", "message": "Image processor initialized", "module": "image_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 11:44:16,480", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ImageProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:44:16,481", "level": "INFO", "logger": "ServiceNowProcessor", "message": "ServiceNow processor initialized", "module": "servicenow_processor", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 11:44:16,481", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: ServiceNowProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:44:16,481", "level": "INFO", "logger": "TextProcessor", "message": "Text processor initialized", "module": "text_processor", "function": "__init__", "line": 30}
{"timestamp": "2025-07-01 11:44:16,482", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: TextProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:44:16,485", "level": "INFO", "logger": "rag_system.src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 11:44:16,485", "level": "INFO", "logger": "rag_system.src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 11:44:16,486", "level": "INFO", "logger": "root", "message": "Enhanced PDF Processor with Azure CV registered successfully", "module": "ingestion_engine", "function": "__init__", "line": 73}
{"timestamp": "2025-07-01 11:44:16,486", "level": "INFO", "logger": "root", "message": "Processor registry initialized with 7 processors", "module": "ingestion_engine", "function": "__init__", "line": 77}
{"timestamp": "2025-07-01 11:44:16,486", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized with managed metadata", "module": "ingestion_engine", "function": "__init__", "line": 89}
{"timestamp": "2025-07-01 11:44:16,487", "level": "INFO", "logger": "root", "message": "\ud83d\udd0d Detected JSON content in ingest_text, attempting processor route", "module": "ingestion_engine", "function": "ingest_text", "line": 562}
{"timestamp": "2025-07-01 11:44:16,488", "level": "INFO", "logger": "root", "message": "\ud83c\udfaf Detected ServiceNow JSON, using processor", "module": "ingestion_engine", "function": "ingest_text", "line": 586}
{"timestamp": "2025-07-01 11:44:16,494", "level": "INFO", "logger": "root", "message": "\ud83d\udce6 Using processor: ServiceNowProcessor for JSON content", "module": "ingestion_engine", "function": "ingest_text", "line": 598}
{"timestamp": "2025-07-01 11:44:16,495", "level": "INFO", "logger": "ServiceNowProcessor", "message": "\ud83c\udfab Starting ServiceNow/JSON processing: C:\\Users\\JAIDEE~2\\AppData\\Local\\Temp\\tmplfdedc0j.json", "module": "servicenow_processor", "function": "process", "line": 49}
{"timestamp": "2025-07-01 11:44:16,495", "level": "INFO", "logger": "ServiceNowProcessor", "message": "\ud83c\udfab File type: JSON | File size: 186 bytes", "module": "servicenow_processor", "function": "process", "line": 50}
{"timestamp": "2025-07-01 11:44:16,496", "level": "INFO", "logger": "ServiceNowProcessor", "message": "\ud83c\udfab Processor: ServiceNowProcessor", "module": "servicenow_processor", "function": "process", "line": 51}
{"timestamp": "2025-07-01 11:44:16,505", "level": "INFO", "logger": "ServiceNowProcessor", "message": "\ud83c\udfab JSON Structure Analysis:", "module": "servicenow_processor", "function": "process", "line": 70}
{"timestamp": "2025-07-01 11:44:16,505", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - Data type: dict", "module": "servicenow_processor", "function": "process", "line": 71}
{"timestamp": "2025-07-01 11:44:16,506", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - Total records: 1", "module": "servicenow_processor", "function": "process", "line": 72}
{"timestamp": "2025-07-01 11:44:16,506", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - Structure depth: 3", "module": "servicenow_processor", "function": "process", "line": 73}
{"timestamp": "2025-07-01 11:44:16,506", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - Field count: 1", "module": "servicenow_processor", "function": "process", "line": 74}
{"timestamp": "2025-07-01 11:44:16,507", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - ServiceNow detected: No", "module": "servicenow_processor", "function": "process", "line": 75}
{"timestamp": "2025-07-01 11:44:16,507", "level": "INFO", "logger": "ServiceNowProcessor", "message": "\u2705 ServiceNow/JSON processing completed in 0.01s", "module": "servicenow_processor", "function": "process", "line": 109}
{"timestamp": "2025-07-01 11:44:16,507", "level": "INFO", "logger": "ServiceNowProcessor", "message": "\ud83c\udfab Final Result:", "module": "servicenow_processor", "function": "process", "line": 110}
{"timestamp": "2025-07-01 11:44:16,508", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - Records processed: 1", "module": "servicenow_processor", "function": "process", "line": 111}
{"timestamp": "2025-07-01 11:44:16,508", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - Chunks created: 1", "module": "servicenow_processor", "function": "process", "line": 112}
{"timestamp": "2025-07-01 11:44:16,509", "level": "INFO", "logger": "ServiceNowProcessor", "message": "  - Total text length: 131 chars", "module": "servicenow_processor", "function": "process", "line": 113}
{"timestamp": "2025-07-01 11:44:16,510", "level": "INFO", "logger": "root", "message": "\u2705 Processor created 1 logical chunks", "module": "ingestion_engine", "function": "ingest_text", "line": 604}
{"timestamp": "2025-07-01 11:44:16,530", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '224'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': 'b79c83ac-5670-11f0-a5f3-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 11:44:16,631", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 11:44:15 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 11:44:16,636", "level": "INFO", "logger": "root", "message": "Removing conflicting key 'content', keeping 'text'", "module": "metadata_manager", "function": "normalize", "line": 227}
{"timestamp": "2025-07-01 11:44:16,754", "level": "INFO", "logger": "httpx", "message": "HTTP Request: PUT http://localhost:6333/collections/rag_documents/points?wait=true \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 11:44:16,756", "level": "INFO", "logger": "root", "message": "Added 1 vectors to Qdrant", "module": "qdrant_store", "function": "add_vectors", "line": 123}
{"timestamp": "2025-07-01 11:44:16,756", "level": "INFO", "logger": "root", "message": "\u2705 Successfully processed JSON content with ServiceNowProcessor (1 logical chunks)", "module": "ingestion_engine", "function": "ingest_text", "line": 665}
{"timestamp": "2025-07-01 11:44:16,758", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Shutting down model memory manager...", "module": "model_memory_manager", "function": "shutdown", "line": 413}
{"timestamp": "2025-07-01 11:44:17,129", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 517.66MB", "module": "model_memory_manager", "function": "_unload_model", "line": 260}
{"timestamp": "2025-07-01 11:44:17,464", "level": "INFO", "logger": "rag_system.src.core.model_memory_manager", "message": "Model memory manager shutdown complete", "module": "model_memory_manager", "function": "shutdown", "line": 426}
{"timestamp": "2025-07-01 15:13:54,005", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 15:13:54,019", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 15:13:54,020", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 15:13:54,021", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 15:13:54,024", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 15:13:54,025", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 15:13:55,628", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:13:55,644", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:13:55,648", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 15:13:55,650", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 15:13:56,096", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 15:13:56,097", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '019e336c-568e-11f0-ab41-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:13:56,241", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:13:55 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:13:56,248", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 15:13:56,248", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 15:13:56,630", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 15:13:56,630", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 15:13:56,643", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 15:13:56,644", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['ingestion_debugger', 'query_enhancer', 'json_store', 'faiss_store', 'verified_ingestion_engine', 'ingestion_verifier', 'llm_client', 'reranker', 'chunker', 'query_engine', 'log_store', 'servicenow_integration', 'metadata_store', 'conversation_manager', 'ingestion_engine', 'embedder', 'vector_store', 'config_manager']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 15:13:56,648", "level": "INFO", "logger": "root", "message": "Setting up monitoring...", "module": "main", "function": "main", "line": 77}
{"timestamp": "2025-07-01 15:13:56,649", "level": "INFO", "logger": "root", "message": "Monitoring setup completed", "module": "setup", "function": "setup_monitoring", "line": 18}
{"timestamp": "2025-07-01 15:13:56,650", "level": "INFO", "logger": "root", "message": "Initializing heartbeat monitor...", "module": "main", "function": "main", "line": 87}
{"timestamp": "2025-07-01 15:13:56,650", "level": "INFO", "logger": "src.monitoring.heartbeat_monitor", "message": "Heartbeat monitor initialized", "module": "heartbeat_monitor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 15:13:56,650", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor initialized successfully", "module": "main", "function": "main", "line": 89}
{"timestamp": "2025-07-01 15:13:56,653", "level": "INFO", "logger": "root", "message": "Initializing folder monitor...", "module": "main", "function": "main", "line": 98}
{"timestamp": "2025-07-01 15:13:56,655", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 15:13:56,655", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 15:13:56,655", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor initialized successfully", "module": "main", "function": "main", "line": 101}
{"timestamp": "2025-07-01 15:13:57,067", "level": "INFO", "logger": "root", "message": "Initializing enhanced folder monitor...", "module": "main", "function": "main", "line": 110}
{"timestamp": "2025-07-01 15:13:57,067", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 15:13:57,067", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 15:13:57,069", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Enhanced folder monitor initialized with pipeline verification", "module": "enhanced_folder_monitor", "function": "__init__", "line": 84}
{"timestamp": "2025-07-01 15:13:57,078", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitor initialized successfully", "module": "main", "function": "main", "line": 113}
{"timestamp": "2025-07-01 15:13:57,126", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor registered with API", "module": "main", "function": "main", "line": 122}
{"timestamp": "2025-07-01 15:13:57,127", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor registered with API", "module": "main", "function": "main", "line": 130}
{"timestamp": "2025-07-01 15:13:57,127", "level": "INFO", "logger": "root", "message": "Creating FastAPI application...", "module": "main", "function": "main", "line": 136}
{"timestamp": "2025-07-01 15:13:57,128", "level": "INFO", "logger": "root", "message": "ResourceManager initialized", "module": "resource_manager", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:13:57,128", "level": "INFO", "logger": "root", "message": "Initialized GlobalRAGSystem lifecycle manager", "module": "resource_manager", "function": "__init__", "line": 390}
{"timestamp": "2025-07-01 15:13:57,128", "level": "INFO", "logger": "root", "message": "Starting GlobalRAGSystem...", "module": "resource_manager", "function": "startup", "line": 398}
{"timestamp": "2025-07-01 15:13:57,130", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_main", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:13:57,130", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'main' with 4 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:13:57,149", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_io", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:13:57,149", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'io' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:13:57,150", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_compute", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:13:57,150", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'compute' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:13:57,150", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_background", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:13:57,151", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'background' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:13:57,151", "level": "INFO", "logger": "root", "message": "GlobalRAGSystem startup completed successfully", "module": "resource_manager", "function": "startup", "line": 411}
{"timestamp": "2025-07-01 15:13:57,152", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_api_operations", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:13:57,152", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'api_operations' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:13:57,152", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor set in API: <class 'src.monitoring.heartbeat_monitor.HeartbeatMonitor'>", "module": "main", "function": "create_api_app", "line": 227}
{"timestamp": "2025-07-01 15:13:57,153", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor set in API: <class 'src.monitoring.folder_monitor.FolderMonitor'>", "module": "main", "function": "create_api_app", "line": 240}
{"timestamp": "2025-07-01 15:13:57,278", "level": "INFO", "logger": "src.storage.feedback_store", "message": "Feedback store initialized at: data\\feedback_store.db", "module": "feedback_store", "function": "__init__", "line": 29}
{"timestamp": "2025-07-01 15:13:57,313", "level": "INFO", "logger": "root", "message": "\u2705 Management API routes registered", "module": "main", "function": "create_api_app", "line": 2341}
{"timestamp": "2025-07-01 15:13:57,336", "level": "WARNING", "logger": "root", "message": "\u26a0\ufe0f ServiceNow API routes not available: No module named 'rag_system'", "module": "main", "function": "create_api_app", "line": 2356}
{"timestamp": "2025-07-01 15:13:57,369", "level": "INFO", "logger": "root", "message": "\u2705 Conversation API routes registered", "module": "main", "function": "create_api_app", "line": 2365}
{"timestamp": "2025-07-01 15:13:57,372", "level": "INFO", "logger": "root", "message": "\u2705 Verification API routes registered", "module": "main", "function": "create_api_app", "line": 2373}
{"timestamp": "2025-07-01 15:13:57,375", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitoring API routes registered", "module": "main", "function": "create_api_app", "line": 2383}
{"timestamp": "2025-07-01 15:13:57,376", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 15:13:57,942", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s", "module": "model_memory_manager", "function": "__init__", "line": 95}
{"timestamp": "2025-07-01 15:13:57,942", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Loading model: semantic_chunker_all-MiniLM-L6-v2", "module": "model_memory_manager", "function": "get_model", "line": 117}
{"timestamp": "2025-07-01 15:13:57,943", "level": "INFO", "logger": "root", "message": "Loading sentence transformer: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "load_model", "line": 340}
{"timestamp": "2025-07-01 15:13:57,959", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-07-01 15:13:57,960", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-07-01 15:13:59,585", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.64s. Current memory: 520.36MB", "module": "model_memory_manager", "function": "get_model", "line": 144}
{"timestamp": "2025-07-01 15:13:59,588", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 345}
{"timestamp": "2025-07-01 15:13:59,588", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 47}
{"timestamp": "2025-07-01 15:13:59,599", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 15:13:59,607", "level": "INFO", "logger": "root", "message": "DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']", "module": "ingestion_engine", "function": "__init__", "line": 45}
{"timestamp": "2025-07-01 15:13:59,608", "level": "INFO", "logger": "root", "message": "Azure AI config added to processor config", "module": "ingestion_engine", "function": "__init__", "line": 54}
{"timestamp": "2025-07-01 15:13:59,608", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 15:13:59,609", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:13:59,609", "level": "INFO", "logger": "root", "message": "Extracted Azure AI config from general config", "module": "pdf_processor", "function": "create_pdf_processor", "line": 82}
{"timestamp": "2025-07-01 15:13:59,983", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 15:13:59,984", "level": "INFO", "logger": "root", "message": "Azure AI client created successfully for PDF processing", "module": "pdf_processor", "function": "create_pdf_processor", "line": 93}
{"timestamp": "2025-07-01 15:13:59,985", "level": "INFO", "logger": "root", "message": "Azure CV endpoint: https://computervision1298.cognitiveservices.azure...", "module": "pdf_processor", "function": "create_pdf_processor", "line": 94}
{"timestamp": "2025-07-01 15:13:59,985", "level": "INFO", "logger": "root", "message": "Using EnhancedPDFProcessor with Azure AI integration", "module": "pdf_processor", "function": "create_pdf_processor", "line": 106}
{"timestamp": "2025-07-01 15:13:59,985", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:13:59,986", "level": "INFO", "logger": "WordProcessor", "message": "Word processor initialized", "module": "word_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 15:13:59,987", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: WordProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:13:59,987", "level": "INFO", "logger": "ImageProcessor", "message": "Image processor initialized", "module": "image_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 15:13:59,987", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ImageProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:13:59,988", "level": "INFO", "logger": "ServiceNowProcessor", "message": "ServiceNow processor initialized", "module": "servicenow_processor", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:13:59,988", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ServiceNowProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:13:59,992", "level": "INFO", "logger": "TextProcessor", "message": "Text processor initialized", "module": "text_processor", "function": "__init__", "line": 30}
{"timestamp": "2025-07-01 15:13:59,992", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: TextProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:13:59,995", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 15:13:59,996", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:13:59,996", "level": "INFO", "logger": "root", "message": "Enhanced PDF Processor with Azure CV registered successfully", "module": "ingestion_engine", "function": "__init__", "line": 73}
{"timestamp": "2025-07-01 15:13:59,997", "level": "INFO", "logger": "root", "message": "Processor registry initialized with 7 processors", "module": "ingestion_engine", "function": "__init__", "line": 77}
{"timestamp": "2025-07-01 15:13:59,997", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized with managed metadata", "module": "ingestion_engine", "function": "__init__", "line": 89}
{"timestamp": "2025-07-01 15:13:59,998", "level": "INFO", "logger": "root", "message": "\u2705 Progress tracker initialized successfully", "module": "main", "function": "create_api_app", "line": 2440}
{"timestamp": "2025-07-01 15:14:00,000", "level": "INFO", "logger": "root", "message": "FastAPI application created", "module": "main", "function": "create_api_app", "line": 2637}
{"timestamp": "2025-07-01 15:14:00,000", "level": "INFO", "logger": "root", "message": "\u2705 Thread pool captured for cleanup", "module": "main", "function": "main", "line": 145}
{"timestamp": "2025-07-01 15:14:00,001", "level": "INFO", "logger": "root", "message": "FastAPI application created successfully", "module": "main", "function": "main", "line": 149}
{"timestamp": "2025-07-01 15:14:00,001", "level": "INFO", "logger": "root", "message": "Heartbeat monitoring disabled in config", "module": "main", "function": "main", "line": 163}
{"timestamp": "2025-07-01 15:14:00,003", "level": "INFO", "logger": "root", "message": "No folders configured for monitoring", "module": "main", "function": "main", "line": 184}
{"timestamp": "2025-07-01 15:14:00,003", "level": "INFO", "logger": "root", "message": "Starting server on 0.0.0.0:8000", "module": "main", "function": "main", "line": 195}
{"timestamp": "2025-07-01 15:14:00,050", "level": "INFO", "logger": "root", "message": "\ud83d\ude80 RAG System API starting up with managed resources...", "module": "main", "function": "startup_event", "line": 2393}
{"timestamp": "2025-07-01 15:14:00,051", "level": "INFO", "logger": "root", "message": "Registered resource: feedback_store", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:14:00,052", "level": "INFO", "logger": "root", "message": "\ud83d\uded1 RAG System API shutting down - cleaning up managed resources...", "module": "main", "function": "shutdown_event", "line": 2406}
{"timestamp": "2025-07-01 15:14:00,499", "level": "INFO", "logger": "root", "message": "Final system stats: {'app_name': 'GlobalRAGSystem', 'startup_complete': True, 'shutdown_complete': False, 'thread_pools': {'main': {'max_workers': 4, 'shutdown': False}, 'io': {'max_workers': 8, 'shutdown': False}, 'compute': {'max_workers': 2, 'shutdown': False}, 'background': {'max_workers': 2, 'shutdown': False}, 'api_operations': {'max_workers': 8, 'shutdown': False}}, 'resources': {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 521.48828125, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 14}, 'models': {'loaded_models': [], 'total_models': 0, 'memory_usage': {}, 'total_memory_mb': 0.0}, 'timestamp': 1751382840.4993231}", "module": "main", "function": "shutdown_event", "line": 2412}
{"timestamp": "2025-07-01 15:14:00,500", "level": "INFO", "logger": "root", "message": "\u2705 Managed resource cleanup initiated", "module": "main", "function": "shutdown_event", "line": 2416}
{"timestamp": "2025-07-01 15:14:00,514", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Shutting down model memory manager...", "module": "model_memory_manager", "function": "shutdown", "line": 413}
{"timestamp": "2025-07-01 15:14:00,829", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 521.51MB", "module": "model_memory_manager", "function": "_unload_model", "line": 260}
{"timestamp": "2025-07-01 15:14:01,173", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model memory manager shutdown complete", "module": "model_memory_manager", "function": "shutdown", "line": 426}
{"timestamp": "2025-07-01 15:14:01,174", "level": "INFO", "logger": "root", "message": "Shutting down GlobalRAGSystem...", "module": "resource_manager", "function": "shutdown", "line": 423}
{"timestamp": "2025-07-01 15:14:01,587", "level": "INFO", "logger": "root", "message": "Pre-shutdown stats: {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 521.51171875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}", "module": "resource_manager", "function": "shutdown", "line": 428}
{"timestamp": "2025-07-01 15:14:01,589", "level": "INFO", "logger": "root", "message": "Starting comprehensive resource cleanup...", "module": "resource_manager", "function": "cleanup_all", "line": 135}
{"timestamp": "2025-07-01 15:14:01,590", "level": "INFO", "logger": "root", "message": "Custom cleanup completed for feedback_store", "module": "resource_manager", "function": "cleanup_resource", "line": 70}
{"timestamp": "2025-07-01 15:14:01,590", "level": "INFO", "logger": "root", "message": "Cleaned up resource: feedback_store", "module": "resource_manager", "function": "cleanup_resource", "line": 82}
{"timestamp": "2025-07-01 15:14:01,590", "level": "INFO", "logger": "root", "message": "Shutting down thread pool 'api_operations' with 8 workers", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 208}
{"timestamp": "2025-07-01 15:14:01,590", "level": "INFO", "logger": "root", "message": "Thread pool 'api_operations' shutdown completed", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 212}
{"timestamp": "2025-07-01 15:14:01,591", "level": "INFO", "logger": "root", "message": "Custom cleanup completed for threadpool_api_operations", "module": "resource_manager", "function": "cleanup_resource", "line": 70}
{"timestamp": "2025-07-01 15:14:01,591", "level": "INFO", "logger": "root", "message": "Shutting down thread pool: threadpool_api_operations", "module": "resource_manager", "function": "_generic_cleanup", "line": 92}
{"timestamp": "2025-07-01 15:14:01,593", "level": "INFO", "logger": "root", "message": "Cleaned up resource: threadpool_api_operations", "module": "resource_manager", "function": "cleanup_resource", "line": 82}
{"timestamp": "2025-07-01 15:14:01,593", "level": "INFO", "logger": "root", "message": "Shutting down thread pool 'background' with 2 workers", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 208}
{"timestamp": "2025-07-01 15:14:01,593", "level": "INFO", "logger": "root", "message": "Thread pool 'background' shutdown completed", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 212}
{"timestamp": "2025-07-01 15:14:01,594", "level": "INFO", "logger": "root", "message": "Custom cleanup completed for threadpool_background", "module": "resource_manager", "function": "cleanup_resource", "line": 70}
{"timestamp": "2025-07-01 15:14:01,597", "level": "INFO", "logger": "root", "message": "Shutting down thread pool: threadpool_background", "module": "resource_manager", "function": "_generic_cleanup", "line": 92}
{"timestamp": "2025-07-01 15:14:01,598", "level": "INFO", "logger": "root", "message": "Cleaned up resource: threadpool_background", "module": "resource_manager", "function": "cleanup_resource", "line": 82}
{"timestamp": "2025-07-01 15:14:01,598", "level": "INFO", "logger": "root", "message": "Shutting down thread pool 'compute' with 2 workers", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 208}
{"timestamp": "2025-07-01 15:14:01,599", "level": "INFO", "logger": "root", "message": "Thread pool 'compute' shutdown completed", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 212}
{"timestamp": "2025-07-01 15:14:01,599", "level": "INFO", "logger": "root", "message": "Custom cleanup completed for threadpool_compute", "module": "resource_manager", "function": "cleanup_resource", "line": 70}
{"timestamp": "2025-07-01 15:14:01,600", "level": "INFO", "logger": "root", "message": "Shutting down thread pool: threadpool_compute", "module": "resource_manager", "function": "_generic_cleanup", "line": 92}
{"timestamp": "2025-07-01 15:14:01,606", "level": "INFO", "logger": "root", "message": "Cleaned up resource: threadpool_compute", "module": "resource_manager", "function": "cleanup_resource", "line": 82}
{"timestamp": "2025-07-01 15:14:01,606", "level": "INFO", "logger": "root", "message": "Shutting down thread pool 'io' with 8 workers", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 208}
{"timestamp": "2025-07-01 15:14:01,608", "level": "INFO", "logger": "root", "message": "Thread pool 'io' shutdown completed", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 212}
{"timestamp": "2025-07-01 15:14:01,608", "level": "INFO", "logger": "root", "message": "Custom cleanup completed for threadpool_io", "module": "resource_manager", "function": "cleanup_resource", "line": 70}
{"timestamp": "2025-07-01 15:14:01,615", "level": "INFO", "logger": "root", "message": "Shutting down thread pool: threadpool_io", "module": "resource_manager", "function": "_generic_cleanup", "line": 92}
{"timestamp": "2025-07-01 15:14:01,616", "level": "INFO", "logger": "root", "message": "Cleaned up resource: threadpool_io", "module": "resource_manager", "function": "cleanup_resource", "line": 82}
{"timestamp": "2025-07-01 15:14:01,616", "level": "INFO", "logger": "root", "message": "Shutting down thread pool 'main' with 4 workers", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 208}
{"timestamp": "2025-07-01 15:14:01,617", "level": "INFO", "logger": "root", "message": "Thread pool 'main' shutdown completed", "module": "resource_manager", "function": "cleanup_with_timeout", "line": 212}
{"timestamp": "2025-07-01 15:14:01,617", "level": "INFO", "logger": "root", "message": "Custom cleanup completed for threadpool_main", "module": "resource_manager", "function": "cleanup_resource", "line": 70}
{"timestamp": "2025-07-01 15:14:01,618", "level": "INFO", "logger": "root", "message": "Shutting down thread pool: threadpool_main", "module": "resource_manager", "function": "_generic_cleanup", "line": 92}
{"timestamp": "2025-07-01 15:14:01,618", "level": "INFO", "logger": "root", "message": "Cleaned up resource: threadpool_main", "module": "resource_manager", "function": "cleanup_resource", "line": 82}
{"timestamp": "2025-07-01 15:14:01,993", "level": "INFO", "logger": "root", "message": "Resource cleanup completed", "module": "resource_manager", "function": "cleanup_all", "line": 150}
{"timestamp": "2025-07-01 15:14:02,311", "level": "INFO", "logger": "root", "message": "GlobalRAGSystem shutdown completed", "module": "resource_manager", "function": "shutdown", "line": 440}
{"timestamp": "2025-07-01 15:14:02,315", "level": "INFO", "logger": "root", "message": "\u2705 API thread pool shutdown complete", "module": "main", "function": "cleanup_thread_pool", "line": 25}
{"timestamp": "2025-07-01 15:14:02,315", "level": "INFO", "logger": "root", "message": "\ud83d\udd04 Cleaning up global application lifecycle...", "module": "main", "function": "cleanup_thread_pool", "line": 36}
{"timestamp": "2025-07-01 15:14:02,316", "level": "INFO", "logger": "root", "message": "\u2705 Global application lifecycle cleanup complete", "module": "main", "function": "cleanup_thread_pool", "line": 38}
{"timestamp": "2025-07-01 15:14:02,316", "level": "INFO", "logger": "root", "message": "\u2705 All resource manager instances cleaned up", "module": "main", "function": "cleanup_thread_pool", "line": 42}
{"timestamp": "2025-07-01 15:22:18,510", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 15:22:18,514", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 15:22:18,515", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 15:22:18,516", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 15:22:18,543", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 15:22:18,543", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 15:22:20,327", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:20,342", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:20,344", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 15:22:20,346", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 15:22:20,915", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 15:22:20,916", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '2e8383d0-568f-11f0-b177-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:22:21,041", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:22:20 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:22:21,047", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 15:22:21,047", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 15:22:21,473", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 15:22:21,474", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 15:22:21,477", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 15:22:21,478", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['servicenow_integration', 'embedder', 'json_store', 'config_manager', 'verified_ingestion_engine', 'chunker', 'ingestion_engine', 'llm_client', 'ingestion_verifier', 'vector_store', 'reranker', 'query_enhancer', 'query_engine', 'log_store', 'metadata_store', 'conversation_manager', 'ingestion_debugger', 'faiss_store']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 15:22:21,482", "level": "INFO", "logger": "root", "message": "Setting up monitoring...", "module": "main", "function": "main", "line": 77}
{"timestamp": "2025-07-01 15:22:21,482", "level": "INFO", "logger": "root", "message": "Monitoring setup completed", "module": "setup", "function": "setup_monitoring", "line": 18}
{"timestamp": "2025-07-01 15:22:21,483", "level": "INFO", "logger": "root", "message": "Initializing heartbeat monitor...", "module": "main", "function": "main", "line": 87}
{"timestamp": "2025-07-01 15:22:21,483", "level": "INFO", "logger": "src.monitoring.heartbeat_monitor", "message": "Heartbeat monitor initialized", "module": "heartbeat_monitor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 15:22:21,484", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor initialized successfully", "module": "main", "function": "main", "line": 89}
{"timestamp": "2025-07-01 15:22:21,486", "level": "INFO", "logger": "root", "message": "Initializing folder monitor...", "module": "main", "function": "main", "line": 98}
{"timestamp": "2025-07-01 15:22:21,486", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 15:22:21,495", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 15:22:21,496", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor initialized successfully", "module": "main", "function": "main", "line": 101}
{"timestamp": "2025-07-01 15:22:21,949", "level": "INFO", "logger": "root", "message": "Initializing enhanced folder monitor...", "module": "main", "function": "main", "line": 110}
{"timestamp": "2025-07-01 15:22:21,950", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 15:22:21,950", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 15:22:21,950", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Enhanced folder monitor initialized with pipeline verification", "module": "enhanced_folder_monitor", "function": "__init__", "line": 84}
{"timestamp": "2025-07-01 15:22:21,951", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitor initialized successfully", "module": "main", "function": "main", "line": 113}
{"timestamp": "2025-07-01 15:22:22,004", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor registered with API", "module": "main", "function": "main", "line": 122}
{"timestamp": "2025-07-01 15:22:22,005", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor registered with API", "module": "main", "function": "main", "line": 130}
{"timestamp": "2025-07-01 15:22:22,005", "level": "INFO", "logger": "root", "message": "Creating FastAPI application...", "module": "main", "function": "main", "line": 136}
{"timestamp": "2025-07-01 15:22:22,006", "level": "INFO", "logger": "root", "message": "ResourceManager initialized", "module": "resource_manager", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:22:22,006", "level": "INFO", "logger": "root", "message": "Initialized GlobalRAGSystem lifecycle manager", "module": "resource_manager", "function": "__init__", "line": 390}
{"timestamp": "2025-07-01 15:22:22,006", "level": "INFO", "logger": "root", "message": "Starting GlobalRAGSystem...", "module": "resource_manager", "function": "startup", "line": 398}
{"timestamp": "2025-07-01 15:22:22,007", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_main", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:22:22,007", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'main' with 4 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:22:22,007", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_io", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:22:22,008", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'io' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:22:22,012", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_compute", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:22:22,015", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'compute' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:22:22,016", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_background", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:22:22,017", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'background' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:22:22,028", "level": "INFO", "logger": "root", "message": "GlobalRAGSystem startup completed successfully", "module": "resource_manager", "function": "startup", "line": 411}
{"timestamp": "2025-07-01 15:22:22,032", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_api_operations", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:22:22,033", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'api_operations' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:22:22,033", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor set in API: <class 'src.monitoring.heartbeat_monitor.HeartbeatMonitor'>", "module": "main", "function": "create_api_app", "line": 227}
{"timestamp": "2025-07-01 15:22:22,034", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor set in API: <class 'src.monitoring.folder_monitor.FolderMonitor'>", "module": "main", "function": "create_api_app", "line": 240}
{"timestamp": "2025-07-01 15:22:22,036", "level": "INFO", "logger": "src.storage.feedback_store", "message": "Feedback store initialized at: data\\feedback_store.db", "module": "feedback_store", "function": "__init__", "line": 29}
{"timestamp": "2025-07-01 15:22:22,071", "level": "INFO", "logger": "root", "message": "\u2705 Management API routes registered", "module": "main", "function": "create_api_app", "line": 2341}
{"timestamp": "2025-07-01 15:22:22,087", "level": "WARNING", "logger": "root", "message": "\u26a0\ufe0f ServiceNow API routes not available: No module named 'rag_system'", "module": "main", "function": "create_api_app", "line": 2356}
{"timestamp": "2025-07-01 15:22:22,099", "level": "INFO", "logger": "root", "message": "\u2705 Conversation API routes registered", "module": "main", "function": "create_api_app", "line": 2365}
{"timestamp": "2025-07-01 15:22:22,109", "level": "INFO", "logger": "root", "message": "\u2705 Verification API routes registered", "module": "main", "function": "create_api_app", "line": 2373}
{"timestamp": "2025-07-01 15:22:22,141", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitoring API routes registered", "module": "main", "function": "create_api_app", "line": 2383}
{"timestamp": "2025-07-01 15:22:22,163", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 15:22:22,518", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s", "module": "model_memory_manager", "function": "__init__", "line": 95}
{"timestamp": "2025-07-01 15:22:22,527", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Loading model: semantic_chunker_all-MiniLM-L6-v2", "module": "model_memory_manager", "function": "get_model", "line": 117}
{"timestamp": "2025-07-01 15:22:22,528", "level": "INFO", "logger": "root", "message": "Loading sentence transformer: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "load_model", "line": 340}
{"timestamp": "2025-07-01 15:22:22,530", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-07-01 15:22:22,531", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-07-01 15:22:25,896", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 loaded in 3.37s. Current memory: 520.27MB", "module": "model_memory_manager", "function": "get_model", "line": 144}
{"timestamp": "2025-07-01 15:22:25,904", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 345}
{"timestamp": "2025-07-01 15:22:25,904", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 47}
{"timestamp": "2025-07-01 15:22:25,907", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 15:22:25,907", "level": "INFO", "logger": "root", "message": "DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']", "module": "ingestion_engine", "function": "__init__", "line": 45}
{"timestamp": "2025-07-01 15:22:25,908", "level": "INFO", "logger": "root", "message": "Azure AI config added to processor config", "module": "ingestion_engine", "function": "__init__", "line": 54}
{"timestamp": "2025-07-01 15:22:25,908", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 15:22:25,908", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:22:25,909", "level": "INFO", "logger": "root", "message": "Extracted Azure AI config from general config", "module": "pdf_processor", "function": "create_pdf_processor", "line": 82}
{"timestamp": "2025-07-01 15:22:26,343", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 15:22:26,344", "level": "INFO", "logger": "root", "message": "Azure AI client created successfully for PDF processing", "module": "pdf_processor", "function": "create_pdf_processor", "line": 93}
{"timestamp": "2025-07-01 15:22:26,344", "level": "INFO", "logger": "root", "message": "Azure CV endpoint: https://computervision1298.cognitiveservices.azure...", "module": "pdf_processor", "function": "create_pdf_processor", "line": 94}
{"timestamp": "2025-07-01 15:22:26,344", "level": "INFO", "logger": "root", "message": "Using EnhancedPDFProcessor with Azure AI integration", "module": "pdf_processor", "function": "create_pdf_processor", "line": 106}
{"timestamp": "2025-07-01 15:22:26,345", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:22:26,345", "level": "INFO", "logger": "WordProcessor", "message": "Word processor initialized", "module": "word_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 15:22:26,345", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: WordProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:22:26,345", "level": "INFO", "logger": "ImageProcessor", "message": "Image processor initialized", "module": "image_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 15:22:26,346", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ImageProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:22:26,346", "level": "INFO", "logger": "ServiceNowProcessor", "message": "ServiceNow processor initialized", "module": "servicenow_processor", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:22:26,347", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ServiceNowProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:22:26,348", "level": "INFO", "logger": "TextProcessor", "message": "Text processor initialized", "module": "text_processor", "function": "__init__", "line": 30}
{"timestamp": "2025-07-01 15:22:26,348", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: TextProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:22:26,351", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 15:22:26,351", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:22:26,352", "level": "INFO", "logger": "root", "message": "Enhanced PDF Processor with Azure CV registered successfully", "module": "ingestion_engine", "function": "__init__", "line": 73}
{"timestamp": "2025-07-01 15:22:26,352", "level": "INFO", "logger": "root", "message": "Processor registry initialized with 7 processors", "module": "ingestion_engine", "function": "__init__", "line": 77}
{"timestamp": "2025-07-01 15:22:26,353", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized with managed metadata", "module": "ingestion_engine", "function": "__init__", "line": 89}
{"timestamp": "2025-07-01 15:22:26,353", "level": "INFO", "logger": "root", "message": "\u2705 Progress tracker initialized successfully", "module": "main", "function": "create_api_app", "line": 2440}
{"timestamp": "2025-07-01 15:22:26,355", "level": "INFO", "logger": "root", "message": "FastAPI application created", "module": "main", "function": "create_api_app", "line": 2637}
{"timestamp": "2025-07-01 15:22:26,355", "level": "INFO", "logger": "root", "message": "\u2705 Thread pool captured for cleanup", "module": "main", "function": "main", "line": 145}
{"timestamp": "2025-07-01 15:22:26,356", "level": "INFO", "logger": "root", "message": "FastAPI application created successfully", "module": "main", "function": "main", "line": 149}
{"timestamp": "2025-07-01 15:22:26,356", "level": "INFO", "logger": "root", "message": "Heartbeat monitoring disabled in config", "module": "main", "function": "main", "line": 163}
{"timestamp": "2025-07-01 15:22:26,357", "level": "INFO", "logger": "root", "message": "No folders configured for monitoring", "module": "main", "function": "main", "line": 184}
{"timestamp": "2025-07-01 15:22:26,357", "level": "INFO", "logger": "root", "message": "Starting server on 0.0.0.0:8000", "module": "main", "function": "main", "line": 195}
{"timestamp": "2025-07-01 15:22:26,418", "level": "INFO", "logger": "root", "message": "\ud83d\ude80 RAG System API starting up with managed resources...", "module": "main", "function": "startup_event", "line": 2393}
{"timestamp": "2025-07-01 15:22:26,419", "level": "INFO", "logger": "root", "message": "Registered resource: feedback_store", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:22:45,795", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Initialized Memory checkpointer for state persistence", "module": "conversation_graph", "function": "__init__", "line": 36}
{"timestamp": "2025-07-01 15:22:45,912", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation graph compiled successfully with Memory state persistence", "module": "conversation_graph", "function": "_build_graph", "line": 104}
{"timestamp": "2025-07-01 15:22:46,017", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "ConversationGraph initialized with state persistence", "module": "conversation_graph", "function": "__init__", "line": 41}
{"timestamp": "2025-07-01 15:22:46,132", "level": "WARNING", "logger": "src.conversation.conversation_graph", "message": "Checkpointer not available or missing client attribute", "module": "conversation_graph", "function": "list_conversation_threads", "line": 309}
{"timestamp": "2025-07-01 15:22:46,203", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Conversation cleanup completed: 0 old conversations removed", "module": "conversation_manager", "function": "cleanup_old_conversations", "line": 173}
{"timestamp": "2025-07-01 15:22:46,252", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Initial conversation cleanup: 0 conversations removed", "module": "conversation_manager", "function": "__init__", "line": 34}
{"timestamp": "2025-07-01 15:22:46,261", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "ConversationManager initialized with LangGraph state persistence and memory management", "module": "conversation_manager", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:22:46,293", "level": "INFO", "logger": "src.api.routes.conversation", "message": "ConversationManager initialized from container", "module": "conversation", "function": "get_conversation_manager", "line": 68}
{"timestamp": "2025-07-01 15:22:46,372", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Created new conversation state for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_graph", "function": "_get_or_create_state", "line": 259}
{"timestamp": "2025-07-01 15:22:46,549", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:22:46,669", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:22:46,671", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: None, turn: 1", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:22:46,672", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:22:46,821", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:22:46,822", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:22:46,822", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 354}
{"timestamp": "2025-07-01 15:22:46,823", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:22:46,823", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=None, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:22:46,825", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 36822106-bd3a-4710-a661-5463c08cbf40, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:22:46,825", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Started/retrieved conversation for thread: 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_manager", "function": "start_conversation", "line": 53}
{"timestamp": "2025-07-01 15:22:48,864", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:22:48,889", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:22:48,890", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:22:48,891", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 424}
{"timestamp": "2025-07-01 15:22:48,891", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'How many tickets in the system'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:22:48,891", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'How many tickets in the system (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:22:48,892", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'How many tickets in the system (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:22:48,893", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'tickets', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:22:48,904", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:22:48,904", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:22:48,905", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 3", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:22:48,984", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:22:48,985", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:22:48,986", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'How many tickets in the system (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:22:48,986", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:22:49,039", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,070", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,199", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,227", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,355", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,356", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 2: Original query: 'How many tickets in the system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 182}
{"timestamp": "2025-07-01 15:22:49,357", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:22:49,437", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,540", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,578", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,673", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,923", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:49,944", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:22:49,945", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'aggregation_results', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:22:49,945", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "No sources found in search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 262}
{"timestamp": "2025-07-01 15:22:49,947", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:22:49,948", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:22:49,948", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 337}
{"timestamp": "2025-07-01 15:22:49,949", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 791}
{"timestamp": "2025-07-01 15:22:49,949", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 807}
{"timestamp": "2025-07-01 15:22:49,950", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 326", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 816}
{"timestamp": "2025-07-01 15:22:49,950", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 842}
{"timestamp": "2025-07-01 15:22:50,410", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:50,422", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 542", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 15:22:50,423", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 4 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 848}
{"timestamp": "2025-07-01 15:22:50,423", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 4 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 851}
{"timestamp": "2025-07-01 15:22:50,439", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 4 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 339}
{"timestamp": "2025-07-01 15:22:50,439", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 354}
{"timestamp": "2025-07-01 15:22:50,440", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:22:50,440", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=4", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:22:50,596", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 36822106-bd3a-4710-a661-5463c08cbf40, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:22:50,640", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:22:54,858", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:22:54,986", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:22:55,008", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:22:55,008", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 414}
{"timestamp": "2025-07-01 15:22:55,009", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'which are these?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:22:55,009", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'which are these? (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:22:55,010", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'which are these? (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:22:55,010", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['which', 'these']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:22:55,010", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:22:55,011", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:22:55,012", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 5", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:22:55,194", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:22:55,199", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:22:55,199", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'which are these? (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:22:55,200", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:22:55,217", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '105'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '42f54e81-568f-11f0-b930-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:22:55,271", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:22:54 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:22:55,353", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:56,245", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:56,247", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:22:56,247", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'sources', 'total_sources', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:22:56,248", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 235}
{"timestamp": "2025-07-01 15:22:56,248", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 259}
{"timestamp": "2025-07-01 15:22:56,249", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:22:56,250", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:22:56,250", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 559}
{"timestamp": "2025-07-01 15:22:56,433", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:57,539", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:22:57,561", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 352}
{"timestamp": "2025-07-01 15:22:57,562", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:22:57,562", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=6", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:22:57,632", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 36822106-bd3a-4710-a661-5463c08cbf40, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:22:57,637", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:23:02,699", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:23:05,726", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:23:05,734", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:23:05,735", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 424}
{"timestamp": "2025-07-01 15:23:05,735", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'We have 3 access points in Building A'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:23:05,736", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'We have 3 access points in Building A (context: access points Building A)'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:23:05,737", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'We have 3 access points in Building A (context: access points Building A)'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:23:05,766", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['access', 'points', 'building']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:23:05,767", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:23:05,767", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:23:05,767", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 7", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:23:05,884", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:23:05,889", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:23:05,889", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'We have 3 access points in Building A (context: access points Building A)'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:23:05,890", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:23:05,898", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '150'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '495327b8-568f-11f0-82a9-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:23:06,013", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:23:05 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:23:06,049", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:06,623", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:06,696", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:23:06,696", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'sources', 'total_sources', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:23:06,697", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 235}
{"timestamp": "2025-07-01 15:23:06,698", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 259}
{"timestamp": "2025-07-01 15:23:06,700", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:23:06,701", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:23:06,702", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 559}
{"timestamp": "2025-07-01 15:23:07,165", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:08,230", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:08,233", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 352}
{"timestamp": "2025-07-01 15:23:08,235", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:23:08,235", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=8", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:23:08,294", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 36822106-bd3a-4710-a661-5463c08cbf40, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:23:08,295", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:23:12,353", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:23:12,360", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:23:12,362", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:23:12,372", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 414}
{"timestamp": "2025-07-01 15:23:12,372", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'what are those?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:23:12,373", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'what are those? access points Building A'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:23:12,373", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'what are those? access points Building A'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:23:12,374", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['those']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:23:12,374", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:23:12,374", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:23:12,375", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 9", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:23:12,381", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:23:12,387", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:23:12,388", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'what are those? access points Building A'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:23:12,388", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:23:12,403", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '117'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '4d33c1ed-568f-11f0-80dd-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:23:12,529", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:23:11 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:23:12,546", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:13,498", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:13,503", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:23:13,504", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'sources', 'total_sources', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:23:13,504", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 235}
{"timestamp": "2025-07-01 15:23:13,504", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 259}
{"timestamp": "2025-07-01 15:23:13,505", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:23:13,506", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:23:13,506", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 559}
{"timestamp": "2025-07-01 15:23:13,691", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:14,775", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:14,778", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 337}
{"timestamp": "2025-07-01 15:23:14,780", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 791}
{"timestamp": "2025-07-01 15:23:14,780", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 807}
{"timestamp": "2025-07-01 15:23:14,780", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 1342", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 816}
{"timestamp": "2025-07-01 15:23:14,780", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 842}
{"timestamp": "2025-07-01 15:23:15,905", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:15,910", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 416", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 15:23:15,911", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 3 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 848}
{"timestamp": "2025-07-01 15:23:15,911", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 3 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 851}
{"timestamp": "2025-07-01 15:23:15,911", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 3 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 339}
{"timestamp": "2025-07-01 15:23:15,912", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 352}
{"timestamp": "2025-07-01 15:23:15,912", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:23:15,912", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=10", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:23:15,978", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 36822106-bd3a-4710-a661-5463c08cbf40, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:23:15,979", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:23:21,036", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:23:21,100", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:23:21,109", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:23:21,110", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 414}
{"timestamp": "2025-07-01 15:23:21,110", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'There are 5 network incidents this week'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:23:21,111", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'There are 5 network incidents this week (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:23:21,111", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'There are 5 network incidents this week (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:23:21,111", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['there', 'network', 'incidents', 'this', 'week']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:23:21,112", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:23:21,112", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:23:21,122", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 11", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:23:21,135", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:23:21,136", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:23:21,137", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'There are 5 network incidents this week (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:23:21,137", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:23:21,143", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '128'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '52695ea4-568f-11f0-a877-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:23:21,353", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:23:20 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:23:21,413", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:21,498", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:21,506", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.386296 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:21,969", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:21,973", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.975405 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:23,007", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:24,580", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:24,580", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:24,581", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "qdrant_query_engine", "function": "_generate_llm_response", "line": 339}
{"timestamp": "2025-07-01 15:23:24,581", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:23:24,591", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'sources', 'total_sources', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:23:24,592", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 235}
{"timestamp": "2025-07-01 15:23:24,592", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 259}
{"timestamp": "2025-07-01 15:23:24,593", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:23:24,594", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:23:24,594", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 559}
{"timestamp": "2025-07-01 15:23:24,657", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:25,124", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.460845 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:25,668", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:25,670", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.851413 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:26,586", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:26,603", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:26,603", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:26,604", "level": "ERROR", "logger": "src.conversation.conversation_nodes", "message": "LLM intent detection failed: Groq generation failed: no healthy upstream", "module": "conversation_nodes", "function": "_is_simple_followup_question", "line": 537}
{"timestamp": "2025-07-01 15:23:26,659", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:26,672", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.438110 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:27,173", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:27,174", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.809703 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:28,044", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:28,133", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:28,133", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:28,133", "level": "ERROR", "logger": "src.conversation.conversation_nodes", "message": "LLM intent detection failed: Groq generation failed: no healthy upstream", "module": "conversation_nodes", "function": "_is_simple_followup_question", "line": 537}
{"timestamp": "2025-07-01 15:23:28,134", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 352}
{"timestamp": "2025-07-01 15:23:28,134", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:23:28,135", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=12", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:23:28,283", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 36822106-bd3a-4710-a661-5463c08cbf40, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:23:28,403", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:23:32,516", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:23:32,541", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:23:32,541", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:23:32,543", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: ^(list|show|give me)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 414}
{"timestamp": "2025-07-01 15:23:32,543", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'show them'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:23:32,543", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'show them (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:23:32,543", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'show them (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:23:32,544", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['show', 'them']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:23:32,544", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:23:32,544", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:23:32,545", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 13", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:23:32,549", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:23:32,549", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:23:32,551", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'show them (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:23:32,551", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:23:32,558", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '98'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '593718d6-568f-11f0-a9b3-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:23:32,630", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:23:31 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:23:32,643", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:32,705", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:32,706", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.424063 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:33,194", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:33,194", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.792215 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:34,048", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:34,061", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:34,061", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:34,062", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "qdrant_query_engine", "function": "_generate_llm_response", "line": 339}
{"timestamp": "2025-07-01 15:23:34,062", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:23:34,062", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'sources', 'total_sources', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:23:34,062", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 235}
{"timestamp": "2025-07-01 15:23:34,063", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 259}
{"timestamp": "2025-07-01 15:23:34,064", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:23:34,064", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:23:34,064", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 559}
{"timestamp": "2025-07-01 15:23:34,119", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:34,125", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.465721 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:34,659", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:34,721", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.816299 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:35,603", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:35,631", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:35,631", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:35,632", "level": "ERROR", "logger": "src.conversation.conversation_nodes", "message": "LLM intent detection failed: Groq generation failed: no healthy upstream", "module": "conversation_nodes", "function": "_is_simple_followup_question", "line": 537}
{"timestamp": "2025-07-01 15:23:35,690", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:35,691", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.491033 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:36,252", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:36,255", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.777485 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:37,095", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:37,106", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:37,106", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:37,107", "level": "ERROR", "logger": "src.conversation.conversation_nodes", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 611}
{"timestamp": "2025-07-01 15:23:37,156", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:37,188", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.464139 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:37,726", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:37,761", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.771815 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:38,593", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:38,598", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:38,599", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:38,599", "level": "ERROR", "logger": "src.conversation.conversation_nodes", "message": "LLM intent detection failed: Groq generation failed: no healthy upstream", "module": "conversation_nodes", "function": "_is_simple_followup_question", "line": 537}
{"timestamp": "2025-07-01 15:23:38,687", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:38,690", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.435810 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:39,195", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:39,206", "level": "INFO", "logger": "groq._base_client", "message": "Retrying request to /openai/v1/chat/completions in 0.874982 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1055}
{"timestamp": "2025-07-01 15:23:40,130", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:23:40,135", "level": "ERROR", "logger": "root", "message": "Groq generation error: no healthy upstream", "module": "llm_client", "function": "generate", "line": 91}
{"timestamp": "2025-07-01 15:23:40,135", "level": "ERROR", "logger": "root", "message": "LLM generation failed: Groq generation failed: no healthy upstream", "module": "llm_client", "function": "generate", "line": 249}
{"timestamp": "2025-07-01 15:23:40,136", "level": "ERROR", "logger": "src.conversation.conversation_nodes", "message": "LLM generation failed for context-based response: Groq generation failed: no healthy upstream", "module": "conversation_nodes", "function": "_generate_general_response", "line": 781}
{"timestamp": "2025-07-01 15:23:40,136", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 352}
{"timestamp": "2025-07-01 15:23:40,136", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:23:40,137", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=14", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:23:40,175", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 36822106-bd3a-4710-a661-5463c08cbf40, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:23:40,186", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 36822106-bd3a-4710-a661-5463c08cbf40", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:25:04,808", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Created new conversation state for thread 471d6f3c-c512-4f53-b94d-d5f71820a25f", "module": "conversation_graph", "function": "_get_or_create_state", "line": 259}
{"timestamp": "2025-07-01 15:25:04,810", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:25:04,811", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:25:04,811", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: None, turn: 1", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:25:04,812", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:25:04,813", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:25:04,813", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:25:04,814", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 354}
{"timestamp": "2025-07-01 15:25:04,814", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:25:04,814", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=None, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:25:04,817", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 471d6f3c-c512-4f53-b94d-d5f71820a25f, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:25:04,818", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Started/retrieved conversation for thread: 471d6f3c-c512-4f53-b94d-d5f71820a25f", "module": "conversation_manager", "function": "start_conversation", "line": 53}
{"timestamp": "2025-07-01 15:25:04,819", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Created new conversation state for thread 45aa7b65-67cf-49d3-8572-144560671c82", "module": "conversation_graph", "function": "_get_or_create_state", "line": 259}
{"timestamp": "2025-07-01 15:25:04,822", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:25:04,822", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:25:04,824", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: None, turn: 1", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:25:04,825", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:25:04,826", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:25:04,827", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:25:04,828", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 354}
{"timestamp": "2025-07-01 15:25:04,828", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:25:04,829", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=None, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:25:04,830", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 45aa7b65-67cf-49d3-8572-144560671c82, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:25:04,832", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Started/retrieved conversation for thread: 45aa7b65-67cf-49d3-8572-144560671c82", "module": "conversation_manager", "function": "start_conversation", "line": 53}
{"timestamp": "2025-07-01 15:25:15,437", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 45aa7b65-67cf-49d3-8572-144560671c82", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:25:15,451", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:25:15,453", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:25:15,454", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 424}
{"timestamp": "2025-07-01 15:25:15,454", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'how many incidents are in system'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:25:15,454", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:25:15,455", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:25:15,456", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'incidents', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:25:15,456", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:25:15,457", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:25:15,458", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 3", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:25:15,473", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:25:15,473", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:25:15,474", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:25:15,474", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:25:15,487", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,505", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,523", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,535", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,543", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,545", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 2: Original query: 'how many incidents are in system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 182}
{"timestamp": "2025-07-01 15:25:15,545", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:25:15,552", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,574", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,581", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,594", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,606", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,607", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:25:15,608", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'aggregation_results', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:25:15,608", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "No sources found in search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 262}
{"timestamp": "2025-07-01 15:25:15,610", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:25:15,611", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:25:15,612", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 337}
{"timestamp": "2025-07-01 15:25:15,612", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 791}
{"timestamp": "2025-07-01 15:25:15,613", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 807}
{"timestamp": "2025-07-01 15:25:15,614", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 328", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 816}
{"timestamp": "2025-07-01 15:25:15,615", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 842}
{"timestamp": "2025-07-01 15:25:15,990", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:15,991", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 401", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 15:25:15,991", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 3 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 848}
{"timestamp": "2025-07-01 15:25:15,992", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 3 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 851}
{"timestamp": "2025-07-01 15:25:15,992", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 3 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 339}
{"timestamp": "2025-07-01 15:25:15,992", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 354}
{"timestamp": "2025-07-01 15:25:15,992", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:25:15,992", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=4", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:25:15,995", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 45aa7b65-67cf-49d3-8572-144560671c82, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:25:15,995", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 45aa7b65-67cf-49d3-8572-144560671c82", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:25:30,495", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 45aa7b65-67cf-49d3-8572-144560671c82", "module": "conversation_graph", "function": "_get_or_create_state", "line": 253}
{"timestamp": "2025-07-01 15:25:30,523", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:25:30,544", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:25:30,545", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 414}
{"timestamp": "2025-07-01 15:25:30,545", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'What are these?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 432}
{"timestamp": "2025-07-01 15:25:30,546", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'What are these? (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 496}
{"timestamp": "2025-07-01 15:25:30,547", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'What are these? (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:25:30,548", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['these']", "module": "conversation_nodes", "function": "understand_intent", "line": 140}
{"timestamp": "2025-07-01 15:25:30,548", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 141}
{"timestamp": "2025-07-01 15:25:30,548", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 142}
{"timestamp": "2025-07-01 15:25:30,550", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 5", "module": "conversation_graph", "function": "_route_after_understanding", "line": 114}
{"timestamp": "2025-07-01 15:25:30,550", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 147}
{"timestamp": "2025-07-01 15:25:30,552", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Handling contextual query with multiple search strategies", "module": "conversation_nodes", "function": "search_knowledge", "line": 170}
{"timestamp": "2025-07-01 15:25:30,552", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 1: Enhanced query: 'What are these? (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 173}
{"timestamp": "2025-07-01 15:25:30,553", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:25:30,570", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '104'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '9f8e3dde-568f-11f0-8f27-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:25:30,717", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:25:30 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:25:30,729", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:31,791", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:31,792", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result type: <class 'dict'>", "module": "conversation_nodes", "function": "search_knowledge", "line": 223}
{"timestamp": "2025-07-01 15:25:31,792", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search result keys: ['query', 'response', 'confidence_score', 'confidence_level', 'sources', 'total_sources', 'query_type', 'method', 'timestamp']", "module": "conversation_nodes", "function": "search_knowledge", "line": 224}
{"timestamp": "2025-07-01 15:25:31,793", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 235}
{"timestamp": "2025-07-01 15:25:31,793", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 259}
{"timestamp": "2025-07-01 15:25:31,795", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 287}
{"timestamp": "2025-07-01 15:25:31,796", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 288}
{"timestamp": "2025-07-01 15:25:31,796", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 559}
{"timestamp": "2025-07-01 15:25:31,865", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:33,070", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:25:33,071", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 352}
{"timestamp": "2025-07-01 15:25:33,071", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 356}
{"timestamp": "2025-07-01 15:25:33,071", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=6", "module": "conversation_graph", "function": "_route_conversation_end", "line": 177}
{"timestamp": "2025-07-01 15:25:33,072", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 45aa7b65-67cf-49d3-8572-144560671c82, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 215}
{"timestamp": "2025-07-01 15:25:33,072", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 45aa7b65-67cf-49d3-8572-144560671c82", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:28:22,919", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Cleaning up 1 idle models", "module": "model_memory_manager", "function": "_cleanup_idle_models", "line": 338}
{"timestamp": "2025-07-01 15:28:23,295", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 536.77MB", "module": "model_memory_manager", "function": "_unload_model", "line": 260}
{"timestamp": "2025-07-01 15:50:31,702", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 15:50:31,705", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 15:50:31,705", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 15:50:31,709", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 15:50:31,711", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 15:50:31,711", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 15:50:33,187", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:50:33,195", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:50:33,200", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 15:50:33,201", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 15:50:33,584", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 15:50:33,586", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '1f6c512c-5693-11f0-84d1-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:50:33,778", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:50:33 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:50:33,784", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 15:50:33,784", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 15:50:34,118", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 15:50:34,118", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 15:50:34,131", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 15:50:34,133", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['metadata_store', 'verified_ingestion_engine', 'servicenow_integration', 'llm_client', 'chunker', 'json_store', 'reranker', 'vector_store', 'ingestion_verifier', 'conversation_manager', 'query_enhancer', 'config_manager', 'embedder', 'query_engine', 'ingestion_engine', 'log_store', 'ingestion_debugger', 'faiss_store']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 15:50:34,138", "level": "INFO", "logger": "root", "message": "Setting up monitoring...", "module": "main", "function": "main", "line": 77}
{"timestamp": "2025-07-01 15:50:34,138", "level": "INFO", "logger": "root", "message": "Monitoring setup completed", "module": "setup", "function": "setup_monitoring", "line": 18}
{"timestamp": "2025-07-01 15:50:34,138", "level": "INFO", "logger": "root", "message": "Initializing heartbeat monitor...", "module": "main", "function": "main", "line": 87}
{"timestamp": "2025-07-01 15:50:34,139", "level": "INFO", "logger": "src.monitoring.heartbeat_monitor", "message": "Heartbeat monitor initialized", "module": "heartbeat_monitor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 15:50:34,139", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor initialized successfully", "module": "main", "function": "main", "line": 89}
{"timestamp": "2025-07-01 15:50:34,142", "level": "INFO", "logger": "root", "message": "Initializing folder monitor...", "module": "main", "function": "main", "line": 98}
{"timestamp": "2025-07-01 15:50:34,142", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 15:50:34,142", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 15:50:34,144", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor initialized successfully", "module": "main", "function": "main", "line": 101}
{"timestamp": "2025-07-01 15:50:34,547", "level": "INFO", "logger": "root", "message": "Initializing enhanced folder monitor...", "module": "main", "function": "main", "line": 110}
{"timestamp": "2025-07-01 15:50:34,548", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 15:50:34,549", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 15:50:34,549", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Enhanced folder monitor initialized with pipeline verification", "module": "enhanced_folder_monitor", "function": "__init__", "line": 84}
{"timestamp": "2025-07-01 15:50:34,552", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitor initialized successfully", "module": "main", "function": "main", "line": 113}
{"timestamp": "2025-07-01 15:50:34,598", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor registered with API", "module": "main", "function": "main", "line": 122}
{"timestamp": "2025-07-01 15:50:34,600", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor registered with API", "module": "main", "function": "main", "line": 130}
{"timestamp": "2025-07-01 15:50:34,600", "level": "INFO", "logger": "root", "message": "Creating FastAPI application...", "module": "main", "function": "main", "line": 136}
{"timestamp": "2025-07-01 15:50:34,601", "level": "INFO", "logger": "root", "message": "ResourceManager initialized", "module": "resource_manager", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:50:34,601", "level": "INFO", "logger": "root", "message": "Initialized GlobalRAGSystem lifecycle manager", "module": "resource_manager", "function": "__init__", "line": 390}
{"timestamp": "2025-07-01 15:50:34,601", "level": "INFO", "logger": "root", "message": "Starting GlobalRAGSystem...", "module": "resource_manager", "function": "startup", "line": 398}
{"timestamp": "2025-07-01 15:50:34,601", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_main", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:50:34,601", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'main' with 4 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:50:34,603", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_io", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:50:34,603", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'io' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:50:34,606", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_compute", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:50:34,608", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'compute' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:50:34,608", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_background", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:50:34,609", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'background' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:50:34,609", "level": "INFO", "logger": "root", "message": "GlobalRAGSystem startup completed successfully", "module": "resource_manager", "function": "startup", "line": 411}
{"timestamp": "2025-07-01 15:50:34,609", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_api_operations", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:50:34,610", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'api_operations' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 15:50:34,610", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor set in API: <class 'src.monitoring.heartbeat_monitor.HeartbeatMonitor'>", "module": "main", "function": "create_api_app", "line": 227}
{"timestamp": "2025-07-01 15:50:34,611", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor set in API: <class 'src.monitoring.folder_monitor.FolderMonitor'>", "module": "main", "function": "create_api_app", "line": 240}
{"timestamp": "2025-07-01 15:50:34,613", "level": "INFO", "logger": "src.storage.feedback_store", "message": "Feedback store initialized at: data\\feedback_store.db", "module": "feedback_store", "function": "__init__", "line": 29}
{"timestamp": "2025-07-01 15:50:34,649", "level": "INFO", "logger": "root", "message": "\u2705 Management API routes registered", "module": "main", "function": "create_api_app", "line": 2341}
{"timestamp": "2025-07-01 15:50:34,670", "level": "WARNING", "logger": "root", "message": "\u26a0\ufe0f ServiceNow API routes not available: No module named 'rag_system'", "module": "main", "function": "create_api_app", "line": 2356}
{"timestamp": "2025-07-01 15:50:34,690", "level": "INFO", "logger": "root", "message": "\u2705 Conversation API routes registered", "module": "main", "function": "create_api_app", "line": 2365}
{"timestamp": "2025-07-01 15:50:34,700", "level": "INFO", "logger": "root", "message": "\u2705 Verification API routes registered", "module": "main", "function": "create_api_app", "line": 2373}
{"timestamp": "2025-07-01 15:50:34,702", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitoring API routes registered", "module": "main", "function": "create_api_app", "line": 2383}
{"timestamp": "2025-07-01 15:50:34,704", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 15:50:34,991", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s", "module": "model_memory_manager", "function": "__init__", "line": 95}
{"timestamp": "2025-07-01 15:50:34,991", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Loading model: semantic_chunker_all-MiniLM-L6-v2", "module": "model_memory_manager", "function": "get_model", "line": 117}
{"timestamp": "2025-07-01 15:50:34,992", "level": "INFO", "logger": "root", "message": "Loading sentence transformer: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "load_model", "line": 340}
{"timestamp": "2025-07-01 15:50:34,993", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-07-01 15:50:34,994", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-07-01 15:50:36,661", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.67s. Current memory: 520.66MB", "module": "model_memory_manager", "function": "get_model", "line": 144}
{"timestamp": "2025-07-01 15:50:36,662", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 345}
{"timestamp": "2025-07-01 15:50:36,663", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 47}
{"timestamp": "2025-07-01 15:50:36,663", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 15:50:36,665", "level": "INFO", "logger": "root", "message": "DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']", "module": "ingestion_engine", "function": "__init__", "line": 45}
{"timestamp": "2025-07-01 15:50:36,665", "level": "INFO", "logger": "root", "message": "Azure AI config added to processor config", "module": "ingestion_engine", "function": "__init__", "line": 54}
{"timestamp": "2025-07-01 15:50:36,665", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 15:50:36,665", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:50:36,666", "level": "INFO", "logger": "root", "message": "Extracted Azure AI config from general config", "module": "pdf_processor", "function": "create_pdf_processor", "line": 82}
{"timestamp": "2025-07-01 15:50:37,118", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 15:50:37,132", "level": "INFO", "logger": "root", "message": "Azure AI client created successfully for PDF processing", "module": "pdf_processor", "function": "create_pdf_processor", "line": 93}
{"timestamp": "2025-07-01 15:50:37,133", "level": "INFO", "logger": "root", "message": "Azure CV endpoint: https://computervision1298.cognitiveservices.azure...", "module": "pdf_processor", "function": "create_pdf_processor", "line": 94}
{"timestamp": "2025-07-01 15:50:37,133", "level": "INFO", "logger": "root", "message": "Using EnhancedPDFProcessor with Azure AI integration", "module": "pdf_processor", "function": "create_pdf_processor", "line": 106}
{"timestamp": "2025-07-01 15:50:37,134", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:50:37,135", "level": "INFO", "logger": "WordProcessor", "message": "Word processor initialized", "module": "word_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 15:50:37,135", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: WordProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:50:37,136", "level": "INFO", "logger": "ImageProcessor", "message": "Image processor initialized", "module": "image_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 15:50:37,137", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ImageProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:50:37,137", "level": "INFO", "logger": "ServiceNowProcessor", "message": "ServiceNow processor initialized", "module": "servicenow_processor", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:50:37,137", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ServiceNowProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:50:37,138", "level": "INFO", "logger": "TextProcessor", "message": "Text processor initialized", "module": "text_processor", "function": "__init__", "line": 30}
{"timestamp": "2025-07-01 15:50:37,138", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: TextProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:50:37,141", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 15:50:37,145", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 15:50:37,146", "level": "INFO", "logger": "root", "message": "Enhanced PDF Processor with Azure CV registered successfully", "module": "ingestion_engine", "function": "__init__", "line": 73}
{"timestamp": "2025-07-01 15:50:37,146", "level": "INFO", "logger": "root", "message": "Processor registry initialized with 7 processors", "module": "ingestion_engine", "function": "__init__", "line": 77}
{"timestamp": "2025-07-01 15:50:37,147", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized with managed metadata", "module": "ingestion_engine", "function": "__init__", "line": 89}
{"timestamp": "2025-07-01 15:50:37,147", "level": "INFO", "logger": "root", "message": "\u2705 Progress tracker initialized successfully", "module": "main", "function": "create_api_app", "line": 2440}
{"timestamp": "2025-07-01 15:50:37,149", "level": "INFO", "logger": "root", "message": "FastAPI application created", "module": "main", "function": "create_api_app", "line": 2637}
{"timestamp": "2025-07-01 15:50:37,149", "level": "INFO", "logger": "root", "message": "\u2705 Thread pool captured for cleanup", "module": "main", "function": "main", "line": 145}
{"timestamp": "2025-07-01 15:50:37,149", "level": "INFO", "logger": "root", "message": "FastAPI application created successfully", "module": "main", "function": "main", "line": 149}
{"timestamp": "2025-07-01 15:50:37,156", "level": "INFO", "logger": "root", "message": "Heartbeat monitoring disabled in config", "module": "main", "function": "main", "line": 163}
{"timestamp": "2025-07-01 15:50:37,156", "level": "INFO", "logger": "root", "message": "No folders configured for monitoring", "module": "main", "function": "main", "line": 184}
{"timestamp": "2025-07-01 15:50:37,156", "level": "INFO", "logger": "root", "message": "Starting server on 0.0.0.0:8000", "module": "main", "function": "main", "line": 195}
{"timestamp": "2025-07-01 15:50:37,228", "level": "INFO", "logger": "root", "message": "\ud83d\ude80 RAG System API starting up with managed resources...", "module": "main", "function": "startup_event", "line": 2393}
{"timestamp": "2025-07-01 15:50:37,228", "level": "INFO", "logger": "root", "message": "Registered resource: feedback_store", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 15:52:47,052", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Initialized Memory checkpointer for state persistence", "module": "conversation_graph", "function": "__init__", "line": 36}
{"timestamp": "2025-07-01 15:52:47,070", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation graph compiled successfully with Memory state persistence", "module": "conversation_graph", "function": "_build_graph", "line": 104}
{"timestamp": "2025-07-01 15:52:47,070", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "ConversationGraph initialized with state persistence", "module": "conversation_graph", "function": "__init__", "line": 41}
{"timestamp": "2025-07-01 15:52:47,072", "level": "WARNING", "logger": "src.conversation.conversation_graph", "message": "Checkpointer not available or missing client attribute", "module": "conversation_graph", "function": "list_conversation_threads", "line": 330}
{"timestamp": "2025-07-01 15:52:47,072", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Conversation cleanup completed: 0 old conversations removed", "module": "conversation_manager", "function": "cleanup_old_conversations", "line": 173}
{"timestamp": "2025-07-01 15:52:47,075", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Initial conversation cleanup: 0 conversations removed", "module": "conversation_manager", "function": "__init__", "line": 34}
{"timestamp": "2025-07-01 15:52:47,075", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "ConversationManager initialized with LangGraph state persistence and memory management", "module": "conversation_manager", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 15:52:47,076", "level": "INFO", "logger": "src.api.routes.conversation", "message": "ConversationManager initialized from container", "module": "conversation", "function": "get_conversation_manager", "line": 68}
{"timestamp": "2025-07-01 15:52:47,082", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Created new conversation state for thread test123", "module": "conversation_graph", "function": "_get_or_create_state", "line": 280}
{"timestamp": "2025-07-01 15:52:47,090", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:52:47,091", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:52:47,100", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['test']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:52:47,101", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:52:47,101", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: False", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:52:47,101", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: False", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:52:47,102", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 1, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:52:47,110", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:52:47,114", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:52:47,114", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'test'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:52:47,116", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:52:47,123", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '6f047649-5693-11f0-a7c4-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:52:47,262", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:52:47 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:52:47,277", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:52:47,729", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:52:47,732", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:52:47,732", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:52:47,732", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:52:47,732", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:52:47,732", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:52:47,733", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:52:47,734", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:52:47,734", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:52:48,419", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:52:49,354", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:52:49,355", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:52:49,355", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:52:49,355", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:52:49,357", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread test123, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:52:49,358", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread test123", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:53:33,551", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Created new conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 280}
{"timestamp": "2025-07-01 15:53:33,554", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:53:33,554", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:53:33,554", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'incidents', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:53:33,555", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:53:33,555", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: False", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:53:33,555", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:53:33,556", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 1, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:53:33,556", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:53:33,559", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:53:33,560", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'how many incidents are in system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:53:33,560", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:53:33,592", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:33,625", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:33,643", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:33,665", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:33,681", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:33,682", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 15:53:33,683", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'keywords': 'many incidents system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:53:33,684", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:53:33,699", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '98'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '8ac766dd-5693-11f0-ba9e-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:53:33,769", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:53:33 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:53:33,779", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:34,629", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:34,631", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'keywords' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:53:34,632", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'keywords', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:53:34,633", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'keywords' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:53:34,633", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:53:34,636", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:53:34,637", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:53:34,639", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:53:34,639", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:53:34,930", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:35,935", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:35,936", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:53:35,936", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:53:35,941", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:53:35,946", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:53:35,946", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:53:40,993", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 15:53:40,996", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:53:40,996", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:53:40,997", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 15:53:40,997", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'which are these?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 15:53:40,997", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'which are these? (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 15:53:40,997", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'which are these? (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:53:40,999", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['which', 'these']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:53:40,999", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:53:40,999", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:53:40,999", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:53:41,001", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 3, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:53:41,001", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:53:41,006", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:53:41,006", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'which are these? (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:53:41,006", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:53:41,013", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '105'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '8f2379a5-5693-11f0-943f-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:53:41,098", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:53:40 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:53:41,109", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:42,103", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:42,105", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:53:42,106", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:53:42,106", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:53:42,107", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:53:42,107", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:53:42,108", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:53:42,112", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:53:42,112", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:53:42,194", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:43,258", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:43,258", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 365}
{"timestamp": "2025-07-01 15:53:43,260", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 819}
{"timestamp": "2025-07-01 15:53:43,260", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 835}
{"timestamp": "2025-07-01 15:53:43,260", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 1016", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 15:53:43,260", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 870}
{"timestamp": "2025-07-01 15:53:44,440", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:44,441", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 670", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 872}
{"timestamp": "2025-07-01 15:53:44,441", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 4 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 876}
{"timestamp": "2025-07-01 15:53:44,441", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 4 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 879}
{"timestamp": "2025-07-01 15:53:44,443", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 4 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 367}
{"timestamp": "2025-07-01 15:53:44,443", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:53:44,443", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:53:44,443", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=4", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:53:44,453", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:53:44,454", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:53:48,478", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 15:53:48,484", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:53:48,486", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:53:48,486", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: ^(tell me more|more about|what about|how about)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 15:53:48,486", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'tell me more about them'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 15:53:48,486", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'them detailed information'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 15:53:48,487", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'them detailed information'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:53:48,487", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['them']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:53:48,487", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:53:48,487", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:53:48,488", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:53:48,488", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 5, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:53:48,488", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:53:48,490", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:53:48,490", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'them detailed information'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:53:48,490", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:53:48,496", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '102'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '9399469d-5693-11f0-a42e-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:53:48,569", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:53:48 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:53:48,581", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:49,442", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:49,450", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:53:49,450", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:53:49,451", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:53:49,451", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:53:49,451", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:53:49,452", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:53:49,453", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:53:49,453", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:53:49,730", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:50,698", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:53:50,699", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:53:50,699", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:53:50,700", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=6", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:53:50,702", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:53:50,702", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:54:27,446", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Created new conversation state for thread de1cf667-1c9e-4e7a-9109-1279cccf36e6", "module": "conversation_graph", "function": "_get_or_create_state", "line": 280}
{"timestamp": "2025-07-01 15:54:27,449", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:54:27,449", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:54:27,450", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: None, turn: 1, phase: ConversationPhase.UNDERSTANDING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:54:27,450", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Unknown/general intent 'None' - defaulting to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 153}
{"timestamp": "2025-07-01 15:54:27,452", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:54:27,452", "level": "WARNING", "logger": "src.conversation.conversation_nodes", "message": "No query to search or query engine unavailable", "module": "conversation_nodes", "function": "search_knowledge", "line": 171}
{"timestamp": "2025-07-01 15:54:27,453", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:54:27,453", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:54:27,453", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 382}
{"timestamp": "2025-07-01 15:54:27,454", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:54:27,454", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=None, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:54:27,457", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread de1cf667-1c9e-4e7a-9109-1279cccf36e6, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:54:27,458", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Started/retrieved conversation for thread: de1cf667-1c9e-4e7a-9109-1279cccf36e6", "module": "conversation_manager", "function": "start_conversation", "line": 53}
{"timestamp": "2025-07-01 15:54:27,458", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Created new conversation state for thread 40666573-6a95-4277-b986-9ac48e6f5d92", "module": "conversation_graph", "function": "_get_or_create_state", "line": 280}
{"timestamp": "2025-07-01 15:54:27,511", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:54:27,512", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:54:27,513", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: None, turn: 1, phase: ConversationPhase.UNDERSTANDING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:54:27,513", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Unknown/general intent 'None' - defaulting to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 153}
{"timestamp": "2025-07-01 15:54:27,514", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:54:27,515", "level": "WARNING", "logger": "src.conversation.conversation_nodes", "message": "No query to search or query engine unavailable", "module": "conversation_nodes", "function": "search_knowledge", "line": 171}
{"timestamp": "2025-07-01 15:54:27,515", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:54:27,515", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:54:27,517", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated without search results", "module": "conversation_nodes", "function": "generate_response", "line": 382}
{"timestamp": "2025-07-01 15:54:27,517", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:54:27,517", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=None, turns=2", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:54:27,524", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 40666573-6a95-4277-b986-9ac48e6f5d92, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:54:27,525", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Started/retrieved conversation for thread: 40666573-6a95-4277-b986-9ac48e6f5d92", "module": "conversation_manager", "function": "start_conversation", "line": 53}
{"timestamp": "2025-07-01 15:56:26,606", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread 40666573-6a95-4277-b986-9ac48e6f5d92", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 15:56:26,611", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:56:26,612", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:56:26,613", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 452}
{"timestamp": "2025-07-01 15:56:26,614", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'How many incidents are there in the system'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 15:56:26,614", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'How many incidents are there in the system (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 15:56:26,615", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'How many incidents are there in the system (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:56:26,615", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'incidents', 'there', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:56:26,615", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:56:26,615", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:56:26,617", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:56:26,618", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 3, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:56:26,618", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:56:26,621", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:56:26,621", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'How many incidents are there in the system (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:56:26,622", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:56:26,642", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,650", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,657", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,663", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,672", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,674", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 15:56:26,675", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'original': 'How many incidents are there in the system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:56:26,676", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:56:26,683", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,697", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,707", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,716", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,720", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:26,722", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'original' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 15:56:26,724", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'keywords': 'many incidents there'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:56:26,724", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:56:26,737", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '97'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': 'f1eb063f-5693-11f0-8662-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:56:26,914", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:56:26 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:56:26,922", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:27,319", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:27,320", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'keywords' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:56:27,320", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'keywords', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:56:27,320", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'keywords' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:56:27,321", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:56:27,322", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:56:27,323", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:56:27,323", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:56:27,324", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:56:28,057", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:28,991", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:28,992", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 365}
{"timestamp": "2025-07-01 15:56:28,993", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 819}
{"timestamp": "2025-07-01 15:56:28,993", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 835}
{"timestamp": "2025-07-01 15:56:28,994", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 1249", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 15:56:28,994", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 870}
{"timestamp": "2025-07-01 15:56:30,357", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:56:30,358", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 603", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 872}
{"timestamp": "2025-07-01 15:56:30,358", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 4 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 876}
{"timestamp": "2025-07-01 15:56:30,358", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 4 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 879}
{"timestamp": "2025-07-01 15:56:30,359", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 4 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 367}
{"timestamp": "2025-07-01 15:56:30,359", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:56:30,359", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:56:30,360", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=4", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:56:30,362", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread 40666573-6a95-4277-b986-9ac48e6f5d92, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:56:30,362", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread 40666573-6a95-4277-b986-9ac48e6f5d92", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:56:35,033", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Cleaning up 1 idle models", "module": "model_memory_manager", "function": "_cleanup_idle_models", "line": 338}
{"timestamp": "2025-07-01 15:56:35,313", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 534.09MB", "module": "model_memory_manager", "function": "_unload_model", "line": 260}
{"timestamp": "2025-07-01 15:58:14,514", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 15:58:14,721", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:58:14,723", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:58:14,734", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 452}
{"timestamp": "2025-07-01 15:58:14,736", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'how many incidents are in system'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 15:58:14,737", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 15:58:14,738", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:58:14,739", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'incidents', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:58:14,739", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:58:14,739", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:58:14,739", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:58:14,741", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 7, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:58:14,742", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:58:14,873", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:58:14,874", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:58:14,874", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:58:15,018", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,080", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,179", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,199", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,376", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,377", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 15:58:15,377", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'original': 'how many incidents are in system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:58:15,378", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:58:15,522", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,627", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,674", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,716", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,747", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:15,748", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'original' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 15:58:15,748", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'keywords': 'many incidents system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:58:15,748", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:58:15,761", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '98'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '32e6afe6-5694-11f0-9013-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:58:15,867", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:58:15 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:58:16,280", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:17,245", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:17,247", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'keywords' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:58:17,247", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'keywords', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:58:17,247", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'keywords' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:58:17,247", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:58:17,248", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:58:17,249", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:58:17,249", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:58:17,249", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:58:17,371", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:18,471", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:18,472", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:58:18,472", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:58:18,472", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=8", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:58:18,503", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:58:18,504", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:58:23,564", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 15:58:23,596", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:58:23,597", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:58:23,598", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 15:58:23,598", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'which are these?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 15:58:23,599", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'which are these? (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 15:58:23,599", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'which are these? (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:58:23,599", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['which', 'these']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:58:23,600", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:58:23,600", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:58:23,600", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:58:23,601", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 9, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:58:23,602", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:58:23,652", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:58:23,652", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'which are these? (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:58:23,653", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:58:23,658", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '105'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '379b970a-5694-11f0-9c41-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:58:23,743", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:58:23 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:58:23,785", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:24,864", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:24,866", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:58:24,866", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:58:24,866", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:58:24,866", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:58:24,866", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:58:24,879", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:58:24,879", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:58:24,879", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:58:24,961", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:25,996", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:25,997", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 365}
{"timestamp": "2025-07-01 15:58:25,997", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 819}
{"timestamp": "2025-07-01 15:58:26,029", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 835}
{"timestamp": "2025-07-01 15:58:26,049", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 1041", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 15:58:26,052", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 870}
{"timestamp": "2025-07-01 15:58:27,218", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:27,219", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 507", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 872}
{"timestamp": "2025-07-01 15:58:27,220", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 3 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 876}
{"timestamp": "2025-07-01 15:58:27,220", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 3 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 879}
{"timestamp": "2025-07-01 15:58:27,220", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 3 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 367}
{"timestamp": "2025-07-01 15:58:27,220", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:58:27,221", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:58:27,221", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=10", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:58:27,622", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:58:27,627", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 15:58:32,221", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 15:58:32,270", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 15:58:32,272", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 15:58:32,272", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: ^(tell me more|more about|what about|how about)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 15:58:32,272", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'tell me more about them'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 15:58:32,274", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'them detailed information'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 15:58:32,274", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'them detailed information'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 15:58:32,274", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['them']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 15:58:32,274", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 15:58:32,274", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 15:58:32,275", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 15:58:32,276", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 11, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 15:58:32,276", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 15:58:32,308", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 15:58:32,309", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'them detailed information'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 15:58:32,309", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 15:58:32,315", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '102'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '3cc49d68-5694-11f0-861e-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 15:58:32,427", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 15:58:32 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 15:58:32,615", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:33,772", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:33,773", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 15:58:33,773", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 15:58:33,773", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 15:58:33,775", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 15:58:33,775", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 15:58:33,776", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 15:58:33,776", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 15:58:33,776", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 15:58:33,867", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:35,046", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 15:58:35,116", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 15:58:35,122", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 15:58:35,344", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=12", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 15:58:35,499", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 15:58:35,500", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 16:03:22,395", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 16:03:22,409", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 16:03:22,410", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 16:03:22,410", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 452}
{"timestamp": "2025-07-01 16:03:22,412", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'how many incidents are in system'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 16:03:22,412", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 16:03:22,412", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 16:03:22,413", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'incidents', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 16:03:22,413", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 16:03:22,413", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 16:03:22,413", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 16:03:22,413", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 13, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 16:03:22,413", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 16:03:22,415", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 16:03:22,415", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:03:22,415", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:03:22,446", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,471", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,481", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,498", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,522", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,523", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 16:03:22,525", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'original': 'how many incidents are in system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:03:22,526", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:03:22,534", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,542", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,560", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,576", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,598", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:22,601", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'original' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 16:03:22,601", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'keywords': 'many incidents system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:03:22,601", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:03:22,610", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '98'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': 'e9cc3230-5694-11f0-9ae7-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 16:03:22,752", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 16:03:22 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 16:03:22,775", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:23,705", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:23,710", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'keywords' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 16:03:23,710", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'keywords', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 16:03:23,710", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'keywords' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 16:03:23,710", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 16:03:23,711", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 16:03:23,713", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 16:03:23,713", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 16:03:23,713", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 16:03:23,963", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:24,969", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:24,971", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 16:03:24,972", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 16:03:24,972", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=14", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 16:03:24,976", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 16:03:24,976", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 16:03:30,024", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 16:03:30,029", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 16:03:30,029", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 16:03:30,031", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 16:03:30,031", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'which are these?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 16:03:30,032", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'which are these? (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 16:03:30,032", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'which are these? (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 16:03:30,032", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['which', 'these']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 16:03:30,032", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 16:03:30,032", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 16:03:30,033", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 16:03:30,033", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 15, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 16:03:30,033", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 16:03:30,035", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 16:03:30,035", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'which are these? (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:03:30,035", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:03:30,042", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '105'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': 'ee3a234d-5694-11f0-85a7-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 16:03:30,129", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 16:03:30 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 16:03:30,148", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:30,986", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:30,993", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 16:03:30,994", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 16:03:30,994", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 16:03:30,994", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 16:03:30,995", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 16:03:30,996", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 16:03:30,996", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 16:03:30,997", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 16:03:31,229", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:32,337", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:32,338", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 365}
{"timestamp": "2025-07-01 16:03:32,342", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 819}
{"timestamp": "2025-07-01 16:03:32,342", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 835}
{"timestamp": "2025-07-01 16:03:32,342", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 1092", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 16:03:32,343", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 870}
{"timestamp": "2025-07-01 16:03:33,500", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:33,501", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 689", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 872}
{"timestamp": "2025-07-01 16:03:33,502", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 4 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 876}
{"timestamp": "2025-07-01 16:03:33,502", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 4 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 879}
{"timestamp": "2025-07-01 16:03:33,502", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 4 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 367}
{"timestamp": "2025-07-01 16:03:33,503", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 16:03:33,504", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 16:03:33,504", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=16", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 16:03:33,506", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 16:03:33,507", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 16:03:37,553", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 16:03:37,574", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 16:03:37,575", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 16:03:37,582", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: ^(tell me more|more about|what about|how about)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 16:03:37,582", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'tell me more about them'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 16:03:37,582", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'them detailed information'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 16:03:37,582", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'them detailed information'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 16:03:37,584", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['them']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 16:03:37,584", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 16:03:37,584", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 16:03:37,584", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 16:03:37,585", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 17, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 16:03:37,585", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 16:03:37,596", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 16:03:37,597", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'them detailed information'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:03:37,597", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:03:37,604", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '102'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': 'f2bc2298-5694-11f0-ae18-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 16:03:37,694", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 16:03:37 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 16:03:37,714", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:38,879", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:38,880", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 16:03:38,881", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 16:03:38,881", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 16:03:38,881", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 16:03:38,882", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 16:03:38,883", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 16:03:38,883", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 16:03:38,884", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 16:03:39,031", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:40,006", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:03:40,008", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 16:03:40,008", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 16:03:40,009", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=18", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 16:03:40,010", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 16:03:40,010", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 16:04:49,871", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 16:04:49,874", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 16:04:49,875", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 16:04:49,875", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Conversation has history - treating with partial context", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 452}
{"timestamp": "2025-07-01 16:04:49,877", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'how many incidents are in system'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 16:04:49,877", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 16:04:49,878", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 16:04:49,878", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['many', 'incidents', 'system']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 16:04:49,879", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 16:04:49,879", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 16:04:49,879", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 16:04:49,880", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 19, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 16:04:49,880", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 16:04:49,883", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 16:04:49,883", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'how many incidents are in system (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:04:49,884", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:04:49,901", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:49,916", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:49,933", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:49,946", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:49,967", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:49,968", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 16:04:49,970", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'original': 'how many incidents are in system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:04:49,970", "level": "INFO", "logger": "root", "message": "Query type detected: aggregation", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:04:49,977", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:49,982", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:49,994", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:50,004", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:50,010", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/scroll \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:50,012", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'original' returned no sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 242}
{"timestamp": "2025-07-01 16:04:50,012", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'keywords': 'many incidents system'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:04:50,013", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:04:50,025", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '98'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '1de69902-5695-11f0-87e2-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 16:04:50,115", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 16:04:49 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 16:04:50,131", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:50,967", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:51,023", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'keywords' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 16:04:51,023", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'keywords', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 16:04:51,024", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'keywords' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 16:04:51,024", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 16:04:51,025", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 16:04:51,029", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 16:04:51,030", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 16:04:51,030", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 16:04:51,327", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:52,392", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:52,395", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 16:04:52,396", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 16:04:52,397", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=20", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 16:04:52,399", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 16:04:52,400", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 16:04:57,435", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 16:04:57,438", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 16:04:57,439", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 16:04:57,440", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: (that|this|those|these|it|them)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 16:04:57,440", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'which are these?'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 16:04:57,441", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'which are these? (context: )'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 16:04:57,441", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'which are these? (context: )'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 16:04:57,442", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['which', 'these']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 16:04:57,442", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 16:04:57,442", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 16:04:57,442", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 16:04:57,442", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 21, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 16:04:57,442", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 16:04:57,445", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 16:04:57,445", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'which are these? (context: )'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:04:57,446", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:04:57,451", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '105'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '2253c4ee-5695-11f0-9333-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 16:04:57,550", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 16:04:57 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 16:04:57,584", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:58,414", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:58,416", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 16:04:58,416", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 16:04:58,416", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 16:04:58,416", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 16:04:58,417", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 16:04:58,418", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 16:04:58,418", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 16:04:58,419", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 16:04:58,661", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:59,757", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:04:59,758", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\ude80 Generating follow-up questions", "module": "conversation_nodes", "function": "generate_response", "line": 365}
{"timestamp": "2025-07-01 16:04:59,759", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83d\udd25 FOLLOW-UP QUESTIONS METHOD CALLED \ud83d\udd25", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 819}
{"timestamp": "2025-07-01 16:04:59,759", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Starting follow-up question generation", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 835}
{"timestamp": "2025-07-01 16:04:59,759", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Built context info with length: 1181", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 844}
{"timestamp": "2025-07-01 16:04:59,760", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Sending prompt to LLM for follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 870}
{"timestamp": "2025-07-01 16:05:00,910", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:05:00,911", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "LLM response length: 670", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 872}
{"timestamp": "2025-07-01 16:05:00,911", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Parsed 4 questions from LLM response", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 876}
{"timestamp": "2025-07-01 16:05:00,911", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generated 4 contextual follow-up questions", "module": "conversation_nodes", "function": "_generate_follow_up_questions", "line": 879}
{"timestamp": "2025-07-01 16:05:00,912", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf Generated 4 suggested questions", "module": "conversation_nodes", "function": "generate_response", "line": 367}
{"timestamp": "2025-07-01 16:05:00,912", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 16:05:00,912", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 16:05:00,913", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=22", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 16:05:00,915", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 16:05:00,915", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 16:05:04,959", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Retrieved existing conversation state for thread enhanced_test_thread", "module": "conversation_graph", "function": "_get_or_create_state", "line": 274}
{"timestamp": "2025-07-01 16:05:04,964", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing greeting node", "module": "conversation_nodes", "function": "greet_user", "line": 34}
{"timestamp": "2025-07-01 16:05:04,964", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing intent understanding node", "module": "conversation_nodes", "function": "understand_intent", "line": 48}
{"timestamp": "2025-07-01 16:05:04,965", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual pattern matched: ^(tell me more|more about|what about|how about)", "module": "conversation_nodes", "function": "_is_contextual_query", "line": 442}
{"timestamp": "2025-07-01 16:05:04,965", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Building contextual query from: 'tell me more about them'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 460}
{"timestamp": "2025-07-01 16:05:04,965", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Enhanced query to: 'them detailed information'", "module": "conversation_nodes", "function": "_build_contextual_query", "line": 524}
{"timestamp": "2025-07-01 16:05:04,966", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Contextual query detected. Enhanced: 'them detailed information'", "module": "conversation_nodes", "function": "understand_intent", "line": 76}
{"timestamp": "2025-07-01 16:05:04,966", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Intent: information_seeking, Keywords: ['them']", "module": "conversation_nodes", "function": "understand_intent", "line": 158}
{"timestamp": "2025-07-01 16:05:04,966", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Current phase after intent: ConversationPhase.SEARCHING", "module": "conversation_nodes", "function": "understand_intent", "line": 159}
{"timestamp": "2025-07-01 16:05:04,966", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is contextual: True", "module": "conversation_nodes", "function": "understand_intent", "line": 160}
{"timestamp": "2025-07-01 16:05:04,967", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Is info seeking: True", "module": "conversation_nodes", "function": "understand_intent", "line": 161}
{"timestamp": "2025-07-01 16:05:04,967", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Routing after understanding - intent: information_seeking, turn: 23, phase: ConversationPhase.SEARCHING", "module": "conversation_graph", "function": "_route_after_understanding", "line": 115}
{"timestamp": "2025-07-01 16:05:04,967", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Information seeking intent - routing to SEARCH", "module": "conversation_graph", "function": "_route_after_understanding", "line": 137}
{"timestamp": "2025-07-01 16:05:04,968", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing knowledge search node", "module": "conversation_nodes", "function": "search_knowledge", "line": 166}
{"timestamp": "2025-07-01 16:05:04,969", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Search strategy 'enhanced': 'them detailed information'", "module": "conversation_nodes", "function": "search_knowledge", "line": 216}
{"timestamp": "2025-07-01 16:05:04,970", "level": "INFO", "logger": "root", "message": "Query type detected: semantic_search", "module": "qdrant_query_engine", "function": "process_query", "line": 24}
{"timestamp": "2025-07-01 16:05:04,975", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '102'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '26cfd6bf-5695-11f0-ae2e-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 16:05:05,052", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 16:05:04 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 16:05:05,062", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST http://localhost:6333/collections/rag_documents/points/search \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:05:06,083", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:05:06,084", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Strategy 'enhanced' found 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 227}
{"timestamp": "2025-07-01 16:05:06,084", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Good results from strategy 'enhanced', using these", "module": "conversation_nodes", "function": "search_knowledge", "line": 239}
{"timestamp": "2025-07-01 16:05:06,086", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Using results from strategy 'enhanced' with 5 sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 253}
{"timestamp": "2025-07-01 16:05:06,086", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing 5 sources from search result", "module": "conversation_nodes", "function": "search_knowledge", "line": 263}
{"timestamp": "2025-07-01 16:05:06,086", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Found 5 relevant sources", "module": "conversation_nodes", "function": "search_knowledge", "line": 287}
{"timestamp": "2025-07-01 16:05:06,086", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "\ud83c\udfaf GENERATE_RESPONSE METHOD CALLED \ud83c\udfaf", "module": "conversation_nodes", "function": "generate_response", "line": 315}
{"timestamp": "2025-07-01 16:05:06,086", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Processing response generation node", "module": "conversation_nodes", "function": "generate_response", "line": 316}
{"timestamp": "2025-07-01 16:05:06,088", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Generating contextual response with 5 search results", "module": "conversation_nodes", "function": "_generate_contextual_response", "line": 587}
{"timestamp": "2025-07-01 16:05:06,268", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:05:07,304", "level": "INFO", "logger": "httpx", "message": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:05:07,307", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated using sources: ['unknown', 'unknown', 'unknown']", "module": "conversation_nodes", "function": "generate_response", "line": 380}
{"timestamp": "2025-07-01 16:05:07,308", "level": "INFO", "logger": "src.conversation.conversation_nodes", "message": "Response generated successfully", "module": "conversation_nodes", "function": "generate_response", "line": 384}
{"timestamp": "2025-07-01 16:05:07,308", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Ending conversation: phase=ConversationPhase.RESPONDING, intent=information_seeking, turns=24", "module": "conversation_graph", "function": "_route_conversation_end", "line": 198}
{"timestamp": "2025-07-01 16:05:07,309", "level": "INFO", "logger": "src.conversation.conversation_graph", "message": "Conversation processed successfully for thread enhanced_test_thread, phase: ConversationPhase.RESPONDING", "module": "conversation_graph", "function": "process_message", "line": 236}
{"timestamp": "2025-07-01 16:05:07,310", "level": "INFO", "logger": "src.conversation.conversation_manager", "message": "Processed message for thread enhanced_test_thread", "module": "conversation_manager", "function": "process_user_message", "line": 74}
{"timestamp": "2025-07-01 16:07:38,596", "level": "INFO", "logger": "root", "message": "RAG System initialization started", "module": "system_init", "function": "initialize_system", "line": 222}
{"timestamp": "2025-07-01 16:07:38,601", "level": "INFO", "logger": "root", "message": "Created data directories: ['data', 'data/metadata', 'data/metadata/config', 'data/vectors', 'data/uploads', 'data/logs', 'data/backups', 'logs']", "module": "system_init", "function": "create_data_directories", "line": 102}
{"timestamp": "2025-07-01 16:07:38,601", "level": "INFO", "logger": "root", "message": "Validating system requirements...", "module": "system_init", "function": "validate_system_requirements", "line": 108}
{"timestamp": "2025-07-01 16:07:38,604", "level": "INFO", "logger": "root", "message": "System requirements validation completed", "module": "system_init", "function": "validate_system_requirements", "line": 152}
{"timestamp": "2025-07-01 16:07:38,607", "level": "INFO", "logger": "root", "message": "Error tracking initialized", "module": "system_init", "function": "initialize_error_tracking", "line": 169}
{"timestamp": "2025-07-01 16:07:38,608", "level": "INFO", "logger": "root", "message": "Initializing core components...", "module": "system_init", "function": "initialize_system", "line": 241}
{"timestamp": "2025-07-01 16:07:39,857", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:07:39,862", "level": "INFO", "logger": "httpx", "message": "HTTP Request: GET http://localhost:6333/collections \"HTTP/1.1 200 OK\"", "module": "_client", "function": "_send_single_request", "line": 1025}
{"timestamp": "2025-07-01 16:07:39,866", "level": "INFO", "logger": "root", "message": "Using existing collection: rag_documents", "module": "qdrant_store", "function": "_init_collection", "line": 84}
{"timestamp": "2025-07-01 16:07:39,867", "level": "INFO", "logger": "root", "message": "Qdrant store initialized: localhost:6333/rag_documents", "module": "qdrant_store", "function": "__init__", "line": 67}
{"timestamp": "2025-07-01 16:07:40,228", "level": "INFO", "logger": "root", "message": "Loaded Azure AI Inference client with model: Cohere-embed-v3-english", "module": "embedder", "function": "_load_client", "line": 178}
{"timestamp": "2025-07-01 16:07:40,229", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'\nRequest method: 'POST'\nRequest headers:\n    'Content-Type': 'application/json'\n    'Content-Length': '81'\n    'Accept': 'application/json'\n    'x-ms-client-request-id': '83597e42-5695-11f0-abae-000d3a9b67b4'\n    'api-key': 'REDACTED'\n    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'\n    'Authorization': 'REDACTED'\nA body is sent with the request", "module": "_universal", "function": "on_request", "line": 508}
{"timestamp": "2025-07-01 16:07:40,351", "level": "INFO", "logger": "azure.core.pipeline.policies.http_logging_policy", "message": "Response status: 200\nResponse headers:\n    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'\n    'Transfer-Encoding': 'chunked'\n    'Content-Type': 'application/json'\n    'Content-Encoding': 'REDACTED'\n    'Vary': 'REDACTED'\n    'pragma': 'no-cache'\n    'apim-request-id': 'REDACTED'\n    'request-context': 'REDACTED'\n    'num_chars': 'REDACTED'\n    'num_tokens': 'REDACTED'\n    'prompt_token_len': 'REDACTED'\n    'sampling_token_len': 'REDACTED'\n    'x-content-type-options': 'REDACTED'\n    'x-ms-region': 'REDACTED'\n    'Strict-Transport-Security': 'REDACTED'\n    'Date': 'Tue, 01 Jul 2025 16:07:40 GMT'", "module": "_universal", "function": "on_response", "line": 547}
{"timestamp": "2025-07-01 16:07:40,359", "level": "INFO", "logger": "root", "message": "Azure embedding dimension: 1024", "module": "embedder", "function": "_load_client", "line": 187}
{"timestamp": "2025-07-01 16:07:40,359", "level": "INFO", "logger": "root", "message": "Embedder initialized with provider: azure", "module": "embedder", "function": "__init__", "line": 243}
{"timestamp": "2025-07-01 16:07:40,696", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq (timeout: 30s)", "module": "llm_client", "function": "__init__", "line": 196}
{"timestamp": "2025-07-01 16:07:40,698", "level": "INFO", "logger": "root", "message": "LLM client initialized: groq", "module": "system_init", "function": "initialize_system", "line": 270}
{"timestamp": "2025-07-01 16:07:40,702", "level": "INFO", "logger": "root", "message": "\u2705 RAG System initialization completed successfully", "module": "system_init", "function": "initialize_system", "line": 282}
{"timestamp": "2025-07-01 16:07:40,704", "level": "INFO", "logger": "root", "message": "System Configuration: {'environment': 'development', 'debug_mode': True, 'data_directory': 'data', 'embedding_model': 'Cohere-embed-v3-english', 'llm_provider': 'groq', 'llm_model': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'api_host': '0.0.0.0', 'api_port': 8000, 'registered_services': ['embedder', 'ingestion_verifier', 'metadata_store', 'reranker', 'vector_store', 'log_store', 'query_enhancer', 'ingestion_debugger', 'config_manager', 'servicenow_integration', 'chunker', 'json_store', 'ingestion_engine', 'verified_ingestion_engine', 'faiss_store', 'conversation_manager', 'llm_client', 'query_engine']}", "module": "system_init", "function": "log_system_info", "line": 319}
{"timestamp": "2025-07-01 16:07:40,712", "level": "INFO", "logger": "root", "message": "Setting up monitoring...", "module": "main", "function": "main", "line": 77}
{"timestamp": "2025-07-01 16:07:40,713", "level": "INFO", "logger": "root", "message": "Monitoring setup completed", "module": "setup", "function": "setup_monitoring", "line": 18}
{"timestamp": "2025-07-01 16:07:40,714", "level": "INFO", "logger": "root", "message": "Initializing heartbeat monitor...", "module": "main", "function": "main", "line": 87}
{"timestamp": "2025-07-01 16:07:40,714", "level": "INFO", "logger": "src.monitoring.heartbeat_monitor", "message": "Heartbeat monitor initialized", "module": "heartbeat_monitor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 16:07:40,714", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor initialized successfully", "module": "main", "function": "main", "line": 89}
{"timestamp": "2025-07-01 16:07:40,716", "level": "INFO", "logger": "root", "message": "Initializing folder monitor...", "module": "main", "function": "main", "line": 98}
{"timestamp": "2025-07-01 16:07:40,717", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 16:07:40,717", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 16:07:40,718", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor initialized successfully", "module": "main", "function": "main", "line": 101}
{"timestamp": "2025-07-01 16:07:41,065", "level": "INFO", "logger": "root", "message": "Initializing enhanced folder monitor...", "module": "main", "function": "main", "line": 110}
{"timestamp": "2025-07-01 16:07:41,065", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Loaded folder monitoring config: 0 folders, 60s interval", "module": "folder_monitor", "function": "_load_config", "line": 95}
{"timestamp": "2025-07-01 16:07:41,067", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Folder monitor initialized", "module": "folder_monitor", "function": "__init__", "line": 76}
{"timestamp": "2025-07-01 16:07:41,067", "level": "INFO", "logger": "src.monitoring.folder_monitor", "message": "Enhanced folder monitor initialized with pipeline verification", "module": "enhanced_folder_monitor", "function": "__init__", "line": 84}
{"timestamp": "2025-07-01 16:07:41,067", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitor initialized successfully", "module": "main", "function": "main", "line": 113}
{"timestamp": "2025-07-01 16:07:41,094", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor registered with API", "module": "main", "function": "main", "line": 122}
{"timestamp": "2025-07-01 16:07:41,095", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor registered with API", "module": "main", "function": "main", "line": 130}
{"timestamp": "2025-07-01 16:07:41,096", "level": "INFO", "logger": "root", "message": "Creating FastAPI application...", "module": "main", "function": "main", "line": 136}
{"timestamp": "2025-07-01 16:07:41,096", "level": "INFO", "logger": "root", "message": "ResourceManager initialized", "module": "resource_manager", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 16:07:41,097", "level": "INFO", "logger": "root", "message": "Initialized GlobalRAGSystem lifecycle manager", "module": "resource_manager", "function": "__init__", "line": 390}
{"timestamp": "2025-07-01 16:07:41,108", "level": "INFO", "logger": "root", "message": "Starting GlobalRAGSystem...", "module": "resource_manager", "function": "startup", "line": 398}
{"timestamp": "2025-07-01 16:07:41,109", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_main", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 16:07:41,109", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'main' with 4 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 16:07:41,110", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_io", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 16:07:41,110", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'io' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 16:07:41,111", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_compute", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 16:07:41,111", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'compute' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 16:07:41,111", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_background", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 16:07:41,111", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'background' with 2 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 16:07:41,111", "level": "INFO", "logger": "root", "message": "GlobalRAGSystem startup completed successfully", "module": "resource_manager", "function": "startup", "line": 411}
{"timestamp": "2025-07-01 16:07:41,112", "level": "INFO", "logger": "root", "message": "Registered resource: threadpool_api_operations", "module": "resource_manager", "function": "register_resource", "line": 50}
{"timestamp": "2025-07-01 16:07:41,118", "level": "INFO", "logger": "root", "message": "Created managed thread pool 'api_operations' with 8 workers", "module": "resource_manager", "function": "__init__", "line": 229}
{"timestamp": "2025-07-01 16:07:41,118", "level": "INFO", "logger": "root", "message": "\u2705 Heartbeat monitor set in API: <class 'src.monitoring.heartbeat_monitor.HeartbeatMonitor'>", "module": "main", "function": "create_api_app", "line": 227}
{"timestamp": "2025-07-01 16:07:41,119", "level": "INFO", "logger": "root", "message": "\u2705 Folder monitor set in API: <class 'src.monitoring.folder_monitor.FolderMonitor'>", "module": "main", "function": "create_api_app", "line": 240}
{"timestamp": "2025-07-01 16:07:41,122", "level": "INFO", "logger": "src.storage.feedback_store", "message": "Feedback store initialized at: data\\feedback_store.db", "module": "feedback_store", "function": "__init__", "line": 29}
{"timestamp": "2025-07-01 16:07:41,158", "level": "INFO", "logger": "root", "message": "\u2705 Management API routes registered", "module": "main", "function": "create_api_app", "line": 2341}
{"timestamp": "2025-07-01 16:07:41,174", "level": "WARNING", "logger": "root", "message": "\u26a0\ufe0f ServiceNow API routes not available: No module named 'rag_system'", "module": "main", "function": "create_api_app", "line": 2356}
{"timestamp": "2025-07-01 16:07:41,192", "level": "INFO", "logger": "root", "message": "\u2705 Conversation API routes registered", "module": "main", "function": "create_api_app", "line": 2365}
{"timestamp": "2025-07-01 16:07:41,212", "level": "INFO", "logger": "root", "message": "\u2705 Verification API routes registered", "module": "main", "function": "create_api_app", "line": 2373}
{"timestamp": "2025-07-01 16:07:41,216", "level": "INFO", "logger": "root", "message": "\u2705 Enhanced folder monitoring API routes registered", "module": "main", "function": "create_api_app", "line": 2383}
{"timestamp": "2025-07-01 16:07:41,218", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 16:07:41,496", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s", "module": "model_memory_manager", "function": "__init__", "line": 95}
{"timestamp": "2025-07-01 16:07:41,497", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Loading model: semantic_chunker_all-MiniLM-L6-v2", "module": "model_memory_manager", "function": "get_model", "line": 117}
{"timestamp": "2025-07-01 16:07:41,498", "level": "INFO", "logger": "root", "message": "Loading sentence transformer: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "load_model", "line": 340}
{"timestamp": "2025-07-01 16:07:41,499", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Use pytorch device_name: cpu", "module": "SentenceTransformer", "function": "__init__", "line": 211}
{"timestamp": "2025-07-01 16:07:41,499", "level": "INFO", "logger": "sentence_transformers.SentenceTransformer", "message": "Load pretrained SentenceTransformer: all-MiniLM-L6-v2", "module": "SentenceTransformer", "function": "__init__", "line": 219}
{"timestamp": "2025-07-01 16:07:42,710", "level": "INFO", "logger": "src.core.model_memory_manager", "message": "Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.21s. Current memory: 519.82MB", "module": "model_memory_manager", "function": "get_model", "line": 144}
{"timestamp": "2025-07-01 16:07:42,711", "level": "INFO", "logger": "root", "message": "Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2", "module": "semantic_chunker", "function": "_initialize_model", "line": 345}
{"timestamp": "2025-07-01 16:07:42,711", "level": "INFO", "logger": "root", "message": "Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200", "module": "chunker", "function": "__init__", "line": 47}
{"timestamp": "2025-07-01 16:07:42,711", "level": "INFO", "logger": "root", "message": "Progress tracker initialized", "module": "progress_tracker", "function": "__init__", "line": 172}
{"timestamp": "2025-07-01 16:07:42,711", "level": "INFO", "logger": "root", "message": "DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']", "module": "ingestion_engine", "function": "__init__", "line": 45}
{"timestamp": "2025-07-01 16:07:42,711", "level": "INFO", "logger": "root", "message": "Azure AI config added to processor config", "module": "ingestion_engine", "function": "__init__", "line": 54}
{"timestamp": "2025-07-01 16:07:42,711", "level": "INFO", "logger": "ExcelProcessor", "message": "Excel processor initialized with Azure AI support", "module": "excel_processor", "function": "__init__", "line": 85}
{"timestamp": "2025-07-01 16:07:42,714", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ExcelProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 16:07:42,714", "level": "INFO", "logger": "root", "message": "Extracted Azure AI config from general config", "module": "pdf_processor", "function": "create_pdf_processor", "line": 82}
{"timestamp": "2025-07-01 16:07:43,076", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 16:07:43,076", "level": "INFO", "logger": "root", "message": "Azure AI client created successfully for PDF processing", "module": "pdf_processor", "function": "create_pdf_processor", "line": 93}
{"timestamp": "2025-07-01 16:07:43,076", "level": "INFO", "logger": "root", "message": "Azure CV endpoint: https://computervision1298.cognitiveservices.azure...", "module": "pdf_processor", "function": "create_pdf_processor", "line": 94}
{"timestamp": "2025-07-01 16:07:43,076", "level": "INFO", "logger": "root", "message": "Using EnhancedPDFProcessor with Azure AI integration", "module": "pdf_processor", "function": "create_pdf_processor", "line": 106}
{"timestamp": "2025-07-01 16:07:43,076", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 16:07:43,078", "level": "INFO", "logger": "WordProcessor", "message": "Word processor initialized", "module": "word_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 16:07:43,078", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: WordProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 16:07:43,078", "level": "INFO", "logger": "ImageProcessor", "message": "Image processor initialized", "module": "image_processor", "function": "__init__", "line": 26}
{"timestamp": "2025-07-01 16:07:43,079", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ImageProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 16:07:43,079", "level": "INFO", "logger": "ServiceNowProcessor", "message": "ServiceNow processor initialized", "module": "servicenow_processor", "function": "__init__", "line": 38}
{"timestamp": "2025-07-01 16:07:43,079", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: ServiceNowProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 16:07:43,079", "level": "INFO", "logger": "TextProcessor", "message": "Text processor initialized", "module": "text_processor", "function": "__init__", "line": 30}
{"timestamp": "2025-07-01 16:07:43,079", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: TextProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 16:07:43,082", "level": "INFO", "logger": "src.integrations.azure_ai.azure_client", "message": "Azure Computer Vision client initialized successfully", "module": "azure_client", "function": "_initialize_clients", "line": 78}
{"timestamp": "2025-07-01 16:07:43,083", "level": "INFO", "logger": "src.ingestion.processors.base_processor", "message": "Registered processor: EnhancedPDFProcessor", "module": "base_processor", "function": "register", "line": 90}
{"timestamp": "2025-07-01 16:07:43,083", "level": "INFO", "logger": "root", "message": "Enhanced PDF Processor with Azure CV registered successfully", "module": "ingestion_engine", "function": "__init__", "line": 73}
{"timestamp": "2025-07-01 16:07:43,083", "level": "INFO", "logger": "root", "message": "Processor registry initialized with 7 processors", "module": "ingestion_engine", "function": "__init__", "line": 77}
{"timestamp": "2025-07-01 16:07:43,084", "level": "INFO", "logger": "root", "message": "Ingestion engine initialized with managed metadata", "module": "ingestion_engine", "function": "__init__", "line": 89}
{"timestamp": "2025-07-01 16:07:43,084", "level": "INFO", "logger": "root", "message": "\u2705 Progress tracker initialized successfully", "module": "main", "function": "create_api_app", "line": 2440}
{"timestamp": "2025-07-01 16:07:43,086", "level": "INFO", "logger": "root", "message": "FastAPI application created", "module": "main", "function": "create_api_app", "line": 2637}
{"timestamp": "2025-07-01 16:07:43,086", "level": "INFO", "logger": "root", "message": "\u2705 Thread pool captured for cleanup", "module": "main", "function": "main", "line": 145}
{"timestamp": "2025-07-01 16:07:43,087", "level": "INFO", "logger": "root", "message": "FastAPI application created successfully", "module": "main", "function": "main", "line": 149}
{"timestamp": "2025-07-01 16:07:43,087", "level": "INFO", "logger": "root", "message": "Heartbeat monitoring disabled in config", "module": "main", "function": "main", "line": 163}
{"timestamp": "2025-07-01 16:07:43,088", "level": "INFO", "logger": "root", "message": "No folders configured for monitoring", "module": "main", "function": "main", "line": 184}
{"timestamp": "2025-07-01 16:07:43,088", "level": "INFO", "logger": "root", "message": "Starting server on 0.0.0.0:8000", "module": "main", "function": "main", "line": 195}
{"timestamp": "2025-07-01 16:07:43,167", "level": "INFO", "logger": "root", "message": "\ud83d\ude80 RAG System API starting up with managed resources...", "module": "main", "function": "startup_event", "line": 2393}
{"timestamp": "2025-07-01 16:07:43,167", "level": "INFO", "logger": "root", "message": "Registered resource: feedback_store", "module": "resource_manager", "function": "register_resource", "line": 50}
2025-07-01 17:15:41,181 - root - INFO - ResourceManager initialized
2025-07-01 17:15:41,183 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-01 17:15:41,236 - root - INFO - Starting RAGSystem...
2025-07-01 17:15:41,246 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:15:41,246 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:15:41,247 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:15:41,247 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:15:41,247 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:15:41,247 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:15:41,248 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:15:41,248 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:15:41,248 - root - INFO - RAGSystem startup completed successfully
2025-07-01 17:15:41,253 - root - INFO - Registered resource: config_manager
2025-07-01 17:15:41,262 - root - INFO - Registered resource: container
2025-07-01 17:15:41,262 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-01 17:15:41,263 - root - INFO - Registered resource: heartbeat_monitor
2025-07-01 17:15:41,263 - root - INFO - ResourceManager initialized
2025-07-01 17:15:41,263 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:15:41,263 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:15:41,264 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:15:41,264 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:15:41,264 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:15:41,264 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:15:41,264 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:15:41,265 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:15:41,265 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:15:41,265 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:15:41,265 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:15:41,265 - root - INFO - Registered resource: threadpool_api_operations
2025-07-01 17:15:41,265 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-01 17:15:41,288 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-01 17:15:41,403 - root - INFO - Progress tracker initialized
2025-07-01 17:15:59,141 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-01 17:15:59,141 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-01 17:15:59,142 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-01 17:15:59,143 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:15:59,149 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-01 17:16:03,718 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 4.58s. Current memory: 469.45MB
2025-07-01 17:16:03,723 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-01 17:16:03,723 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-01 17:16:04,049 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-01 17:16:04,066 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': '116d7d67-569f-11f0-a596-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-01 17:16:04,219 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Tue, 01 Jul 2025 17:16:04 GMT'
2025-07-01 17:16:04,223 - root - INFO - Azure embedding dimension: 1024
2025-07-01 17:16:04,226 - root - INFO - Embedder initialized with provider: azure
2025-07-01 17:16:06,746 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-01 17:16:06,756 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-01 17:16:06,758 - root - INFO - Using existing collection: rag_documents
2025-07-01 17:16:06,759 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-01 17:16:06,778 - root - INFO - Progress tracker initialized
2025-07-01 17:16:06,778 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-01 17:16:06,778 - root - INFO - Azure AI config added to processor config
2025-07-01 17:16:06,778 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-01 17:16:06,778 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-01 17:16:06,778 - root - INFO - Extracted Azure AI config from general config
2025-07-01 17:16:06,871 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:16:06,873 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-01 17:16:06,873 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-01 17:16:06,873 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-01 17:16:06,874 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:16:06,874 - WordProcessor - INFO - Word processor initialized
2025-07-01 17:16:06,874 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-01 17:16:06,874 - ImageProcessor - INFO - Image processor initialized
2025-07-01 17:16:06,878 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-01 17:16:06,878 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-01 17:16:06,878 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-01 17:16:06,879 - TextProcessor - INFO - Text processor initialized
2025-07-01 17:16:06,879 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-01 17:16:06,883 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:16:06,883 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:16:06,883 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-01 17:16:06,884 - root - INFO - Processor registry initialized with 7 processors
2025-07-01 17:16:06,884 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-01 17:16:06,889 - root - INFO - FastAPI application created
2025-07-01 17:16:06,913 - root - INFO - Loading model 'embedder_Cohere-embed-v3-english' of type SentenceTransformer
2025-07-01 17:16:06,914 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:16:06,923 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: Cohere-embed-v3-english
2025-07-01 17:16:06,975 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name sentence-transformers/Cohere-embed-v3-english. Creating a new one with mean pooling.
2025-07-01 17:16:06,992 - root - ERROR - Failed to load model 'embedder_Cohere-embed-v3-english': sentence-transformers/Cohere-embed-v3-english is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
2025-07-01 17:16:07,267 - root - INFO - Registered resource: uvicorn_server
2025-07-01 17:16:07,756 - rag_system.src.core.model_memory_manager - INFO - Shutting down model memory manager...
2025-07-01 17:16:08,236 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 512.53MB
2025-07-01 17:16:08,795 - rag_system.src.core.model_memory_manager - INFO - Model memory manager shutdown complete
2025-07-01 17:16:08,799 - root - INFO - Shutting down GlobalRAGSystem...
2025-07-01 17:16:09,755 - root - INFO - Pre-shutdown stats: {'total_resources': 5, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations'], 'memory_usage_mb': 512.546875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}
2025-07-01 17:16:09,756 - root - INFO - Starting comprehensive resource cleanup...
2025-07-01 17:16:09,756 - root - INFO - Shutting down thread pool 'api_operations' with 8 workers
2025-07-01 17:16:09,756 - root - INFO - Thread pool 'api_operations' shutdown completed
2025-07-01 17:16:09,756 - root - INFO - Custom cleanup completed for threadpool_api_operations
2025-07-01 17:16:09,757 - root - INFO - Shutting down thread pool: threadpool_api_operations
2025-07-01 17:16:09,757 - root - INFO - Cleaned up resource: threadpool_api_operations
2025-07-01 17:16:09,757 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-01 17:16:09,757 - root - INFO - Thread pool 'background' shutdown completed
2025-07-01 17:16:09,757 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-01 17:16:09,757 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-01 17:16:09,757 - root - INFO - Cleaned up resource: threadpool_background
2025-07-01 17:16:09,757 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-01 17:16:09,758 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-01 17:16:09,758 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-01 17:16:09,759 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-01 17:16:09,759 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-01 17:16:09,759 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-01 17:16:09,759 - root - INFO - Thread pool 'io' shutdown completed
2025-07-01 17:16:09,759 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-01 17:16:09,760 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-01 17:16:09,760 - root - INFO - Cleaned up resource: threadpool_io
2025-07-01 17:16:09,760 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-01 17:16:09,760 - root - INFO - Thread pool 'main' shutdown completed
2025-07-01 17:16:09,761 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-01 17:16:09,761 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-01 17:16:09,761 - root - INFO - Cleaned up resource: threadpool_main
2025-07-01 17:16:10,202 - root - INFO - Resource cleanup completed
2025-07-01 17:16:10,634 - root - INFO - GlobalRAGSystem shutdown completed
2025-07-01 17:16:10,634 - root - INFO - Shutting down RAGSystem...
2025-07-01 17:16:11,234 - root - INFO - Pre-shutdown stats: {'total_resources': 8, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'config_manager', 'container', 'heartbeat_monitor', 'uvicorn_server'], 'memory_usage_mb': 513.08203125, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}
2025-07-01 17:16:11,235 - root - INFO - Starting comprehensive resource cleanup...
2025-07-01 17:16:11,244 - root - INFO - Custom cleanup completed for uvicorn_server
2025-07-01 17:16:11,245 - root - INFO - Shutting down thread pool: uvicorn_server
2025-07-01 17:16:11,246 - root - ERROR - Error in generic cleanup for uvicorn_server: Server.shutdown() got an unexpected keyword argument 'wait'
2025-07-01 17:16:11,246 - root - INFO - Cleaned up resource: uvicorn_server
2025-07-01 17:16:11,247 - root - ERROR - Error in cleanup handler for heartbeat_monitor: I/O operation on closed file.
2025-07-01 17:16:11,247 - root - INFO - Cleaned up resource: heartbeat_monitor
2025-07-01 17:16:11,247 - root - INFO - Custom cleanup completed for container
2025-07-01 17:16:11,247 - root - INFO - Cleaned up resource: container
2025-07-01 17:16:11,248 - root - INFO - Cleaned up resource: config_manager
2025-07-01 17:16:11,248 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-01 17:16:11,248 - root - INFO - Thread pool 'background' shutdown completed
2025-07-01 17:16:11,248 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-01 17:16:11,248 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-01 17:16:11,249 - root - INFO - Cleaned up resource: threadpool_background
2025-07-01 17:16:11,249 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-01 17:16:11,249 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-01 17:16:11,249 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-01 17:16:11,249 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-01 17:16:11,250 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-01 17:16:11,250 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-01 17:16:11,250 - root - INFO - Thread pool 'io' shutdown completed
2025-07-01 17:16:11,250 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-01 17:16:11,250 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-01 17:16:11,250 - root - INFO - Cleaned up resource: threadpool_io
2025-07-01 17:16:11,251 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-01 17:16:11,251 - root - INFO - Thread pool 'main' shutdown completed
2025-07-01 17:16:11,251 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-01 17:16:11,251 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-01 17:16:11,251 - root - INFO - Cleaned up resource: threadpool_main
2025-07-01 17:16:11,664 - root - INFO - Resource cleanup completed
2025-07-01 17:16:12,091 - root - INFO - RAGSystem shutdown completed
2025-07-01 17:16:13,005 - root - INFO - ResourceManager initialized
2025-07-01 17:16:13,006 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:16:13,006 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:16:13,006 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:16:13,006 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:16:13,007 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:16:13,007 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:16:13,007 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:16:13,007 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:16:13,007 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:16:13,008 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:16:13,008 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:16:13,008 - root - INFO - Cleaned up semantic chunker model: all-MiniLM-L6-v2
2025-07-01 17:17:25,819 - root - INFO - ResourceManager initialized
2025-07-01 17:17:25,819 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-01 17:17:25,823 - root - INFO - Starting RAGSystem...
2025-07-01 17:17:25,823 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:17:25,823 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:17:25,823 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:17:25,825 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:17:25,825 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:17:25,825 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:17:25,825 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:17:25,825 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:17:25,826 - root - INFO - RAGSystem startup completed successfully
2025-07-01 17:17:25,827 - root - INFO - Registered resource: config_manager
2025-07-01 17:17:25,827 - root - INFO - Registered resource: container
2025-07-01 17:17:25,829 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-01 17:17:25,829 - root - INFO - Registered resource: heartbeat_monitor
2025-07-01 17:17:25,829 - root - INFO - ResourceManager initialized
2025-07-01 17:17:25,829 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:17:25,830 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:17:25,830 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:17:25,830 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:17:25,830 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:17:25,831 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:17:25,831 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:17:25,831 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:17:25,831 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:17:25,831 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:17:25,831 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:17:25,832 - root - INFO - Registered resource: threadpool_api_operations
2025-07-01 17:17:25,832 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-01 17:17:25,853 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-01 17:17:25,988 - root - INFO - Progress tracker initialized
2025-07-01 17:17:38,820 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-01 17:17:38,820 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-01 17:17:38,820 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-01 17:17:38,822 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:17:38,822 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-01 17:17:41,119 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 2.30s. Current memory: 468.46MB
2025-07-01 17:17:41,127 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-01 17:17:41,127 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-01 17:17:41,387 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-01 17:17:41,388 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': '4b6f96ce-569f-11f0-8555-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-01 17:17:41,541 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Tue, 01 Jul 2025 17:17:40 GMT'
2025-07-01 17:17:41,545 - root - INFO - Azure embedding dimension: 1024
2025-07-01 17:17:41,545 - root - INFO - Embedder initialized with provider: azure
2025-07-01 17:17:43,338 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-01 17:17:43,356 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-01 17:17:43,358 - root - INFO - Using existing collection: rag_documents
2025-07-01 17:17:43,359 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-01 17:17:43,390 - root - INFO - Progress tracker initialized
2025-07-01 17:17:43,391 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-01 17:17:43,391 - root - INFO - Azure AI config added to processor config
2025-07-01 17:17:43,391 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-01 17:17:43,391 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-01 17:17:43,391 - root - INFO - Extracted Azure AI config from general config
2025-07-01 17:17:43,474 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:17:43,475 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-01 17:17:43,475 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-01 17:17:43,475 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-01 17:17:43,475 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:17:43,475 - WordProcessor - INFO - Word processor initialized
2025-07-01 17:17:43,475 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-01 17:17:43,476 - ImageProcessor - INFO - Image processor initialized
2025-07-01 17:17:43,476 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-01 17:17:43,476 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-01 17:17:43,477 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-01 17:17:43,477 - TextProcessor - INFO - Text processor initialized
2025-07-01 17:17:43,477 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-01 17:17:43,479 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:17:43,479 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:17:43,479 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-01 17:17:43,479 - root - INFO - Processor registry initialized with 7 processors
2025-07-01 17:17:43,479 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-01 17:17:43,486 - root - INFO - FastAPI application created
2025-07-01 17:17:43,517 - root - INFO - Loading model 'embedder_Cohere-embed-v3-english' of type SentenceTransformer
2025-07-01 17:17:43,518 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:17:43,518 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: Cohere-embed-v3-english
2025-07-01 17:17:43,541 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name sentence-transformers/Cohere-embed-v3-english. Creating a new one with mean pooling.
2025-07-01 17:17:43,565 - root - ERROR - Failed to load model 'embedder_Cohere-embed-v3-english': sentence-transformers/Cohere-embed-v3-english is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
2025-07-01 17:17:43,607 - root - INFO - Registered resource: uvicorn_server
2025-07-01 17:17:43,676 - root - INFO - Registered resource: feedback_store
2025-07-01 17:23:38,833 - rag_system.src.core.model_memory_manager - INFO - Cleaning up 1 idle models
2025-07-01 17:23:39,171 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 510.63MB
2025-07-01 17:35:36,655 - root - INFO - ResourceManager initialized
2025-07-01 17:35:36,660 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-01 17:35:36,662 - root - INFO - Starting RAGSystem...
2025-07-01 17:35:36,663 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:35:36,663 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:35:36,663 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:35:36,663 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:35:36,663 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:35:36,663 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:35:36,663 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:35:36,663 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:35:36,663 - root - INFO - RAGSystem startup completed successfully
2025-07-01 17:35:36,665 - root - INFO - Registered resource: config_manager
2025-07-01 17:35:36,665 - root - INFO - Registered resource: container
2025-07-01 17:35:36,665 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-01 17:35:36,665 - root - INFO - Registered resource: heartbeat_monitor
2025-07-01 17:35:36,665 - root - INFO - ResourceManager initialized
2025-07-01 17:35:36,665 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:35:36,665 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:35:36,665 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:35:36,665 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:35:36,666 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:35:36,666 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:35:36,666 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:35:36,666 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:35:36,666 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:35:36,666 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:35:36,666 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:35:36,666 - root - INFO - Registered resource: threadpool_api_operations
2025-07-01 17:35:36,666 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-01 17:35:36,683 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-01 17:35:36,821 - root - INFO - Progress tracker initialized
2025-07-01 17:35:47,606 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-01 17:35:47,607 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-01 17:35:47,607 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-01 17:35:47,607 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:35:47,621 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-01 17:35:49,877 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 2.27s. Current memory: 469.05MB
2025-07-01 17:35:49,879 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-01 17:35:49,879 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-01 17:35:50,016 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-01 17:35:50,025 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': 'd4508954-56a1-11f0-8fd6-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-01 17:35:50,123 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Tue, 01 Jul 2025 17:35:49 GMT'
2025-07-01 17:35:50,127 - root - INFO - Azure embedding dimension: 1024
2025-07-01 17:35:50,127 - root - INFO - Embedder initialized with provider: azure
2025-07-01 17:35:51,720 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-01 17:35:51,726 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-01 17:35:51,728 - root - INFO - Using existing collection: rag_documents
2025-07-01 17:35:51,729 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-01 17:35:51,747 - root - INFO - Progress tracker initialized
2025-07-01 17:35:51,747 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-01 17:35:51,748 - root - INFO - Azure AI config added to processor config
2025-07-01 17:35:51,748 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-01 17:35:51,748 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-01 17:35:51,748 - root - INFO - Extracted Azure AI config from general config
2025-07-01 17:35:51,851 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:35:51,852 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-01 17:35:51,852 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-01 17:35:51,852 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-01 17:35:51,852 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:35:51,852 - WordProcessor - INFO - Word processor initialized
2025-07-01 17:35:51,852 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-01 17:35:51,852 - ImageProcessor - INFO - Image processor initialized
2025-07-01 17:35:51,854 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-01 17:35:51,854 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-01 17:35:51,854 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-01 17:35:51,854 - TextProcessor - INFO - Text processor initialized
2025-07-01 17:35:51,854 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-01 17:35:51,856 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:35:51,856 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:35:51,857 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-01 17:35:51,857 - root - INFO - Processor registry initialized with 7 processors
2025-07-01 17:35:51,857 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-01 17:35:51,874 - root - INFO - FastAPI application created
2025-07-01 17:35:51,879 - root - INFO - Loading model 'embedder_Cohere-embed-v3-english' of type SentenceTransformer
2025-07-01 17:35:51,880 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:35:51,880 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: Cohere-embed-v3-english
2025-07-01 17:35:51,898 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name sentence-transformers/Cohere-embed-v3-english. Creating a new one with mean pooling.
2025-07-01 17:35:51,917 - root - ERROR - Failed to load model 'embedder_Cohere-embed-v3-english': sentence-transformers/Cohere-embed-v3-english is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
2025-07-01 17:35:51,990 - root - INFO - Registered resource: uvicorn_server
2025-07-01 17:35:52,043 - root - INFO - Registered resource: feedback_store
2025-07-01 17:35:53,522 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 17:36:45,960 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 17:37:16,445 - root - INFO - ResourceManager initialized
2025-07-01 17:37:16,465 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-01 17:37:16,470 - root - INFO - Starting RAGSystem...
2025-07-01 17:37:16,470 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:37:16,470 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:37:16,470 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:37:16,472 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:37:16,475 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:37:16,476 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:37:16,476 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:37:16,476 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:37:16,476 - root - INFO - RAGSystem startup completed successfully
2025-07-01 17:37:16,478 - root - INFO - Registered resource: config_manager
2025-07-01 17:37:16,478 - root - INFO - Registered resource: container
2025-07-01 17:37:16,478 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-01 17:37:16,478 - root - INFO - Registered resource: heartbeat_monitor
2025-07-01 17:37:16,484 - root - INFO - ResourceManager initialized
2025-07-01 17:37:16,484 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:37:16,484 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:37:16,484 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:37:16,484 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:37:16,484 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:37:16,484 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:37:16,484 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:37:16,484 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:37:16,484 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:37:16,484 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:37:16,484 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:37:16,484 - root - INFO - Registered resource: threadpool_api_operations
2025-07-01 17:37:16,484 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-01 17:37:16,510 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-01 17:37:16,697 - root - INFO - Progress tracker initialized
2025-07-01 17:37:29,825 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-01 17:37:29,826 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-01 17:37:29,826 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-01 17:37:29,826 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:37:29,826 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-01 17:37:34,309 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 17:37:36,188 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 6.36s. Current memory: 468.53MB
2025-07-01 17:37:36,198 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-01 17:37:36,200 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-01 17:37:36,361 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-01 17:37:36,368 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': '13b33798-56a2-11f0-8901-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-01 17:37:36,508 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Tue, 01 Jul 2025 17:37:36 GMT'
2025-07-01 17:37:36,513 - root - INFO - Azure embedding dimension: 1024
2025-07-01 17:37:36,514 - root - INFO - Embedder initialized with provider: azure
2025-07-01 17:37:37,935 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-01 17:37:37,941 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-01 17:37:37,946 - root - INFO - Using existing collection: rag_documents
2025-07-01 17:37:37,946 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-01 17:37:37,976 - root - INFO - Progress tracker initialized
2025-07-01 17:37:37,976 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-01 17:37:37,976 - root - INFO - Azure AI config added to processor config
2025-07-01 17:37:37,976 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-01 17:37:37,976 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-01 17:37:37,977 - root - INFO - Extracted Azure AI config from general config
2025-07-01 17:37:38,073 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:37:38,074 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-01 17:37:38,074 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-01 17:37:38,074 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-01 17:37:38,074 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:37:38,075 - WordProcessor - INFO - Word processor initialized
2025-07-01 17:37:38,075 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-01 17:37:38,075 - ImageProcessor - INFO - Image processor initialized
2025-07-01 17:37:38,075 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-01 17:37:38,075 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-01 17:37:38,076 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-01 17:37:38,076 - TextProcessor - INFO - Text processor initialized
2025-07-01 17:37:38,076 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-01 17:37:38,078 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:37:38,078 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:37:38,078 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-01 17:37:38,078 - root - INFO - Processor registry initialized with 7 processors
2025-07-01 17:37:38,078 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-01 17:37:38,090 - root - INFO - FastAPI application created
2025-07-01 17:37:38,096 - root - INFO - Loading model 'embedder_Cohere-embed-v3-english' of type SentenceTransformer
2025-07-01 17:37:38,098 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:37:38,098 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: Cohere-embed-v3-english
2025-07-01 17:37:38,157 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name sentence-transformers/Cohere-embed-v3-english. Creating a new one with mean pooling.
2025-07-01 17:37:38,213 - root - ERROR - Failed to load model 'embedder_Cohere-embed-v3-english': sentence-transformers/Cohere-embed-v3-english is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
2025-07-01 17:37:38,260 - root - INFO - Registered resource: uvicorn_server
2025-07-01 17:37:38,384 - root - INFO - Registered resource: feedback_store
2025-07-01 17:37:38,922 - root - INFO - Final system stats: {'app_name': 'GlobalRAGSystem', 'startup_complete': True, 'shutdown_complete': False, 'thread_pools': {'main': {'max_workers': 4, 'shutdown': False}, 'io': {'max_workers': 8, 'shutdown': False}, 'compute': {'max_workers': 2, 'shutdown': False}, 'background': {'max_workers': 2, 'shutdown': False}, 'api_operations': {'max_workers': 8, 'shutdown': False}}, 'resources': {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 511.1796875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}, 'models': {'loaded_models': [], 'total_models': 0, 'memory_usage': {}, 'total_memory_mb': 0.0}, 'timestamp': 1751391458.922601}
2025-07-01 17:37:38,931 - root - INFO - Shutting down RAGSystem...
2025-07-01 17:37:39,434 - root - INFO - Pre-shutdown stats: {'total_resources': 8, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'config_manager', 'container', 'heartbeat_monitor', 'uvicorn_server'], 'memory_usage_mb': 511.8046875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}
2025-07-01 17:37:39,440 - root - INFO - Starting comprehensive resource cleanup...
2025-07-01 17:37:39,446 - root - INFO - Custom cleanup completed for uvicorn_server
2025-07-01 17:37:39,446 - root - INFO - Shutting down thread pool: uvicorn_server
2025-07-01 17:37:39,446 - root - ERROR - Error in generic cleanup for uvicorn_server: Server.shutdown() got an unexpected keyword argument 'wait'
2025-07-01 17:37:39,446 - root - INFO - Cleaned up resource: uvicorn_server
2025-07-01 17:37:39,449 - root - INFO - Custom cleanup completed for heartbeat_monitor
2025-07-01 17:37:39,449 - root - INFO - Cleaned up resource: heartbeat_monitor
2025-07-01 17:37:39,449 - root - INFO - Custom cleanup completed for container
2025-07-01 17:37:39,449 - root - INFO - Cleaned up resource: container
2025-07-01 17:37:39,450 - root - INFO - Cleaned up resource: config_manager
2025-07-01 17:37:39,450 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-01 17:37:39,450 - root - INFO - Thread pool 'background' shutdown completed
2025-07-01 17:37:39,450 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-01 17:37:39,451 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-01 17:37:39,451 - root - INFO - Cleaned up resource: threadpool_background
2025-07-01 17:37:39,451 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-01 17:37:39,451 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-01 17:37:39,451 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-01 17:37:39,451 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-01 17:37:39,451 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-01 17:37:39,451 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-01 17:37:39,451 - root - INFO - Thread pool 'io' shutdown completed
2025-07-01 17:37:39,451 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-01 17:37:39,451 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-01 17:37:39,451 - root - INFO - Cleaned up resource: threadpool_io
2025-07-01 17:37:39,452 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-01 17:37:39,452 - root - INFO - Thread pool 'main' shutdown completed
2025-07-01 17:37:39,452 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-01 17:37:39,452 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-01 17:37:39,452 - root - INFO - Cleaned up resource: threadpool_main
2025-07-01 17:37:39,797 - root - INFO - Resource cleanup completed
2025-07-01 17:37:40,103 - root - INFO - RAGSystem shutdown completed
2025-07-01 17:37:40,118 - rag_system.src.core.model_memory_manager - INFO - Shutting down model memory manager...
2025-07-01 17:37:40,426 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 511.88MB
2025-07-01 17:37:40,745 - rag_system.src.core.model_memory_manager - INFO - Model memory manager shutdown complete
2025-07-01 17:37:40,749 - root - INFO - Shutting down GlobalRAGSystem...
2025-07-01 17:37:41,200 - root - INFO - Pre-shutdown stats: {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 511.87890625, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 12}
2025-07-01 17:37:41,201 - root - INFO - Starting comprehensive resource cleanup...
2025-07-01 17:37:41,201 - root - INFO - Custom cleanup completed for feedback_store
2025-07-01 17:37:41,201 - root - INFO - Cleaned up resource: feedback_store
2025-07-01 17:37:41,202 - root - INFO - Shutting down thread pool 'api_operations' with 8 workers
2025-07-01 17:37:41,203 - root - INFO - Thread pool 'api_operations' shutdown completed
2025-07-01 17:37:41,203 - root - INFO - Custom cleanup completed for threadpool_api_operations
2025-07-01 17:37:41,203 - root - INFO - Shutting down thread pool: threadpool_api_operations
2025-07-01 17:37:41,203 - root - INFO - Cleaned up resource: threadpool_api_operations
2025-07-01 17:37:41,203 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-01 17:37:41,203 - root - INFO - Thread pool 'background' shutdown completed
2025-07-01 17:37:41,208 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-01 17:37:41,208 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-01 17:37:41,208 - root - INFO - Cleaned up resource: threadpool_background
2025-07-01 17:37:41,208 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-01 17:37:41,208 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-01 17:37:41,208 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-01 17:37:41,208 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-01 17:37:41,208 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-01 17:37:41,208 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-01 17:37:41,208 - root - INFO - Thread pool 'io' shutdown completed
2025-07-01 17:37:41,208 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-01 17:37:41,208 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-01 17:37:41,208 - root - INFO - Cleaned up resource: threadpool_io
2025-07-01 17:37:41,209 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-01 17:37:41,209 - root - INFO - Thread pool 'main' shutdown completed
2025-07-01 17:37:41,209 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-01 17:37:41,209 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-01 17:37:41,209 - root - INFO - Cleaned up resource: threadpool_main
2025-07-01 17:37:41,764 - root - INFO - Resource cleanup completed
2025-07-01 17:37:42,286 - root - INFO - GlobalRAGSystem shutdown completed
2025-07-01 17:37:43,279 - root - INFO - ResourceManager initialized
2025-07-01 17:37:43,284 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:37:43,287 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:37:43,288 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:37:43,288 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:37:43,288 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:37:43,288 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:37:43,288 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:37:43,288 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:37:43,289 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:37:43,289 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:37:43,289 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:37:43,289 - root - INFO - Cleaned up semantic chunker model: all-MiniLM-L6-v2
2025-07-01 17:41:47,646 - rag_system.src.core.model_memory_manager - INFO - Cleaning up 1 idle models
2025-07-01 17:41:48,006 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 512.43MB
2025-07-01 17:44:34,991 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 17:44:41,243 - root - INFO - ResourceManager initialized
2025-07-01 17:44:41,245 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-01 17:44:41,270 - root - INFO - Starting RAGSystem...
2025-07-01 17:44:41,270 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:44:41,270 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:44:41,270 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:44:41,271 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:44:41,271 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:44:41,271 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:44:41,271 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:44:41,271 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:44:41,271 - root - INFO - RAGSystem startup completed successfully
2025-07-01 17:44:41,272 - root - INFO - Registered resource: config_manager
2025-07-01 17:44:41,273 - root - INFO - Registered resource: container
2025-07-01 17:44:41,273 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-01 17:44:41,273 - root - INFO - Registered resource: heartbeat_monitor
2025-07-01 17:44:41,274 - root - INFO - ResourceManager initialized
2025-07-01 17:44:41,274 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:44:41,275 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:44:41,275 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:44:41,275 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:44:41,275 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:44:41,276 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:44:41,276 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:44:41,283 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:44:41,285 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:44:41,285 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:44:41,287 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:44:41,288 - root - INFO - Registered resource: threadpool_api_operations
2025-07-01 17:44:41,289 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-01 17:44:41,338 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-01 17:44:41,617 - root - INFO - Progress tracker initialized
2025-07-01 17:44:51,045 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-01 17:44:51,048 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-01 17:44:51,049 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-01 17:44:51,051 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:44:51,051 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-01 17:44:53,341 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 2.29s. Current memory: 468.73MB
2025-07-01 17:44:53,344 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-01 17:44:53,344 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-01 17:44:53,466 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-01 17:44:53,468 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': '183b1a55-56a3-11f0-b9fe-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-01 17:44:53,567 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Tue, 01 Jul 2025 17:44:53 GMT'
2025-07-01 17:44:53,571 - root - INFO - Azure embedding dimension: 1024
2025-07-01 17:44:53,571 - root - INFO - Embedder initialized with provider: azure
2025-07-01 17:44:54,869 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-01 17:44:54,882 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-01 17:44:54,884 - root - INFO - Using existing collection: rag_documents
2025-07-01 17:44:54,885 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-01 17:44:54,897 - root - INFO - Progress tracker initialized
2025-07-01 17:44:54,897 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-01 17:44:54,898 - root - INFO - Azure AI config added to processor config
2025-07-01 17:44:54,898 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-01 17:44:54,898 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-01 17:44:54,898 - root - INFO - Extracted Azure AI config from general config
2025-07-01 17:44:54,975 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:44:54,978 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-01 17:44:54,978 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-01 17:44:54,978 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-01 17:44:54,979 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:44:54,979 - WordProcessor - INFO - Word processor initialized
2025-07-01 17:44:54,979 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-01 17:44:54,979 - ImageProcessor - INFO - Image processor initialized
2025-07-01 17:44:54,979 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-01 17:44:54,980 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-01 17:44:54,980 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-01 17:44:54,981 - TextProcessor - INFO - Text processor initialized
2025-07-01 17:44:54,984 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-01 17:44:54,986 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:44:54,987 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:44:54,988 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-01 17:44:54,988 - root - INFO - Processor registry initialized with 7 processors
2025-07-01 17:44:54,989 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-01 17:44:54,995 - root - INFO - FastAPI application created
2025-07-01 17:44:55,050 - root - INFO - Loading model 'embedder_Cohere-embed-v3-english' of type SentenceTransformer
2025-07-01 17:44:55,052 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:44:55,053 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: Cohere-embed-v3-english
2025-07-01 17:44:55,111 - sentence_transformers.SentenceTransformer - WARNING - No sentence-transformers model found with name sentence-transformers/Cohere-embed-v3-english. Creating a new one with mean pooling.
2025-07-01 17:44:55,165 - root - ERROR - Failed to load model 'embedder_Cohere-embed-v3-english': sentence-transformers/Cohere-embed-v3-english is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`
2025-07-01 17:44:55,207 - root - INFO - Registered resource: uvicorn_server
2025-07-01 17:44:55,284 - root - INFO - Registered resource: feedback_store
2025-07-01 17:44:55,820 - root - INFO - Final system stats: {'app_name': 'GlobalRAGSystem', 'startup_complete': True, 'shutdown_complete': False, 'thread_pools': {'main': {'max_workers': 4, 'shutdown': False}, 'io': {'max_workers': 8, 'shutdown': False}, 'compute': {'max_workers': 2, 'shutdown': False}, 'background': {'max_workers': 2, 'shutdown': False}, 'api_operations': {'max_workers': 8, 'shutdown': False}}, 'resources': {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 511.57421875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}, 'models': {'loaded_models': [], 'total_models': 0, 'memory_usage': {}, 'total_memory_mb': 0.0}, 'timestamp': 1751391895.8188174}
2025-07-01 17:44:55,845 - root - INFO - Shutting down RAGSystem...
2025-07-01 17:44:57,508 - root - INFO - Pre-shutdown stats: {'total_resources': 8, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'config_manager', 'container', 'heartbeat_monitor', 'uvicorn_server'], 'memory_usage_mb': 512.234375, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}
2025-07-01 17:44:57,513 - root - INFO - Starting comprehensive resource cleanup...
2025-07-01 17:44:57,515 - root - INFO - Custom cleanup completed for uvicorn_server
2025-07-01 17:44:57,515 - root - INFO - Shutting down thread pool: uvicorn_server
2025-07-01 17:44:57,516 - root - ERROR - Error in generic cleanup for uvicorn_server: Server.shutdown() got an unexpected keyword argument 'wait'
2025-07-01 17:44:57,516 - root - INFO - Cleaned up resource: uvicorn_server
2025-07-01 17:44:57,522 - root - INFO - Custom cleanup completed for heartbeat_monitor
2025-07-01 17:44:57,523 - root - INFO - Cleaned up resource: heartbeat_monitor
2025-07-01 17:44:57,523 - root - INFO - Custom cleanup completed for container
2025-07-01 17:44:57,523 - root - INFO - Cleaned up resource: container
2025-07-01 17:44:57,523 - root - INFO - Cleaned up resource: config_manager
2025-07-01 17:44:57,524 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-01 17:44:57,524 - root - INFO - Thread pool 'background' shutdown completed
2025-07-01 17:44:57,524 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-01 17:44:57,524 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-01 17:44:57,524 - root - INFO - Cleaned up resource: threadpool_background
2025-07-01 17:44:57,525 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-01 17:44:57,525 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-01 17:44:57,525 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-01 17:44:57,526 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-01 17:44:57,526 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-01 17:44:57,526 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-01 17:44:57,527 - root - INFO - Thread pool 'io' shutdown completed
2025-07-01 17:44:57,527 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-01 17:44:57,527 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-01 17:44:57,533 - root - INFO - Cleaned up resource: threadpool_io
2025-07-01 17:44:57,533 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-01 17:44:57,533 - root - INFO - Thread pool 'main' shutdown completed
2025-07-01 17:44:57,534 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-01 17:44:57,534 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-01 17:44:57,534 - root - INFO - Cleaned up resource: threadpool_main
2025-07-01 17:44:57,883 - root - INFO - Resource cleanup completed
2025-07-01 17:44:58,255 - root - INFO - RAGSystem shutdown completed
2025-07-01 17:44:58,275 - rag_system.src.core.model_memory_manager - INFO - Shutting down model memory manager...
2025-07-01 17:44:58,606 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 512.31MB
2025-07-01 17:44:58,955 - rag_system.src.core.model_memory_manager - INFO - Model memory manager shutdown complete
2025-07-01 17:44:58,958 - root - INFO - Shutting down GlobalRAGSystem...
2025-07-01 17:44:59,409 - root - INFO - Pre-shutdown stats: {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 512.31640625, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 12}
2025-07-01 17:44:59,426 - root - INFO - Starting comprehensive resource cleanup...
2025-07-01 17:44:59,427 - root - INFO - Custom cleanup completed for feedback_store
2025-07-01 17:44:59,427 - root - INFO - Cleaned up resource: feedback_store
2025-07-01 17:44:59,427 - root - INFO - Shutting down thread pool 'api_operations' with 8 workers
2025-07-01 17:44:59,427 - root - INFO - Thread pool 'api_operations' shutdown completed
2025-07-01 17:44:59,428 - root - INFO - Custom cleanup completed for threadpool_api_operations
2025-07-01 17:44:59,428 - root - INFO - Shutting down thread pool: threadpool_api_operations
2025-07-01 17:44:59,428 - root - INFO - Cleaned up resource: threadpool_api_operations
2025-07-01 17:44:59,428 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-01 17:44:59,428 - root - INFO - Thread pool 'background' shutdown completed
2025-07-01 17:44:59,429 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-01 17:44:59,429 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-01 17:44:59,429 - root - INFO - Cleaned up resource: threadpool_background
2025-07-01 17:44:59,429 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-01 17:44:59,429 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-01 17:44:59,429 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-01 17:44:59,430 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-01 17:44:59,430 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-01 17:44:59,430 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-01 17:44:59,430 - root - INFO - Thread pool 'io' shutdown completed
2025-07-01 17:44:59,430 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-01 17:44:59,431 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-01 17:44:59,431 - root - INFO - Cleaned up resource: threadpool_io
2025-07-01 17:44:59,431 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-01 17:44:59,431 - root - INFO - Thread pool 'main' shutdown completed
2025-07-01 17:44:59,431 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-01 17:44:59,432 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-01 17:44:59,432 - root - INFO - Cleaned up resource: threadpool_main
2025-07-01 17:44:59,733 - root - INFO - Resource cleanup completed
2025-07-01 17:45:00,075 - root - INFO - GlobalRAGSystem shutdown completed
2025-07-01 17:45:00,734 - root - INFO - ResourceManager initialized
2025-07-01 17:45:00,734 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:45:00,734 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:45:00,735 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:45:00,735 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:45:00,735 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:45:00,735 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:45:00,735 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:45:00,735 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:45:00,736 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:45:00,736 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:45:00,736 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:45:00,736 - root - INFO - Cleaned up semantic chunker model: all-MiniLM-L6-v2
2025-07-01 17:49:36,625 - root - INFO - ResourceManager initialized
2025-07-01 17:49:36,626 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-01 17:49:36,647 - root - INFO - Starting RAGSystem...
2025-07-01 17:49:36,647 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:49:36,647 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:49:36,647 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:49:36,647 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:49:36,647 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:49:36,647 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:49:36,649 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:49:36,649 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:49:36,649 - root - INFO - RAGSystem startup completed successfully
2025-07-01 17:49:36,650 - root - INFO - Registered resource: config_manager
2025-07-01 17:49:36,650 - root - INFO - Registered resource: container
2025-07-01 17:49:36,652 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-01 17:49:36,653 - root - INFO - Registered resource: heartbeat_monitor
2025-07-01 17:49:36,653 - root - INFO - ResourceManager initialized
2025-07-01 17:49:36,653 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 17:49:36,653 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 17:49:36,653 - root - INFO - Registered resource: threadpool_main
2025-07-01 17:49:36,654 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 17:49:36,654 - root - INFO - Registered resource: threadpool_io
2025-07-01 17:49:36,655 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 17:49:36,655 - root - INFO - Registered resource: threadpool_compute
2025-07-01 17:49:36,656 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 17:49:36,656 - root - INFO - Registered resource: threadpool_background
2025-07-01 17:49:36,657 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 17:49:36,657 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 17:49:36,658 - root - INFO - Registered resource: threadpool_api_operations
2025-07-01 17:49:36,658 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-01 17:49:38,560 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-01 17:49:38,994 - root - INFO - Progress tracker initialized
2025-07-01 17:49:50,016 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-01 17:49:50,016 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-01 17:49:50,018 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-01 17:49:50,019 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 17:49:50,019 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-01 17:49:52,419 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 2.40s. Current memory: 467.21MB
2025-07-01 17:49:52,421 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-01 17:49:52,422 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-01 17:49:52,557 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-01 17:49:52,557 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': 'ca80be27-56a3-11f0-892f-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-01 17:49:52,700 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Tue, 01 Jul 2025 17:49:52 GMT'
2025-07-01 17:49:52,705 - root - INFO - Azure embedding dimension: 1024
2025-07-01 17:49:52,706 - root - INFO - Embedder initialized with provider: azure
2025-07-01 17:49:54,216 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-01 17:49:54,243 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-01 17:49:54,246 - root - INFO - Using existing collection: rag_documents
2025-07-01 17:49:54,246 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-01 17:49:54,292 - root - INFO - Progress tracker initialized
2025-07-01 17:49:54,292 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-01 17:49:54,292 - root - INFO - Azure AI config added to processor config
2025-07-01 17:49:54,294 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-01 17:49:54,294 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-01 17:49:54,294 - root - INFO - Extracted Azure AI config from general config
2025-07-01 17:49:54,391 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:49:54,392 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-01 17:49:54,392 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-01 17:49:54,392 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-01 17:49:54,392 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:49:54,393 - WordProcessor - INFO - Word processor initialized
2025-07-01 17:49:54,393 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-01 17:49:54,393 - ImageProcessor - INFO - Image processor initialized
2025-07-01 17:49:54,393 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-01 17:49:54,393 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-01 17:49:54,394 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-01 17:49:54,394 - TextProcessor - INFO - Text processor initialized
2025-07-01 17:49:54,394 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-01 17:49:54,397 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 17:49:54,397 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 17:49:54,397 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-01 17:49:54,398 - root - INFO - Processor registry initialized with 7 processors
2025-07-01 17:49:54,398 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-01 17:49:54,403 - root - INFO - FastAPI application created
2025-07-01 17:49:54,479 - root - INFO - Registered resource: uvicorn_server
2025-07-01 17:49:54,560 - root - INFO - Registered resource: feedback_store
2025-07-01 17:53:43,268 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 17:53:43,270 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 18:00:52,351 - root - INFO - ResourceManager initialized
2025-07-01 18:00:52,353 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-01 18:00:52,353 - __main__ - INFO - Initializing Managed RAG System...
2025-07-01 18:00:52,353 - root - INFO - Starting RAGSystem...
2025-07-01 18:00:52,354 - root - INFO - Registered resource: threadpool_main
2025-07-01 18:00:52,354 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 18:00:52,354 - root - INFO - Registered resource: threadpool_io
2025-07-01 18:00:52,354 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 18:00:52,354 - root - INFO - Registered resource: threadpool_compute
2025-07-01 18:00:52,354 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 18:00:52,355 - root - INFO - Registered resource: threadpool_background
2025-07-01 18:00:52,355 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 18:00:52,356 - root - INFO - RAGSystem startup completed successfully
2025-07-01 18:00:52,357 - root - INFO - Registered resource: config_manager
2025-07-01 18:00:52,357 - root - INFO - Registered resource: container
2025-07-01 18:00:52,357 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-01 18:00:52,358 - root - INFO - Registered resource: heartbeat_monitor
2025-07-01 18:00:52,359 - root - INFO - ResourceManager initialized
2025-07-01 18:00:52,360 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-01 18:00:52,360 - root - INFO - Starting GlobalRAGSystem...
2025-07-01 18:00:52,360 - root - INFO - Registered resource: threadpool_main
2025-07-01 18:00:52,360 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-01 18:00:52,360 - root - INFO - Registered resource: threadpool_io
2025-07-01 18:00:52,360 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-01 18:00:52,360 - root - INFO - Registered resource: threadpool_compute
2025-07-01 18:00:52,360 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-01 18:00:52,378 - root - INFO - Registered resource: threadpool_background
2025-07-01 18:00:52,383 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-01 18:00:52,388 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-01 18:00:52,388 - root - INFO - Registered resource: threadpool_api_operations
2025-07-01 18:00:52,388 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-01 18:00:52,388 - root - INFO -  Heartbeat monitor set in API: <class 'monitoring.heartbeat_monitor.HeartbeatMonitor'>
2025-07-01 18:00:52,388 - root - WARNING -  No folder monitor instance provided to API
2025-07-01 18:00:52,391 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-01 18:00:52,449 - root - INFO -  Management API routes registered
2025-07-01 18:00:52,536 - root - WARNING -  ServiceNow API routes not available: cannot import name 'IntegrationError' from 'rag_system.src.core.error_handling' (D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\error_handling.py)
2025-07-01 18:00:52,569 - root - INFO -  Conversation API routes registered
2025-07-01 18:00:52,575 - root - INFO -  Verification API routes registered
2025-07-01 18:00:52,576 - root - INFO -  Enhanced folder monitoring API routes registered
2025-07-01 18:00:52,578 - root - INFO - Progress tracker initialized
2025-07-01 18:01:17,599 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-01 18:01:17,599 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-01 18:01:17,599 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-01 18:01:17,600 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-01 18:01:17,600 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-01 18:01:21,646 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 4.05s. Current memory: 469.75MB
2025-07-01 18:01:21,646 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-01 18:01:21,646 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-01 18:01:21,819 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-01 18:01:21,820 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': '6555e2e0-56a5-11f0-a260-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-01 18:01:21,935 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Tue, 01 Jul 2025 18:01:21 GMT'
2025-07-01 18:01:21,941 - root - INFO - Azure embedding dimension: 1024
2025-07-01 18:01:21,941 - root - INFO - Embedder initialized with provider: azure
2025-07-01 18:01:23,543 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-01 18:01:23,549 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-01 18:01:23,562 - root - INFO - Using existing collection: rag_documents
2025-07-01 18:01:23,562 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-01 18:01:23,595 - root - INFO - Progress tracker initialized
2025-07-01 18:01:23,595 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-01 18:01:23,595 - root - INFO - Azure AI config added to processor config
2025-07-01 18:01:23,595 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-01 18:01:23,596 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-01 18:01:23,596 - root - INFO - Extracted Azure AI config from general config
2025-07-01 18:01:23,698 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 18:01:23,699 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-01 18:01:23,699 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-01 18:01:23,699 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-01 18:01:23,699 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 18:01:23,699 - WordProcessor - INFO - Word processor initialized
2025-07-01 18:01:23,699 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-01 18:01:23,699 - ImageProcessor - INFO - Image processor initialized
2025-07-01 18:01:23,699 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-01 18:01:23,700 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-01 18:01:23,700 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-01 18:01:23,700 - TextProcessor - INFO - Text processor initialized
2025-07-01 18:01:23,700 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-01 18:01:23,702 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-01 18:01:23,702 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-01 18:01:23,704 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-01 18:01:23,705 - root - INFO - Processor registry initialized with 7 processors
2025-07-01 18:01:23,718 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-01 18:01:23,719 - root - ERROR -  Failed to initialize progress tracker: attempted relative import beyond top-level package
2025-07-01 18:01:23,721 - root - INFO - FastAPI application created
2025-07-01 18:01:23,723 - __main__ - INFO - Managed RAG System initialized successfully
2025-07-01 18:01:23,723 - __main__ - INFO - Starting managed services...
2025-07-01 18:01:23,723 - monitoring.heartbeat_monitor - INFO -  Heartbeat monitoring is disabled
2025-07-01 18:01:23,723 - __main__ - INFO - Heartbeat monitor started
2025-07-01 18:01:23,724 - __main__ - INFO - Loading managed models...
2025-07-01 18:01:23,724 - __main__ - INFO - Using AzureEmbedder - no managed model loading needed
2025-07-01 18:01:23,724 - __main__ - INFO - All managed services started
2025-07-01 18:01:23,724 - __main__ - INFO - Managed RAG System ready!
2025-07-01 18:01:23,724 - __main__ - INFO - Starting managed server on 0.0.0.0:8000
2025-07-01 18:01:23,767 - root - INFO - Registered resource: uvicorn_server
2025-07-01 18:01:23,827 - root - INFO -  RAG System API starting up with managed resources...
2025-07-01 18:01:23,829 - root - INFO - Registered resource: feedback_store
2025-07-01 18:06:15,511 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 18:06:15,513 - api.routes.conversation - ERROR - Failed to get ConversationManager: attempted relative import beyond top-level package
2025-07-01 18:07:17,761 - rag_system.src.core.model_memory_manager - INFO - Cleaning up 1 idle models
2025-07-01 18:07:18,264 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 509.56MB
2025-07-02 12:25:42,960 - root - INFO - ResourceManager initialized
2025-07-02 12:25:42,960 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-02 12:25:42,960 - __main__ - INFO - Initializing Managed RAG System...
2025-07-02 12:25:42,960 - root - INFO - Starting RAGSystem...
2025-07-02 12:25:42,960 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:25:42,962 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:25:42,962 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:25:42,962 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:25:42,962 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:25:42,962 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:25:42,962 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:25:42,962 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:25:42,962 - root - INFO - RAGSystem startup completed successfully
2025-07-02 12:25:42,964 - root - INFO - Registered resource: config_manager
2025-07-02 12:25:43,700 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-02 12:25:43,716 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-02 12:25:43,720 - root - INFO - Using existing collection: rag_documents
2025-07-02 12:25:43,720 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-02 12:25:43,720 - root - INFO - Registered resource: container
2025-07-02 12:25:43,723 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-02 12:25:43,723 - root - INFO - Registered resource: heartbeat_monitor
2025-07-02 12:25:43,723 - root - INFO - ResourceManager initialized
2025-07-02 12:25:43,723 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-02 12:25:43,723 - root - INFO - Starting GlobalRAGSystem...
2025-07-02 12:25:43,723 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:25:43,723 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:25:43,723 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:25:43,725 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:25:43,725 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:25:43,725 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:25:43,725 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:25:43,725 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:25:43,725 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-02 12:25:43,726 - root - INFO - Registered resource: threadpool_api_operations
2025-07-02 12:25:43,727 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-02 12:25:43,727 - root - INFO -  Heartbeat monitor set in API: <class 'monitoring.heartbeat_monitor.HeartbeatMonitor'>
2025-07-02 12:25:43,728 - root - WARNING -  No folder monitor instance provided to API
2025-07-02 12:25:43,730 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-02 12:25:43,777 - root - INFO -  Management API routes registered
2025-07-02 12:25:43,846 - root - WARNING -  ServiceNow API routes not available: cannot import name 'IntegrationError' from 'rag_system.src.core.error_handling' (D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\error_handling.py)
2025-07-02 12:25:43,904 - root - WARNING -  PowerBI and ServiceNow routers not available - skipping
2025-07-02 12:25:43,911 - root - INFO -  Enhanced folder monitoring API routes registered
2025-07-02 12:25:43,912 - root - INFO - Progress tracker initialized
2025-07-02 12:25:55,693 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-02 12:25:55,695 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-02 12:25:55,695 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-02 12:25:55,696 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-02 12:25:55,696 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-02 12:25:57,541 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.85s. Current memory: 508.52MB
2025-07-02 12:25:57,541 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-02 12:25:57,541 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-02 12:25:57,719 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-02 12:25:57,720 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': 'b4d97d93-573f-11f0-9872-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-02 12:25:57,837 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Wed, 02 Jul 2025 12:25:57 GMT'
2025-07-02 12:25:57,842 - root - INFO - Azure embedding dimension: 1024
2025-07-02 12:25:57,843 - root - INFO - Embedder initialized with provider: azure
2025-07-02 12:25:57,855 - root - INFO - Progress tracker initialized
2025-07-02 12:25:57,855 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-02 12:25:57,855 - root - INFO - Azure AI config added to processor config
2025-07-02 12:25:57,856 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-02 12:25:57,856 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-02 12:25:57,856 - root - INFO - Extracted Azure AI config from general config
2025-07-02 12:25:57,949 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:25:57,949 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-02 12:25:57,950 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-02 12:25:57,950 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-02 12:25:57,950 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:25:57,950 - WordProcessor - INFO - Word processor initialized
2025-07-02 12:25:57,950 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-02 12:25:57,951 - ImageProcessor - INFO - Image processor initialized
2025-07-02 12:25:57,952 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-02 12:25:57,952 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-02 12:25:57,962 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-02 12:25:57,963 - TextProcessor - INFO - Text processor initialized
2025-07-02 12:25:57,963 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-02 12:25:57,965 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:25:57,967 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:25:57,968 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-02 12:25:57,968 - root - INFO - Processor registry initialized with 7 processors
2025-07-02 12:25:57,968 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-02 12:25:57,968 - root - ERROR -  Failed to initialize progress tracker: attempted relative import beyond top-level package
2025-07-02 12:25:57,971 - root - INFO - FastAPI application created
2025-07-02 12:25:57,971 - __main__ - INFO - Managed RAG System initialized successfully
2025-07-02 12:25:57,972 - __main__ - INFO - Starting managed services...
2025-07-02 12:25:57,972 - monitoring.heartbeat_monitor - INFO -  Heartbeat monitoring is disabled
2025-07-02 12:25:57,972 - __main__ - INFO - Heartbeat monitor started
2025-07-02 12:25:57,973 - __main__ - INFO - Loading managed models...
2025-07-02 12:25:57,973 - __main__ - INFO - Using AzureEmbedder - no managed model loading needed
2025-07-02 12:25:57,973 - __main__ - INFO - All managed services started
2025-07-02 12:25:57,973 - __main__ - INFO - Managed RAG System ready!
2025-07-02 12:25:57,974 - __main__ - INFO - Starting managed server on 0.0.0.0:8000
2025-07-02 12:25:58,013 - root - INFO - Registered resource: uvicorn_server
2025-07-02 12:25:58,089 - root - INFO -  RAG System API starting up with managed resources...
2025-07-02 12:25:58,089 - root - INFO - Registered resource: feedback_store
2025-07-02 12:25:58,091 - root - INFO -  RAG System API shutting down - cleaning up managed resources...
2025-07-02 12:26:00,812 - root - INFO - Final system stats: {'app_name': 'GlobalRAGSystem', 'startup_complete': True, 'shutdown_complete': False, 'thread_pools': {'main': {'max_workers': 4, 'shutdown': False}, 'io': {'max_workers': 8, 'shutdown': False}, 'compute': {'max_workers': 2, 'shutdown': False}, 'background': {'max_workers': 2, 'shutdown': False}, 'api_operations': {'max_workers': 8, 'shutdown': False}}, 'resources': {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 511.1875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 14}, 'models': {'loaded_models': [], 'total_models': 0, 'memory_usage': {}, 'total_memory_mb': 0.0}, 'timestamp': 1751459160.812032}
2025-07-02 12:26:00,813 - root - INFO -  Managed resource cleanup initiated
2025-07-02 12:26:00,814 - __main__ - INFO - Shutting down Managed RAG System...
2025-07-02 12:26:00,814 - root - INFO - Shutting down RAGSystem...
2025-07-02 12:26:01,235 - root - INFO - Pre-shutdown stats: {'total_resources': 8, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'config_manager', 'container', 'heartbeat_monitor', 'uvicorn_server'], 'memory_usage_mb': 511.3046875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 14}
2025-07-02 12:26:01,235 - root - INFO - Starting comprehensive resource cleanup...
2025-07-02 12:26:01,258 - root - INFO - Custom cleanup completed for uvicorn_server
2025-07-02 12:26:01,258 - root - INFO - Shutting down thread pool: uvicorn_server
2025-07-02 12:26:01,259 - root - ERROR - Error in generic cleanup for uvicorn_server: Server.shutdown() got an unexpected keyword argument 'wait'
2025-07-02 12:26:01,259 - root - INFO - Cleaned up resource: uvicorn_server
2025-07-02 12:26:01,259 - monitoring.heartbeat_monitor - INFO -  Stopping health monitoring...
2025-07-02 12:26:01,260 - root - INFO - Custom cleanup completed for heartbeat_monitor
2025-07-02 12:26:01,260 - root - INFO - Cleaned up resource: heartbeat_monitor
2025-07-02 12:26:01,260 - root - INFO - Custom cleanup completed for container
2025-07-02 12:26:01,260 - root - INFO - Cleaned up resource: container
2025-07-02 12:26:01,260 - root - INFO - Cleaned up resource: config_manager
2025-07-02 12:26:01,260 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-02 12:26:01,260 - root - INFO - Thread pool 'background' shutdown completed
2025-07-02 12:26:01,261 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-02 12:26:01,261 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-02 12:26:01,261 - root - INFO - Cleaned up resource: threadpool_background
2025-07-02 12:26:01,261 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-02 12:26:01,261 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-02 12:26:01,261 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-02 12:26:01,261 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-02 12:26:01,262 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-02 12:26:01,262 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-02 12:26:01,262 - root - INFO - Thread pool 'io' shutdown completed
2025-07-02 12:26:01,262 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-02 12:26:01,262 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-02 12:26:01,262 - root - INFO - Cleaned up resource: threadpool_io
2025-07-02 12:26:01,262 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-02 12:26:01,262 - root - INFO - Thread pool 'main' shutdown completed
2025-07-02 12:26:01,265 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-02 12:26:01,265 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-02 12:26:01,266 - root - INFO - Cleaned up resource: threadpool_main
2025-07-02 12:26:01,531 - root - INFO - Resource cleanup completed
2025-07-02 12:26:01,800 - root - INFO - RAGSystem shutdown completed
2025-07-02 12:26:01,801 - __main__ - INFO - Managed RAG System shutdown complete
2025-07-02 12:26:01,801 - __main__ - INFO - Shutting down Managed RAG System...
2025-07-02 12:26:01,801 - __main__ - INFO - Managed RAG System shutdown complete
2025-07-02 12:26:01,803 - rag_system.src.core.model_memory_manager - INFO - Shutting down model memory manager...
2025-07-02 12:26:02,081 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 511.43MB
2025-07-02 12:26:02,328 - rag_system.src.core.model_memory_manager - INFO - Model memory manager shutdown complete
2025-07-02 12:26:02,332 - root - INFO - Shutting down GlobalRAGSystem...
2025-07-02 12:26:02,681 - root - INFO - Pre-shutdown stats: {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 511.4296875, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}
2025-07-02 12:26:02,682 - root - INFO - Starting comprehensive resource cleanup...
2025-07-02 12:26:02,682 - root - INFO - Custom cleanup completed for feedback_store
2025-07-02 12:26:02,682 - root - INFO - Cleaned up resource: feedback_store
2025-07-02 12:26:02,682 - root - INFO - Shutting down thread pool 'api_operations' with 8 workers
2025-07-02 12:26:02,682 - root - INFO - Thread pool 'api_operations' shutdown completed
2025-07-02 12:26:02,682 - root - INFO - Custom cleanup completed for threadpool_api_operations
2025-07-02 12:26:02,682 - root - INFO - Shutting down thread pool: threadpool_api_operations
2025-07-02 12:26:02,682 - root - INFO - Cleaned up resource: threadpool_api_operations
2025-07-02 12:26:02,682 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-02 12:26:02,682 - root - INFO - Thread pool 'background' shutdown completed
2025-07-02 12:26:02,684 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-02 12:26:02,684 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-02 12:26:02,684 - root - INFO - Cleaned up resource: threadpool_background
2025-07-02 12:26:02,684 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-02 12:26:02,684 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-02 12:26:02,684 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-02 12:26:02,684 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-02 12:26:02,688 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-02 12:26:02,688 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-02 12:26:02,688 - root - INFO - Thread pool 'io' shutdown completed
2025-07-02 12:26:02,689 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-02 12:26:02,689 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-02 12:26:02,689 - root - INFO - Cleaned up resource: threadpool_io
2025-07-02 12:26:02,689 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-02 12:26:02,690 - root - INFO - Thread pool 'main' shutdown completed
2025-07-02 12:26:02,690 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-02 12:26:02,690 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-02 12:26:02,690 - root - INFO - Cleaned up resource: threadpool_main
2025-07-02 12:26:02,949 - root - INFO - Resource cleanup completed
2025-07-02 12:26:03,203 - root - INFO - GlobalRAGSystem shutdown completed
2025-07-02 12:26:03,757 - root - INFO - ResourceManager initialized
2025-07-02 12:26:03,758 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-02 12:26:03,758 - root - INFO - Starting GlobalRAGSystem...
2025-07-02 12:26:03,758 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:26:03,758 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:26:03,759 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:26:03,759 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:26:03,759 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:26:03,760 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:26:03,760 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:26:03,760 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:26:03,760 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-02 12:26:03,760 - root - INFO - Cleaned up semantic chunker model: all-MiniLM-L6-v2
2025-07-02 12:26:15,644 - root - INFO - ResourceManager initialized
2025-07-02 12:26:15,646 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-02 12:26:15,647 - __main__ - INFO - Initializing Managed RAG System...
2025-07-02 12:26:15,647 - root - INFO - Starting RAGSystem...
2025-07-02 12:26:15,647 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:26:15,647 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:26:15,647 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:26:15,647 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:26:15,647 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:26:15,647 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:26:15,649 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:26:15,649 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:26:15,649 - root - INFO - RAGSystem startup completed successfully
2025-07-02 12:26:15,649 - root - INFO - Registered resource: config_manager
2025-07-02 12:26:16,384 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-02 12:26:16,419 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-02 12:26:16,420 - root - INFO - Using existing collection: rag_documents
2025-07-02 12:26:16,420 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-02 12:26:16,424 - root - INFO - Registered resource: container
2025-07-02 12:26:16,424 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-02 12:26:16,424 - root - INFO - Registered resource: heartbeat_monitor
2025-07-02 12:26:16,424 - root - INFO - ResourceManager initialized
2025-07-02 12:26:16,424 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-02 12:26:16,425 - root - INFO - Starting GlobalRAGSystem...
2025-07-02 12:26:16,425 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:26:16,425 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:26:16,425 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:26:16,426 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:26:16,426 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:26:16,426 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:26:16,426 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:26:16,429 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:26:16,429 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-02 12:26:16,429 - root - INFO - Registered resource: threadpool_api_operations
2025-07-02 12:26:16,429 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-02 12:26:16,430 - root - INFO -  Heartbeat monitor set in API: <class 'monitoring.heartbeat_monitor.HeartbeatMonitor'>
2025-07-02 12:26:16,430 - root - WARNING -  No folder monitor instance provided to API
2025-07-02 12:26:16,433 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-02 12:26:16,456 - root - INFO -  Management API routes registered
2025-07-02 12:26:16,465 - root - WARNING -  ServiceNow API routes not available: cannot import name 'IntegrationError' from 'rag_system.src.core.error_handling' (D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\error_handling.py)
2025-07-02 12:26:16,487 - root - WARNING -  PowerBI and ServiceNow routers not available - skipping
2025-07-02 12:26:16,489 - root - INFO -  Enhanced folder monitoring API routes registered
2025-07-02 12:26:16,538 - root - INFO - Progress tracker initialized
2025-07-02 12:26:25,168 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-02 12:26:25,168 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-02 12:26:25,168 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-02 12:26:25,169 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-02 12:26:25,169 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-02 12:26:26,805 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.64s. Current memory: 508.81MB
2025-07-02 12:26:26,805 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-02 12:26:26,805 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-02 12:26:27,012 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-02 12:26:27,013 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': 'c64f67f1-573f-11f0-b15c-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-02 12:26:27,197 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Wed, 02 Jul 2025 12:26:26 GMT'
2025-07-02 12:26:27,201 - root - INFO - Azure embedding dimension: 1024
2025-07-02 12:26:27,201 - root - INFO - Embedder initialized with provider: azure
2025-07-02 12:26:27,221 - root - INFO - Progress tracker initialized
2025-07-02 12:26:27,221 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-02 12:26:27,221 - root - INFO - Azure AI config added to processor config
2025-07-02 12:26:27,222 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-02 12:26:27,222 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-02 12:26:27,222 - root - INFO - Extracted Azure AI config from general config
2025-07-02 12:26:27,321 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:26:27,321 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-02 12:26:27,322 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-02 12:26:27,322 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-02 12:26:27,322 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:26:27,322 - WordProcessor - INFO - Word processor initialized
2025-07-02 12:26:27,322 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-02 12:26:27,322 - ImageProcessor - INFO - Image processor initialized
2025-07-02 12:26:27,323 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-02 12:26:27,323 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-02 12:26:27,323 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-02 12:26:27,324 - TextProcessor - INFO - Text processor initialized
2025-07-02 12:26:27,324 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-02 12:26:27,326 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:26:27,349 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:26:27,357 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-02 12:26:27,382 - root - INFO - Processor registry initialized with 7 processors
2025-07-02 12:26:27,383 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-02 12:26:27,383 - root - ERROR -  Failed to initialize progress tracker: attempted relative import beyond top-level package
2025-07-02 12:26:27,386 - root - INFO - FastAPI application created
2025-07-02 12:26:27,386 - __main__ - INFO - Managed RAG System initialized successfully
2025-07-02 12:26:27,386 - __main__ - INFO - Starting managed services...
2025-07-02 12:26:27,386 - monitoring.heartbeat_monitor - INFO -  Heartbeat monitoring is disabled
2025-07-02 12:26:27,386 - __main__ - INFO - Heartbeat monitor started
2025-07-02 12:26:27,386 - __main__ - INFO - Loading managed models...
2025-07-02 12:26:27,386 - __main__ - INFO - Using AzureEmbedder - no managed model loading needed
2025-07-02 12:26:27,387 - __main__ - INFO - All managed services started
2025-07-02 12:26:27,387 - __main__ - INFO - Managed RAG System ready!
2025-07-02 12:26:27,387 - __main__ - INFO - Starting managed server on 0.0.0.0:8000
2025-07-02 12:26:27,571 - root - INFO - Registered resource: uvicorn_server
2025-07-02 12:26:27,791 - root - INFO -  RAG System API starting up with managed resources...
2025-07-02 12:26:27,850 - root - INFO - Registered resource: feedback_store
2025-07-02 12:26:27,973 - root - INFO -  RAG System API shutting down - cleaning up managed resources...
2025-07-02 12:26:29,663 - root - INFO - Final system stats: {'app_name': 'GlobalRAGSystem', 'startup_complete': True, 'shutdown_complete': False, 'thread_pools': {'main': {'max_workers': 4, 'shutdown': False}, 'io': {'max_workers': 8, 'shutdown': False}, 'compute': {'max_workers': 2, 'shutdown': False}, 'background': {'max_workers': 2, 'shutdown': False}, 'api_operations': {'max_workers': 8, 'shutdown': False}}, 'resources': {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 511.47265625, 'cpu_percent': 0.0, 'open_files': 4, 'threads': 13}, 'models': {'loaded_models': [], 'total_models': 0, 'memory_usage': {}, 'total_memory_mb': 0.0}, 'timestamp': 1751459189.6636157}
2025-07-02 12:26:29,664 - root - INFO -  Managed resource cleanup initiated
2025-07-02 12:26:29,665 - __main__ - INFO - Shutting down Managed RAG System...
2025-07-02 12:26:29,666 - root - INFO - Shutting down RAGSystem...
2025-07-02 12:26:30,523 - root - INFO - Pre-shutdown stats: {'total_resources': 8, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'config_manager', 'container', 'heartbeat_monitor', 'uvicorn_server'], 'memory_usage_mb': 512.0625, 'cpu_percent': 0.0, 'open_files': 4, 'threads': 13}
2025-07-02 12:26:30,533 - root - INFO - Starting comprehensive resource cleanup...
2025-07-02 12:26:30,537 - root - INFO - Custom cleanup completed for uvicorn_server
2025-07-02 12:26:30,537 - root - INFO - Shutting down thread pool: uvicorn_server
2025-07-02 12:26:30,537 - root - ERROR - Error in generic cleanup for uvicorn_server: Server.shutdown() got an unexpected keyword argument 'wait'
2025-07-02 12:26:30,538 - root - INFO - Cleaned up resource: uvicorn_server
2025-07-02 12:26:30,538 - monitoring.heartbeat_monitor - INFO -  Stopping health monitoring...
2025-07-02 12:26:30,538 - root - INFO - Custom cleanup completed for heartbeat_monitor
2025-07-02 12:26:30,539 - root - INFO - Cleaned up resource: heartbeat_monitor
2025-07-02 12:26:30,539 - root - INFO - Custom cleanup completed for container
2025-07-02 12:26:30,539 - root - INFO - Cleaned up resource: container
2025-07-02 12:26:30,539 - root - INFO - Cleaned up resource: config_manager
2025-07-02 12:26:30,540 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-02 12:26:30,540 - root - INFO - Thread pool 'background' shutdown completed
2025-07-02 12:26:30,540 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-02 12:26:30,541 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-02 12:26:30,541 - root - INFO - Cleaned up resource: threadpool_background
2025-07-02 12:26:30,541 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-02 12:26:30,541 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-02 12:26:30,541 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-02 12:26:30,541 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-02 12:26:30,541 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-02 12:26:30,542 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-02 12:26:30,542 - root - INFO - Thread pool 'io' shutdown completed
2025-07-02 12:26:30,542 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-02 12:26:30,542 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-02 12:26:30,542 - root - INFO - Cleaned up resource: threadpool_io
2025-07-02 12:26:30,542 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-02 12:26:30,542 - root - INFO - Thread pool 'main' shutdown completed
2025-07-02 12:26:30,544 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-02 12:26:30,544 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-02 12:26:30,544 - root - INFO - Cleaned up resource: threadpool_main
2025-07-02 12:26:31,044 - root - INFO - Resource cleanup completed
2025-07-02 12:26:31,412 - root - INFO - RAGSystem shutdown completed
2025-07-02 12:26:31,413 - __main__ - INFO - Managed RAG System shutdown complete
2025-07-02 12:26:31,413 - __main__ - INFO - Shutting down Managed RAG System...
2025-07-02 12:26:31,413 - __main__ - INFO - Managed RAG System shutdown complete
2025-07-02 12:26:31,414 - rag_system.src.core.model_memory_manager - INFO - Shutting down model memory manager...
2025-07-02 12:26:31,771 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 512.19MB
2025-07-02 12:26:32,165 - rag_system.src.core.model_memory_manager - INFO - Model memory manager shutdown complete
2025-07-02 12:26:32,168 - root - INFO - Shutting down GlobalRAGSystem...
2025-07-02 12:26:32,761 - root - INFO - Pre-shutdown stats: {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 512.19140625, 'cpu_percent': 0.0, 'open_files': 4, 'threads': 12}
2025-07-02 12:26:32,762 - root - INFO - Starting comprehensive resource cleanup...
2025-07-02 12:26:32,762 - root - INFO - Custom cleanup completed for feedback_store
2025-07-02 12:26:32,762 - root - INFO - Cleaned up resource: feedback_store
2025-07-02 12:26:32,762 - root - INFO - Shutting down thread pool 'api_operations' with 8 workers
2025-07-02 12:26:32,762 - root - INFO - Thread pool 'api_operations' shutdown completed
2025-07-02 12:26:32,762 - root - INFO - Custom cleanup completed for threadpool_api_operations
2025-07-02 12:26:32,763 - root - INFO - Shutting down thread pool: threadpool_api_operations
2025-07-02 12:26:32,763 - root - INFO - Cleaned up resource: threadpool_api_operations
2025-07-02 12:26:32,763 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-02 12:26:32,763 - root - INFO - Thread pool 'background' shutdown completed
2025-07-02 12:26:32,763 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-02 12:26:32,763 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-02 12:26:32,763 - root - INFO - Cleaned up resource: threadpool_background
2025-07-02 12:26:32,763 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-02 12:26:32,763 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-02 12:26:32,771 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-02 12:26:32,771 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-02 12:26:32,771 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-02 12:26:32,771 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-02 12:26:32,773 - root - INFO - Thread pool 'io' shutdown completed
2025-07-02 12:26:32,773 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-02 12:26:32,773 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-02 12:26:32,773 - root - INFO - Cleaned up resource: threadpool_io
2025-07-02 12:26:32,773 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-02 12:26:32,773 - root - INFO - Thread pool 'main' shutdown completed
2025-07-02 12:26:32,773 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-02 12:26:32,775 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-02 12:26:32,775 - root - INFO - Cleaned up resource: threadpool_main
2025-07-02 12:26:33,196 - root - INFO - Resource cleanup completed
2025-07-02 12:26:33,626 - root - INFO - GlobalRAGSystem shutdown completed
2025-07-02 12:26:34,500 - root - INFO - ResourceManager initialized
2025-07-02 12:26:34,502 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-02 12:26:34,502 - root - INFO - Starting GlobalRAGSystem...
2025-07-02 12:26:34,503 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:26:34,503 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:26:34,503 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:26:34,503 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:26:34,503 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:26:34,503 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:26:34,503 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:26:34,504 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:26:34,504 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-02 12:26:34,504 - root - INFO - Cleaned up semantic chunker model: all-MiniLM-L6-v2
2025-07-02 12:29:37,820 - root - INFO - ResourceManager initialized
2025-07-02 12:29:37,821 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-02 12:29:37,822 - __main__ - INFO - Initializing Managed RAG System...
2025-07-02 12:29:37,822 - root - INFO - Starting RAGSystem...
2025-07-02 12:29:37,822 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:29:37,822 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:29:37,822 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:29:37,822 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:29:37,822 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:29:37,823 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:29:37,823 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:29:37,823 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:29:37,823 - root - INFO - RAGSystem startup completed successfully
2025-07-02 12:29:37,824 - root - INFO - Registered resource: config_manager
2025-07-02 12:29:38,546 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-02 12:29:38,576 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-02 12:29:38,577 - root - INFO - Using existing collection: rag_documents
2025-07-02 12:29:38,577 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-02 12:29:38,578 - root - INFO - Registered resource: container
2025-07-02 12:29:38,578 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-02 12:29:38,579 - root - INFO - Registered resource: heartbeat_monitor
2025-07-02 12:29:38,579 - root - INFO - ResourceManager initialized
2025-07-02 12:29:38,579 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-02 12:29:38,579 - root - INFO - Starting GlobalRAGSystem...
2025-07-02 12:29:38,579 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:29:38,579 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:29:38,581 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:29:38,583 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:29:38,583 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:29:38,583 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:29:38,584 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:29:38,584 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:29:38,584 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-02 12:29:38,584 - root - INFO - Registered resource: threadpool_api_operations
2025-07-02 12:29:38,584 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-02 12:29:38,585 - root - INFO -  Heartbeat monitor set in API: <class 'monitoring.heartbeat_monitor.HeartbeatMonitor'>
2025-07-02 12:29:38,585 - root - WARNING -  No folder monitor instance provided to API
2025-07-02 12:29:38,590 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-02 12:29:38,624 - root - INFO -  Management API routes registered
2025-07-02 12:29:38,636 - root - WARNING -  ServiceNow API routes not available: cannot import name 'IntegrationError' from 'rag_system.src.core.error_handling' (D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\error_handling.py)
2025-07-02 12:29:38,693 - root - WARNING -  PowerBI and ServiceNow routers not available - skipping
2025-07-02 12:29:38,695 - root - INFO -  Enhanced folder monitoring API routes registered
2025-07-02 12:29:38,827 - root - INFO - Progress tracker initialized
2025-07-02 12:29:46,592 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-02 12:29:46,594 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-02 12:29:46,594 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-02 12:29:46,594 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-02 12:29:46,594 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-02 12:29:48,836 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 2.24s. Current memory: 509.29MB
2025-07-02 12:29:48,837 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-02 12:29:48,837 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-02 12:29:49,087 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-02 12:29:49,087 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': '3ec18613-5740-11f0-87d4-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-02 12:29:49,206 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Wed, 02 Jul 2025 12:29:48 GMT'
2025-07-02 12:29:49,211 - root - INFO - Azure embedding dimension: 1024
2025-07-02 12:29:49,213 - root - INFO - Embedder initialized with provider: azure
2025-07-02 12:29:49,245 - root - INFO - Progress tracker initialized
2025-07-02 12:29:49,245 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-02 12:29:49,245 - root - INFO - Azure AI config added to processor config
2025-07-02 12:29:49,245 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-02 12:29:49,246 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-02 12:29:49,246 - root - INFO - Extracted Azure AI config from general config
2025-07-02 12:29:49,296 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:29:49,298 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-02 12:29:49,298 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-02 12:29:49,300 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-02 12:29:49,300 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:29:49,305 - WordProcessor - INFO - Word processor initialized
2025-07-02 12:29:49,305 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-02 12:29:49,306 - ImageProcessor - INFO - Image processor initialized
2025-07-02 12:29:49,306 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-02 12:29:49,307 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-02 12:29:49,307 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-02 12:29:49,309 - TextProcessor - INFO - Text processor initialized
2025-07-02 12:29:49,310 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-02 12:29:49,313 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:29:49,314 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:29:49,314 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-02 12:29:49,314 - root - INFO - Processor registry initialized with 7 processors
2025-07-02 12:29:49,314 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-02 12:29:49,314 - root - ERROR -  Failed to initialize progress tracker: attempted relative import beyond top-level package
2025-07-02 12:29:49,317 - root - INFO - FastAPI application created
2025-07-02 12:29:49,319 - __main__ - INFO - Managed RAG System initialized successfully
2025-07-02 12:29:49,319 - __main__ - INFO - Starting managed services...
2025-07-02 12:29:49,319 - monitoring.heartbeat_monitor - INFO -  Heartbeat monitoring is disabled
2025-07-02 12:29:49,334 - __main__ - INFO - Heartbeat monitor started
2025-07-02 12:29:49,335 - __main__ - INFO - Loading managed models...
2025-07-02 12:29:49,336 - __main__ - INFO - Using AzureEmbedder - no managed model loading needed
2025-07-02 12:29:49,336 - __main__ - INFO - All managed services started
2025-07-02 12:29:49,337 - __main__ - INFO - Managed RAG System ready!
2025-07-02 12:29:49,337 - __main__ - INFO - Starting managed server on 0.0.0.0:8000
2025-07-02 12:29:49,407 - root - INFO - Registered resource: uvicorn_server
2025-07-02 12:29:49,532 - root - INFO -  RAG System API starting up with managed resources...
2025-07-02 12:29:49,538 - root - INFO - Registered resource: feedback_store
2025-07-02 12:30:16,599 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:30:16,610 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    container = get_dependency_container()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
2025-07-02 12:32:25,171 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:32:25,171 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    container = get_dependency_container()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
2025-07-02 12:32:25,174 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:32:25,174 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    container = get_dependency_container()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
2025-07-02 12:35:46,833 - rag_system.src.core.model_memory_manager - INFO - Cleaning up 1 idle models
2025-07-02 12:35:47,291 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 512.42MB
2025-07-02 12:37:34,075 - root - INFO - ResourceManager initialized
2025-07-02 12:37:34,076 - root - INFO - Initialized RAGSystem lifecycle manager
2025-07-02 12:37:34,076 - __main__ - INFO - Initializing Managed RAG System...
2025-07-02 12:37:34,076 - root - INFO - Starting RAGSystem...
2025-07-02 12:37:34,076 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:37:34,076 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:37:34,076 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:37:34,076 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:37:34,076 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:37:34,076 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:37:34,076 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:37:34,076 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:37:34,076 - root - INFO - RAGSystem startup completed successfully
2025-07-02 12:37:34,079 - root - INFO - Registered resource: config_manager
2025-07-02 12:37:34,666 - httpx - INFO - HTTP Request: GET http://localhost:6333 "HTTP/1.1 200 OK"
2025-07-02 12:37:34,671 - httpx - INFO - HTTP Request: GET http://localhost:6333/collections "HTTP/1.1 200 OK"
2025-07-02 12:37:34,673 - root - INFO - Using existing collection: rag_documents
2025-07-02 12:37:34,673 - root - INFO - Qdrant store initialized: localhost:6333/rag_documents
2025-07-02 12:37:34,674 - root - INFO - Registered resource: container
2025-07-02 12:37:34,675 - monitoring.heartbeat_monitor - INFO - Heartbeat monitor initialized
2025-07-02 12:37:34,675 - root - INFO - Registered resource: heartbeat_monitor
2025-07-02 12:37:34,676 - root - INFO - ResourceManager initialized
2025-07-02 12:37:34,676 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-02 12:37:34,676 - root - INFO - Starting GlobalRAGSystem...
2025-07-02 12:37:34,677 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:37:34,678 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:37:34,678 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:37:34,678 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:37:34,678 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:37:34,678 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:37:34,678 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:37:34,679 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:37:34,682 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-02 12:37:34,683 - root - INFO - Registered resource: threadpool_api_operations
2025-07-02 12:37:34,683 - root - INFO - Created managed thread pool 'api_operations' with 8 workers
2025-07-02 12:37:34,684 - root - INFO -  Heartbeat monitor set in API: <class 'monitoring.heartbeat_monitor.HeartbeatMonitor'>
2025-07-02 12:37:34,684 - root - WARNING -  No folder monitor instance provided to API
2025-07-02 12:37:34,685 - storage.feedback_store - INFO - Feedback store initialized at: data\feedback_store.db
2025-07-02 12:37:34,729 - root - INFO -  Management API routes registered
2025-07-02 12:37:34,786 - root - WARNING -  ServiceNow API routes not available: cannot import name 'IntegrationError' from 'rag_system.src.core.error_handling' (D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\error_handling.py)
2025-07-02 12:37:34,805 - root - WARNING -  PowerBI and ServiceNow routers not available - skipping
2025-07-02 12:37:34,954 - root - INFO -  Enhanced folder monitoring API routes registered
2025-07-02 12:37:34,988 - root - INFO - Progress tracker initialized
2025-07-02 12:37:42,822 - rag_system.src.core.model_memory_manager - INFO - Model memory manager initialized - Max memory: 2048MB, Idle timeout: 300s
2025-07-02 12:37:42,827 - rag_system.src.core.model_memory_manager - INFO - Loading model: semantic_chunker_all-MiniLM-L6-v2
2025-07-02 12:37:42,834 - root - INFO - Loading sentence transformer: all-MiniLM-L6-v2
2025-07-02 12:37:42,960 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-07-02 12:37:42,981 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-07-02 12:37:44,680 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 loaded in 1.85s. Current memory: 508.32MB
2025-07-02 12:37:44,683 - root - INFO - Semantic chunker initialized with memory-managed model: all-MiniLM-L6-v2
2025-07-02 12:37:44,683 - root - INFO - Chunker initialized with semantic chunking (model loads on demand): size=1000, overlap=200
2025-07-02 12:37:44,835 - root - INFO - Loaded Azure AI Inference client with model: Cohere-embed-v3-english
2025-07-02 12:37:44,835 - azure.core.pipeline.policies.http_logging_policy - INFO - Request URL: 'https://azurehub1910875317.services.ai.azure.com/models/embeddings?api-version=REDACTED'
Request method: 'POST'
Request headers:
    'Content-Type': 'application/json'
    'Content-Length': '81'
    'Accept': 'application/json'
    'x-ms-client-request-id': '5a52ca7a-5741-11f0-ad5e-000d3a9b67b4'
    'api-key': 'REDACTED'
    'User-Agent': 'azsdk-python-ai-inference/1.0.0b9 Python/3.12.9 (Windows-11-10.0.26100-SP0)'
    'Authorization': 'REDACTED'
A body is sent with the request
2025-07-02 12:37:44,896 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:37:44,896 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    container = get_dependency_container()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
2025-07-02 12:37:44,946 - azure.core.pipeline.policies.http_logging_policy - INFO - Response status: 200
Response headers:
    'Cache-Control': 'no-store, no-transform, must-revalidate, no-cache, max-age=0, private'
    'Transfer-Encoding': 'chunked'
    'Content-Type': 'application/json'
    'Content-Encoding': 'REDACTED'
    'Vary': 'REDACTED'
    'pragma': 'no-cache'
    'apim-request-id': 'REDACTED'
    'request-context': 'REDACTED'
    'num_chars': 'REDACTED'
    'num_tokens': 'REDACTED'
    'prompt_token_len': 'REDACTED'
    'sampling_token_len': 'REDACTED'
    'x-content-type-options': 'REDACTED'
    'x-ms-region': 'REDACTED'
    'Strict-Transport-Security': 'REDACTED'
    'Date': 'Wed, 02 Jul 2025 12:37:44 GMT'
2025-07-02 12:37:44,962 - root - INFO - Azure embedding dimension: 1024
2025-07-02 12:37:44,967 - root - INFO - Embedder initialized with provider: azure
2025-07-02 12:37:44,985 - root - INFO - Progress tracker initialized
2025-07-02 12:37:44,985 - root - INFO - DEBUG: Created processor config with keys: ['chunk_size', 'chunk_overlap', 'supported_formats', 'max_file_size_mb', 'batch_size', 'timeout', 'file_timeout']
2025-07-02 12:37:44,985 - root - INFO - Azure AI config added to processor config
2025-07-02 12:37:44,986 - ExcelProcessor - INFO - Excel processor initialized with Azure AI support
2025-07-02 12:37:44,986 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ExcelProcessor
2025-07-02 12:37:44,987 - root - INFO - Extracted Azure AI config from general config
2025-07-02 12:37:45,110 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:37:45,176 - root - INFO - Azure AI client created successfully for PDF processing
2025-07-02 12:37:45,179 - root - INFO - Azure CV endpoint: https://computervision1298.cognitiveservices.azure...
2025-07-02 12:37:45,180 - root - INFO - Using EnhancedPDFProcessor with Azure AI integration
2025-07-02 12:37:45,304 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:37:45,334 - WordProcessor - INFO - Word processor initialized
2025-07-02 12:37:45,363 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: WordProcessor
2025-07-02 12:37:45,396 - ImageProcessor - INFO - Image processor initialized
2025-07-02 12:37:45,420 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ImageProcessor
2025-07-02 12:37:45,421 - ServiceNowProcessor - INFO - ServiceNow processor initialized
2025-07-02 12:37:45,421 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: ServiceNowProcessor
2025-07-02 12:37:45,422 - TextProcessor - INFO - Text processor initialized
2025-07-02 12:37:45,422 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: TextProcessor
2025-07-02 12:37:45,425 - rag_system.src.integrations.azure_ai.azure_client - INFO - Azure Computer Vision client initialized successfully
2025-07-02 12:37:45,427 - rag_system.src.ingestion.processors.base_processor - INFO - Registered processor: EnhancedPDFProcessor
2025-07-02 12:37:45,427 - root - INFO - Enhanced PDF Processor with Azure CV registered successfully
2025-07-02 12:37:45,427 - root - INFO - Processor registry initialized with 7 processors
2025-07-02 12:37:45,427 - root - INFO - Ingestion engine initialized with managed metadata
2025-07-02 12:37:45,428 - root - ERROR -  Failed to initialize progress tracker: attempted relative import beyond top-level package
2025-07-02 12:37:45,430 - root - INFO - FastAPI application created
2025-07-02 12:37:45,430 - __main__ - INFO - Managed RAG System initialized successfully
2025-07-02 12:37:45,439 - __main__ - INFO - Starting managed services...
2025-07-02 12:37:45,478 - monitoring.heartbeat_monitor - INFO -  Heartbeat monitoring is disabled
2025-07-02 12:37:45,479 - __main__ - INFO - Heartbeat monitor started
2025-07-02 12:37:45,479 - __main__ - INFO - Loading managed models...
2025-07-02 12:37:45,479 - __main__ - INFO - Using AzureEmbedder - no managed model loading needed
2025-07-02 12:37:45,479 - __main__ - INFO - All managed services started
2025-07-02 12:37:45,480 - __main__ - INFO - Managed RAG System ready!
2025-07-02 12:37:45,480 - __main__ - INFO - Starting managed server on 0.0.0.0:8000
2025-07-02 12:37:45,610 - root - INFO - Registered resource: uvicorn_server
2025-07-02 12:37:45,711 - root - INFO -  RAG System API starting up with managed resources...
2025-07-02 12:37:45,770 - root - INFO - Registered resource: feedback_store
2025-07-02 12:37:45,772 - root - INFO -  RAG System API shutting down - cleaning up managed resources...
2025-07-02 12:37:46,256 - root - INFO - Final system stats: {'app_name': 'GlobalRAGSystem', 'startup_complete': True, 'shutdown_complete': False, 'thread_pools': {'main': {'max_workers': 4, 'shutdown': False}, 'io': {'max_workers': 8, 'shutdown': False}, 'compute': {'max_workers': 2, 'shutdown': False}, 'background': {'max_workers': 2, 'shutdown': False}, 'api_operations': {'max_workers': 8, 'shutdown': False}}, 'resources': {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 510.9765625, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}, 'models': {'loaded_models': [], 'total_models': 0, 'memory_usage': {}, 'total_memory_mb': 0.0}, 'timestamp': 1751459866.2567935}
2025-07-02 12:37:46,256 - root - INFO -  Managed resource cleanup initiated
2025-07-02 12:37:46,257 - __main__ - INFO - Shutting down Managed RAG System...
2025-07-02 12:37:46,257 - root - INFO - Shutting down RAGSystem...
2025-07-02 12:37:46,681 - root - INFO - Pre-shutdown stats: {'total_resources': 8, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'config_manager', 'container', 'heartbeat_monitor', 'uvicorn_server'], 'memory_usage_mb': 511.25390625, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 13}
2025-07-02 12:37:46,681 - root - INFO - Starting comprehensive resource cleanup...
2025-07-02 12:37:46,685 - root - INFO - Custom cleanup completed for uvicorn_server
2025-07-02 12:37:46,685 - root - INFO - Shutting down thread pool: uvicorn_server
2025-07-02 12:37:46,685 - root - ERROR - Error in generic cleanup for uvicorn_server: Server.shutdown() got an unexpected keyword argument 'wait'
2025-07-02 12:37:46,685 - root - INFO - Cleaned up resource: uvicorn_server
2025-07-02 12:37:46,685 - monitoring.heartbeat_monitor - INFO -  Stopping health monitoring...
2025-07-02 12:37:46,686 - root - INFO - Custom cleanup completed for heartbeat_monitor
2025-07-02 12:37:46,686 - root - INFO - Cleaned up resource: heartbeat_monitor
2025-07-02 12:37:46,686 - root - INFO - Custom cleanup completed for container
2025-07-02 12:37:46,686 - root - INFO - Cleaned up resource: container
2025-07-02 12:37:46,686 - root - INFO - Cleaned up resource: config_manager
2025-07-02 12:37:46,686 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-02 12:37:46,687 - root - INFO - Thread pool 'background' shutdown completed
2025-07-02 12:37:46,687 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-02 12:37:46,687 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-02 12:37:46,687 - root - INFO - Cleaned up resource: threadpool_background
2025-07-02 12:37:46,687 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-02 12:37:46,688 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-02 12:37:46,688 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-02 12:37:46,689 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-02 12:37:46,689 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-02 12:37:46,689 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-02 12:37:46,689 - root - INFO - Thread pool 'io' shutdown completed
2025-07-02 12:37:46,689 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-02 12:37:46,689 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-02 12:37:46,689 - root - INFO - Cleaned up resource: threadpool_io
2025-07-02 12:37:46,697 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-02 12:37:46,699 - root - INFO - Thread pool 'main' shutdown completed
2025-07-02 12:37:46,701 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-02 12:37:46,710 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-02 12:37:46,719 - root - INFO - Cleaned up resource: threadpool_main
2025-07-02 12:37:47,095 - root - INFO - Resource cleanup completed
2025-07-02 12:37:47,404 - root - INFO - RAGSystem shutdown completed
2025-07-02 12:37:47,404 - __main__ - INFO - Managed RAG System shutdown complete
2025-07-02 12:37:47,405 - __main__ - INFO - Shutting down Managed RAG System...
2025-07-02 12:37:47,405 - __main__ - INFO - Managed RAG System shutdown complete
2025-07-02 12:37:47,405 - rag_system.src.core.model_memory_manager - INFO - Shutting down model memory manager...
2025-07-02 12:37:47,826 - rag_system.src.core.model_memory_manager - INFO - Model semantic_chunker_all-MiniLM-L6-v2 unloaded. Current memory: 511.38MB
2025-07-02 12:37:48,253 - rag_system.src.core.model_memory_manager - INFO - Model memory manager shutdown complete
2025-07-02 12:37:48,257 - root - INFO - Shutting down GlobalRAGSystem...
2025-07-02 12:37:49,333 - root - INFO - Pre-shutdown stats: {'total_resources': 6, 'resource_names': ['threadpool_main', 'threadpool_io', 'threadpool_compute', 'threadpool_background', 'threadpool_api_operations', 'feedback_store'], 'memory_usage_mb': 511.3828125, 'cpu_percent': 0.0, 'open_files': 3, 'threads': 12}
2025-07-02 12:37:49,334 - root - INFO - Starting comprehensive resource cleanup...
2025-07-02 12:37:49,335 - root - INFO - Custom cleanup completed for feedback_store
2025-07-02 12:37:49,335 - root - INFO - Cleaned up resource: feedback_store
2025-07-02 12:37:49,335 - root - INFO - Shutting down thread pool 'api_operations' with 8 workers
2025-07-02 12:37:49,335 - root - INFO - Thread pool 'api_operations' shutdown completed
2025-07-02 12:37:49,336 - root - INFO - Custom cleanup completed for threadpool_api_operations
2025-07-02 12:37:49,347 - root - INFO - Shutting down thread pool: threadpool_api_operations
2025-07-02 12:37:49,349 - root - INFO - Cleaned up resource: threadpool_api_operations
2025-07-02 12:37:49,349 - root - INFO - Shutting down thread pool 'background' with 2 workers
2025-07-02 12:37:49,350 - root - INFO - Thread pool 'background' shutdown completed
2025-07-02 12:37:49,350 - root - INFO - Custom cleanup completed for threadpool_background
2025-07-02 12:37:49,350 - root - INFO - Shutting down thread pool: threadpool_background
2025-07-02 12:37:49,350 - root - INFO - Cleaned up resource: threadpool_background
2025-07-02 12:37:49,351 - root - INFO - Shutting down thread pool 'compute' with 2 workers
2025-07-02 12:37:49,351 - root - INFO - Thread pool 'compute' shutdown completed
2025-07-02 12:37:49,351 - root - INFO - Custom cleanup completed for threadpool_compute
2025-07-02 12:37:49,351 - root - INFO - Shutting down thread pool: threadpool_compute
2025-07-02 12:37:49,351 - root - INFO - Cleaned up resource: threadpool_compute
2025-07-02 12:37:49,352 - root - INFO - Shutting down thread pool 'io' with 8 workers
2025-07-02 12:37:49,352 - root - INFO - Thread pool 'io' shutdown completed
2025-07-02 12:37:49,352 - root - INFO - Custom cleanup completed for threadpool_io
2025-07-02 12:37:49,352 - root - INFO - Shutting down thread pool: threadpool_io
2025-07-02 12:37:49,354 - root - INFO - Cleaned up resource: threadpool_io
2025-07-02 12:37:49,355 - root - INFO - Shutting down thread pool 'main' with 4 workers
2025-07-02 12:37:49,356 - root - INFO - Thread pool 'main' shutdown completed
2025-07-02 12:37:49,356 - root - INFO - Custom cleanup completed for threadpool_main
2025-07-02 12:37:49,356 - root - INFO - Shutting down thread pool: threadpool_main
2025-07-02 12:37:49,357 - root - INFO - Cleaned up resource: threadpool_main
2025-07-02 12:37:50,825 - root - INFO - Resource cleanup completed
2025-07-02 12:37:51,270 - root - INFO - GlobalRAGSystem shutdown completed
2025-07-02 12:37:51,977 - root - INFO - ResourceManager initialized
2025-07-02 12:37:51,977 - root - INFO - Initialized GlobalRAGSystem lifecycle manager
2025-07-02 12:37:51,977 - root - INFO - Starting GlobalRAGSystem...
2025-07-02 12:37:51,978 - root - INFO - Registered resource: threadpool_main
2025-07-02 12:37:51,978 - root - INFO - Created managed thread pool 'main' with 4 workers
2025-07-02 12:37:51,978 - root - INFO - Registered resource: threadpool_io
2025-07-02 12:37:51,978 - root - INFO - Created managed thread pool 'io' with 8 workers
2025-07-02 12:37:51,978 - root - INFO - Registered resource: threadpool_compute
2025-07-02 12:37:51,978 - root - INFO - Created managed thread pool 'compute' with 2 workers
2025-07-02 12:37:51,979 - root - INFO - Registered resource: threadpool_background
2025-07-02 12:37:51,979 - root - INFO - Created managed thread pool 'background' with 2 workers
2025-07-02 12:37:51,979 - root - INFO - GlobalRAGSystem startup completed successfully
2025-07-02 12:37:51,979 - root - INFO - Cleaned up semantic chunker model: all-MiniLM-L6-v2
2025-07-02 12:39:19,541 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:39:19,541 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    logger.info("New ConversationManager initialized successfully.")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
2025-07-02 12:40:06,031 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:40:06,031 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    logger.info("New ConversationManager initialized successfully.")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
2025-07-02 12:42:16,908 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:42:16,910 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    # Create the manager instance - using SimpleConversationManager instead of ConversationManager
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
2025-07-02 12:43:15,518 - api.routes.conversation - INFO - Initializing new ConversationManager...
2025-07-02 12:43:15,518 - api.routes.conversation - ERROR - Failed to initialize ConversationManager: Dependency container not initialized
Traceback (most recent call last):
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\api\routes\conversation.py", line 47, in get_conversation_manager
    # Create the manager instance - using SimpleConversationManager instead of ConversationManager
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Projects-D\pepsi-final3\rag_new\rag_system\src\core\dependency_container.py", line 596, in get_dependency_container
    raise RuntimeError("Dependency container not initialized")
RuntimeError: Dependency container not initialized
