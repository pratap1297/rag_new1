#!/usr/bin/env python3
"""
Test Persistent JSON Metadata Store
Verify that the new persistent store works correctly
"""
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_persistent_metadata_store():
    print("ğŸ§ª Testing Persistent JSON Metadata Store")
    print("=" * 50)
    
    try:
        # Import the persistent store
        from storage.persistent_metadata_store import PersistentJSONMetadataStore
        
        # Test 1: Create store instance
        print("\n1ï¸âƒ£ Creating store instance...")
        store = PersistentJSONMetadataStore("data/metadata_test")
        print("   âœ… Store created successfully")
        
        # Test 2: Add file metadata
        print("\n2ï¸âƒ£ Adding file metadata...")
        file_id = store.add_file_metadata(
            "test_document.pdf", 
            {"size": 1024, "pages": 5, "type": "pdf"}
        )
        print(f"   âœ… File added with ID: {file_id}")
        
        # Test 3: Add chunk metadata with vector linking
        print("\n3ï¸âƒ£ Adding chunk metadata...")
        chunk_id = store.add_chunk_metadata({
            "doc_id": file_id,
            "filename": "test_document.pdf",
            "content": "This is a test chunk from the document",
            "vector_id": "vector_123",
            "chunk_index": 0,
            "start_char": 0,
            "end_char": 100
        })
        print(f"   âœ… Chunk added with ID: {chunk_id}")
        
        # Test 4: Retrieve metadata by vector ID
        print("\n4ï¸âƒ£ Testing vector-metadata linking...")
        metadata = store.get_metadata_by_vector_id("vector_123")
        if metadata:
            print(f"   âœ… Found metadata for vector_123:")
            print(f"      - Filename: {metadata.get('filename')}")
            print(f"      - Content: {metadata.get('content')[:50]}...")
        else:
            print("   âŒ No metadata found for vector_123")
        
        # Test 5: Get all files and chunks
        print("\n5ï¸âƒ£ Testing data retrieval...")
        files = store.get_all_files()
        chunks = store.get_all_chunks()
        print(f"   âœ… Files: {len(files)}")
        print(f"   âœ… Chunks: {len(chunks)}")
        
        # Test 6: Test persistence (create new instance)
        print("\n6ï¸âƒ£ Testing persistence...")
        store2 = PersistentJSONMetadataStore("data/metadata_test")
        files2 = store2.get_all_files()
        chunks2 = store2.get_all_chunks()
        metadata2 = store2.get_metadata_by_vector_id("vector_123")
        
        print(f"   âœ… After reload - Files: {len(files2)}")
        print(f"   âœ… After reload - Chunks: {len(chunks2)}")
        print(f"   âœ… After reload - Vector metadata: {'Found' if metadata2 else 'Not found'}")
        
        # Test 7: Get statistics
        print("\n7ï¸âƒ£ Testing statistics...")
        stats = store.get_stats()
        print(f"   âœ… Total files: {stats['total_files']}")
        print(f"   âœ… Total chunks: {stats['total_chunks']}")
        print(f"   âœ… Total vector mappings: {stats['total_vector_mappings']}")
        print(f"   âœ… Storage path: {stats['storage_path']}")
        
        # Test 8: Test with multiple chunks
        print("\n8ï¸âƒ£ Testing multiple chunks...")
        for i in range(3):
            chunk_id = store.add_chunk_metadata({
                "doc_id": file_id,
                "filename": "test_document.pdf",
                "content": f"This is chunk {i+1} from the document",
                "vector_id": f"vector_{i+100}",
                "chunk_index": i+1,
                "start_char": (i+1) * 100,
                "end_char": (i+2) * 100
            })
        
        final_stats = store.get_stats()
        print(f"   âœ… Final chunks: {final_stats['total_chunks']}")
        print(f"   âœ… Final vector mappings: {final_stats['total_vector_mappings']}")
        
        # Test 9: Test file chunks retrieval
        print("\n9ï¸âƒ£ Testing file chunks retrieval...")
        file_chunks = store.get_file_chunks(file_id)
        print(f"   âœ… Chunks for file {file_id}: {len(file_chunks)}")
        
        # Cleanup
        print("\nğŸ§¹ Cleaning up test data...")
        store.clear_all_data()
        import shutil
        if os.path.exists("data/metadata_test"):
            shutil.rmtree("data/metadata_test")
        print("   âœ… Test data cleaned up")
        
        print("\nâœ… ALL TESTS PASSED!")
        print("\nğŸ¯ **Persistent JSON Metadata Store is working correctly!**")
        print("   â€¢ File metadata storage âœ…")
        print("   â€¢ Chunk metadata storage âœ…") 
        print("   â€¢ Vector-metadata linking âœ…")
        print("   â€¢ Data persistence across restarts âœ…")
        print("   â€¢ Statistics and retrieval âœ…")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_persistent_metadata_store()
    if success:
        print("\nğŸš€ **Ready to use persistent metadata store!**")
        print("   1. Restart your RAG system")
        print("   2. Upload documents - metadata will persist")
        print("   3. No more 'doc_unknown' issues!")
    else:
        print("\nâš ï¸ **Fix issues before proceeding**")
    
    sys.exit(0 if success else 1) 